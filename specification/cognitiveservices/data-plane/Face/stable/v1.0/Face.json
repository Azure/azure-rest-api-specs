{
  "swagger": "3.0",
  "info": {
    "version": "1.0",
    "title": "Face Client",
    "description": "An API for face detection, verification, and identification."
  },
  "securityDefinitions": {
    "apim_key": {
      "type": "apiKey",
      "name": "Ocp-Apim-Subscription-Key",
      "in": "header"
    }
  },
  "security": [
    {
      "apim_key": []
    }
  ],
  "x-ms-parameterized-host": {
    "hostTemplate": "{Endpoint}/face/v1.0",
    "useSchemePrefix": false,
    "parameters": [
      {
        "$ref": "#/parameters/Endpoint"
      }
    ]
  },
  "paths": {
    "/compare": {
      "post": {
        "description": "Compare faces from two images based on similarity.\n<br/>\nRemarks:<br />\n* Higher face image quality means better identification precision. Please consider high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.\n* For the scenarios that are sensitive to accuracy please make your own judgment.\n",
        "operationId": "Face_CompareFaceToFace",
        "parameters": [
          {
            "name": "detectionModel",
            "description": "Name of detection model. Detection model is used to detect faces in the submitted image.",
            "default": "detection_02",
            "required": false,
            "type": "string",
            "in": "query",
            "x-ms-parameter-location": "method",
            "x-nullable": false,
            "x-ms-enum": {
              "name": "DetectionModel",
              "modelAsString": true
            },
            "enum": [
              "detection_01",
              "detection_02"
            ]
          },
          {
            "name": "recognitionModel",
            "description": "Name of recognition model.",
            "default": "recognition_03",
            "required": false,
            "type": "string",
            "in": "query",
            "x-ms-parameter-location": "method",
            "x-nullable": false,
            "x-ms-enum": {
              "name": "RecognitionModel",
              "modelAsString": true
            },
            "enum": [
              "recognition_01",
              "recognition_02",
              "recognition_03"
            ]
          },
        ],
        "requestBody": {
          "required": true,
          "description": "The body of the request",
          "content": {
            "application/json": {
              "description": "Request body for face to face comparison.",
              "x-ms-client-flatten": true,
              "schema": {
                "$ref": "#/definitions/CompareFaceToFaceRequest"
              }
            },
            "multipart/form-data": {
              "description": "Multipart/form-data for face to face comparison.",
              "schema": {
                "$ref": "#/definitions/CompareFaceToFaceFormRequest"
              }
            }
          }
        },
        "produces": [
          "application/json"
        ],
        "responses": {
          "200": {
            "description": "A successful call returns the comparison result.",
            "schema": {
              "$ref": "#/definitions/CompareResult"
            }
          },
          "default": {
            "description": "Error response.",
            "schema": {
              "$ref": "#/definitions/APIError"
            }
          }
        },
        "x-ms-examples": {
          "Verify faces example": {
            "$ref": "./examples/CompareFaceToFace.json"
          }
        }
      }
    }
  },
  "x-ms-paths": {
    "/verify?overload=person": {
      "post": {
        "description": "Verify whether two faces belong to a same person. Compares a face Id with a Person Id",
        "operationId": "Face_VerifyFaceToPerson",
        "parameters": [
          {
            "name": "body",
            "description": "Request body for face to person verification.",
            "in": "body",
            "required": true,
            "x-ms-client-flatten": true,
            "schema": {
              "$ref": "#/definitions/VerifyFaceToPersonRequest"
            }
          }
        ],
        "consumes": [
          "application/json"
        ],
        "produces": [
          "application/json"
        ],
        "responses": {
          "200": {
            "description": "A successful call returns the verification result.",
            "schema": {
              "$ref": "#/definitions/VerifyResult"
            }
          },
          "default": {
            "description": "Error response.",
            "schema": {
              "$ref": "#/definitions/APIError"
            }
          }
        },
        "x-ms-examples": {
          "Verify face to person example": {
            "$ref": "./examples/VerifyFaceToPerson.json"
          }
        }
      }
    },
    "/detect?overload=stream": {
      "post": {
        "description": "Detect human faces in an image, return face rectangles, and optionally with faceIds, landmarks, and attributes.<br />\n* No image will be stored. Only the extracted face feature will be stored on server. The faceId is an identifier of the face feature and will be used in [Face - Identify](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/identify), [Face - Verify](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/verifyfacetoface), and [Face - Find Similar](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/findsimilar). The stored face feature(s) will expire and be deleted 24 hours after the original detection call.\n* Optional parameters include faceId, landmarks, and attributes. Attributes include age, gender, headPose, smile, facialHair, glasses, emotion, hair, makeup, occlusion, accessories, blur, exposure and noise. Some of the results returned for specific attributes may not be highly accurate.\n* JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.\n* Up to 100 faces can be returned for an image. Faces are ranked by face rectangle size from large to small.\n* For optimal results when querying [Face - Identify](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/identify), [Face - Verify](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/verifyfacetoface), and [Face - Find Similar](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/findsimilar) ('returnFaceId' is true), please use faces that are: frontal, clear, and with a minimum size of 200x200 pixels (100 pixels between eyes).\n* The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.\n* Different 'detectionModel' values can be provided. To use and compare different detection models, please refer to [How to specify a detection model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-detection-model)\n  | Model | Recommended use-case(s) |\n  | ---------- | -------- |\n  | 'detection_01': | The default detection model for [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl). Recommend for near frontal face detection. For scenarios with exceptionally large angle (head-pose) faces, occluded faces or wrong image orientation, the faces in such cases may not be detected. |\n  | 'detection_02': | Detection model released in 2019 May with improved accuracy especially on small, side and blurry faces. |\n\n* Different 'recognitionModel' values are provided. If follow-up operations like Verify, Identify, Find Similar are needed, please specify the recognition model with 'recognitionModel' parameter. The default value for 'recognitionModel' is 'recognition_01', if latest model needed, please explicitly specify the model you need in this parameter. Once specified, the detected faceIds will be associated with the specified recognition model. More details, please refer to [How to specify a recognition model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-recognition-model)\n  | Model | Recommended use-case(s) |\n  | ---------- | -------- |\n  | 'recognition_01': | The default recognition model for [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl). All those faceIds created before 2019 March are bonded with this recognition model. |\n  | 'recognition_02': | Recognition model released in 2019 March. |\n  | 'recognition_03': | Recognition model released in 2020 May. 'recognition_03' is recommended since its overall accuracy is improved compared with 'recognition_01' and 'recognition_02'. |",
        "operationId": "Face_DetectWithStream",
        "parameters": [
          {
            "name": "returnFaceId",
            "in": "query",
            "description": "A value indicating whether the operation should return faceIds of detected faces.",
            "type": "boolean",
            "default": true
          },
          {
            "name": "returnFaceLandmarks",
            "in": "query",
            "description": "A value indicating whether the operation should return landmarks of the detected faces.",
            "type": "boolean",
            "default": false
          },
          {
            "$ref": "#/parameters/returnFaceAttributes"
          },
          {
            "$ref": "../../../Common/Parameters.json#/parameters/ImageStream"
          },
          {
            "$ref": "#/parameters/recognitionModel"
          },
          {
            "$ref": "#/parameters/returnRecognitionModel"
          },
          {
            "$ref": "#/parameters/detectionModel"
          }
        ],
        "consumes": [
          "application/octet-stream"
        ],
        "produces": [
          "application/json"
        ],
        "responses": {
          "200": {
            "description": "A successful call returns an array of face entries ranked by face rectangle size in descending order. An empty response indicates no faces detected.",
            "schema": {
              "$ref": "#/definitions/DetectedFaces"
            }
          },
          "default": {
            "description": "Error response.",
            "schema": {
              "$ref": "#/definitions/APIError"
            }
          }
        },
        "x-ms-examples": {
          "Detect with stream example": {
            "$ref": "./examples/DetectWithStream.json"
          }
        }
      }
    },
    "/persongroups/{personGroupId}/persons/{personId}/persistedfaces?overload=stream": {
      "post": {
        "description": "Add a face to a person into a person group for face identification or verification. To deal with an image contains multiple faces, input face can be specified as an image with a targetFace rectangle. It returns a persistedFaceId representing the added face. No image will be stored. Only the extracted face feature will be stored on server until [PersonGroup PersonFace - Delete](https://docs.microsoft.com/rest/api/cognitiveservices/face/persongroupperson/deleteface), [PersonGroup Person - Delete](https://docs.microsoft.com/rest/api/cognitiveservices/face/persongroupperson/delete) or [PersonGroup - Delete](https://docs.microsoft.com/rest/api/cognitiveservices/face/persongroup/delete) is called.\n<br /> Note persistedFaceId is different from faceId generated by [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl).\n*   Higher face image quality means better recognition precision. Please consider high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.\n*   Each person entry can hold up to 248 faces.\n*   JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.\n*   \"targetFace\" rectangle should contain one face. Zero or multiple faces will be regarded as an error. If the provided \"targetFace\" rectangle is not returned from [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl), there’s no guarantee to detect and add the face successfully.\n*   Out of detectable face size (36x36 - 4096x4096 pixels), large head-pose, or large occlusions will cause failures.\n*   Adding/deleting faces to/from a same person will be processed sequentially. Adding/deleting faces to/from different persons are processed in parallel.\n* The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.\n* Different 'detectionModel' values can be provided. To use and compare different detection models, please refer to [How to specify a detection model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-detection-model)\n  | Model | Recommended use-case(s) |\n  | ---------- | -------- |\n  | 'detection_01': | The default detection model for [PersonGroup Person - Add Face](https://docs.microsoft.com/rest/api/cognitiveservices/face/persongroupperson/addfacefromurl). Recommend for near frontal face detection. For scenarios with exceptionally large angle (head-pose) faces, occluded faces or wrong image orientation, the faces in such cases may not be detected. |\n  | 'detection_02': | Detection model released in 2019 May with improved accuracy especially on small, side and blurry faces. |",
        "operationId": "PersonGroupPerson_AddFaceFromStream",
        "parameters": [
          {
            "$ref": "#/parameters/personGroupId"
          },
          {
            "$ref": "#/parameters/personId"
          },
          {
            "$ref": "#/parameters/faceUserData"
          },
          {
            "$ref": "#/parameters/targetFace"
          },
          {
            "$ref": "../../../Common/Parameters.json#/parameters/ImageStream"
          },
          {
            "$ref": "#/parameters/detectionModel"
          }
        ],
        "produces": [
          "application/json"
        ],
        "consumes": [
          "application/octet-stream"
        ],
        "responses": {
          "200": {
            "description": "A successful call returns the new persistedFaceId.",
            "schema": {
              "$ref": "#/definitions/PersistedFace"
            }
          },
          "default": {
            "description": "Error response.",
            "schema": {
              "$ref": "#/definitions/APIError"
            }
          }
        },
        "x-ms-examples": {
          "Add face to person with stream example": {
            "$ref": "./examples/AddPersonGroupPersonFaceFromStream.json"
          }
        }
      }
    },
    "/facelists/{faceListId}/persistedfaces?overload=stream": {
      "post": {
        "description": "Add a face to a specified face list, up to 1,000 faces.\n<br /> To deal with an image contains multiple faces, input face can be specified as an image with a targetFace rectangle. It returns a persistedFaceId representing the added face. No image will be stored. Only the extracted face feature will be stored on server until [FaceList - Delete Face](https://docs.microsoft.com/rest/api/cognitiveservices/face/facelist/deleteface) or [FaceList - Delete](https://docs.microsoft.com/rest/api/cognitiveservices/face/facelist/delete) is called.\n<br /> Note persistedFaceId is different from faceId generated by [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl).\n* Higher face image quality means better detection and recognition precision. Please consider high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.\n* JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.\n* \"targetFace\" rectangle should contain one face. Zero or multiple faces will be regarded as an error. If the provided \"targetFace\" rectangle is not returned from [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl), there’s no guarantee to detect and add the face successfully.\n* Out of detectable face size (36x36 - 4096x4096 pixels), large head-pose, or large occlusions will cause failures.\n* Adding/deleting faces to/from a same face list are processed sequentially and to/from different face lists are in parallel.\n* The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.\n* Different 'detectionModel' values can be provided. To use and compare different detection models, please refer to [How to specify a detection model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-detection-model)\n  | Model | Recommended use-case(s) |\n  | ---------- | -------- |\n  | 'detection_01': | The default detection model for [FaceList - Add Face](https://docs.microsoft.com/rest/api/cognitiveservices/face/facelist/addfacefromurl). Recommend for near frontal face detection. For scenarios with exceptionally large angle (head-pose) faces, occluded faces or wrong image orientation, the faces in such cases may not be detected. |\n  | 'detection_02': | Detection model released in 2019 May with improved accuracy especially on small, side and blurry faces. |",
        "operationId": "FaceList_AddFaceFromStream",
        "parameters": [
          {
            "$ref": "#/parameters/faceListId"
          },
          {
            "$ref": "#/parameters/faceUserData"
          },
          {
            "$ref": "#/parameters/targetFace"
          },
          {
            "$ref": "../../../Common/Parameters.json#/parameters/ImageStream"
          },
          {
            "$ref": "#/parameters/detectionModel"
          }
        ],
        "consumes": [
          "application/octet-stream"
        ],
        "responses": {
          "200": {
            "description": "A successful call returns a new persistedFaceId.",
            "schema": {
              "$ref": "#/definitions/PersistedFace"
            }
          },
          "default": {
            "description": "Error response.",
            "schema": {
              "$ref": "#/definitions/APIError"
            }
          }
        },
        "produces": [
          "application/json"
        ],
        "x-ms-examples": {
          "Add face to face list from stream example": {
            "$ref": "./examples/AddFaceListFaceFromStream.json"
          }
        }
      }
    },
    "/largepersongroups/{largePersonGroupId}/persons/{personId}/persistedfaces?overload=stream": {
      "post": {
        "description": "Add a face to a person into a large person group for face identification or verification. To deal with an image contains multiple faces, input face can be specified as an image with a targetFace rectangle. It returns a persistedFaceId representing the added face. No image will be stored. Only the extracted face feature will be stored on server until [LargePersonGroup PersonFace - Delete](https://docs.microsoft.com/rest/api/cognitiveservices/face/largepersongroupperson/deleteface), [LargePersonGroup Person - Delete](https://docs.microsoft.com/rest/api/cognitiveservices/face/largepersongroupperson/delete) or [LargePersonGroup - Delete](https://docs.microsoft.com/rest/api/cognitiveservices/face/largepersongroup/delete) is called.\n<br /> Note persistedFaceId is different from faceId generated by [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl).\n* Higher face image quality means better recognition precision. Please consider high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.\n* Each person entry can hold up to 248 faces.\n* JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.\n* \"targetFace\" rectangle should contain one face. Zero or multiple faces will be regarded as an error. If the provided \"targetFace\" rectangle is not returned from [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl), there’s no guarantee to detect and add the face successfully.\n* Out of detectable face size (36x36 - 4096x4096 pixels), large head-pose, or large occlusions will cause failures.\n* Adding/deleting faces to/from a same person will be processed sequentially. Adding/deleting faces to/from different persons are processed in parallel.\n* The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.\n* Different 'detectionModel' values can be provided. To use and compare different detection models, please refer to [How to specify a detection model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-detection-model)\n  | Model | Recommended use-case(s) |\n  | ---------- | -------- |\n  | 'detection_01': | The default detection model for [LargePersonGroup Person - Add Face](https://docs.microsoft.com/rest/api/cognitiveservices/face/largepersongroupperson/addfacefromurl). Recommend for near frontal face detection. For scenarios with exceptionally large angle (head-pose) faces, occluded faces or wrong image orientation, the faces in such cases may not be detected. |\n  | 'detection_02': | Detection model released in 2019 May with improved accuracy especially on small, side and blurry faces. |",
        "operationId": "LargePersonGroupPerson_AddFaceFromStream",
        "parameters": [
          {
            "$ref": "#/parameters/largePersonGroupId"
          },
          {
            "$ref": "#/parameters/personId"
          },
          {
            "$ref": "#/parameters/faceUserData"
          },
          {
            "$ref": "#/parameters/targetFace"
          },
          {
            "$ref": "../../../Common/Parameters.json#/parameters/ImageStream"
          },
          {
            "$ref": "#/parameters/detectionModel"
          }
        ],
        "produces": [
          "application/json"
        ],
        "consumes": [
          "application/octet-stream"
        ],
        "responses": {
          "200": {
            "description": "A successful call returns the new persistedFaceId.",
            "schema": {
              "$ref": "#/definitions/PersistedFace"
            }
          },
          "default": {
            "description": "Error response.",
            "schema": {
              "$ref": "#/definitions/APIError"
            }
          }
        },
        "x-ms-examples": {
          "Add face to person with stream example": {
            "$ref": "./examples/AddLargePersonGroupPersonFaceFromStream.json"
          }
        }
      }
    },
    "/largefacelists/{largeFaceListId}/persistedfaces?overload=stream": {
      "post": {
        "description": "Add a face to a specified large face list, up to 1,000,000 faces.\n<br /> To deal with an image contains multiple faces, input face can be specified as an image with a targetFace rectangle. It returns a persistedFaceId representing the added face. No image will be stored. Only the extracted face feature will be stored on server until [LargeFaceList Face - Delete](https://docs.microsoft.com/rest/api/cognitiveservices/face/largefacelist/deleteface) or [LargeFaceList - Delete](https://docs.microsoft.com/rest/api/cognitiveservices/face/largefacelist/delete) is called.\n<br /> Note persistedFaceId is different from faceId generated by [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl).\n* Higher face image quality means better recognition precision. Please consider high-quality faces: frontal, clear, and face size is 200x200 pixels (100 pixels between eyes) or bigger.\n* JPEG, PNG, GIF (the first frame), and BMP format are supported. The allowed image file size is from 1KB to 6MB.\n* \"targetFace\" rectangle should contain one face. Zero or multiple faces will be regarded as an error. If the provided \"targetFace\" rectangle is not returned from [Face - Detect](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithurl), there’s no guarantee to detect and add the face successfully.\n* Out of detectable face size (36x36 - 4096x4096 pixels), large head-pose, or large occlusions will cause failures.\n* Adding/deleting faces to/from a same face list are processed sequentially and to/from different face lists are in parallel.\n* The minimum detectable face size is 36x36 pixels in an image no larger than 1920x1080 pixels. Images with dimensions higher than 1920x1080 pixels will need a proportionally larger minimum face size.\n* Different 'detectionModel' values can be provided. To use and compare different detection models, please refer to [How to specify a detection model](https://docs.microsoft.com/azure/cognitive-services/face/face-api-how-to-topics/specify-detection-model)\n  | Model | Recommended use-case(s) |\n  | ---------- | -------- |\n  | 'detection_01': | The default detection model for [LargeFaceList - Add Face](https://docs.microsoft.com/rest/api/cognitiveservices/face/largefacelist/addfacefromurl). Recommend for near frontal face detection. For scenarios with exceptionally large angle (head-pose) faces, occluded faces or wrong image orientation, the faces in such cases may not be detected. |\n  | 'detection_02': | Detection model released in 2019 May with improved accuracy especially on small, side and blurry faces. |\n\nQuota:\n* Free-tier subscription quota: 1,000 faces per large face list.\n* S0-tier subscription quota: 1,000,000 faces per large face list.",
        "operationId": "LargeFaceList_AddFaceFromStream",
        "parameters": [
          {
            "$ref": "#/parameters/largeFaceListId"
          },
          {
            "$ref": "#/parameters/faceUserData"
          },
          {
            "$ref": "#/parameters/targetFace"
          },
          {
            "$ref": "../../../Common/Parameters.json#/parameters/ImageStream"
          },
          {
            "$ref": "#/parameters/detectionModel"
          }
        ],
        "consumes": [
          "application/octet-stream"
        ],
        "responses": {
          "200": {
            "description": "A successful call returns a new persistedFaceId.",
            "schema": {
              "$ref": "#/definitions/PersistedFace"
            }
          },
          "default": {
            "description": "Error response.",
            "schema": {
              "$ref": "#/definitions/APIError"
            }
          }
        },
        "produces": [
          "application/json"
        ],
        "x-ms-examples": {
          "Add face to large face list from stream example": {
            "$ref": "./examples/AddLargeFaceListFaceFromStream.json"
          }
        }
      }
    }
  },
  "definitions": {
    "APIError": {
      "type": "object",
      "description": "Error information returned by the API",
      "properties": {
        "error": {
          "$ref": "#/definitions/Error"
        }
      }
    },
    "Error": {
      "type": "object",
      "description": "Error body.",
      "properties": {
        "code": {
          "type": "string"
        },
        "message": {
          "type": "string"
        }
      }
    },
    "DetectedFaces": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/DetectedFace"
      }
    },
    "DetectedFace": {
      "type": "object",
      "required": [
        "faceRectangle"
      ],
      "description": "Detected Face object.",
      "properties": {
        "faceId": {
          "type": "string",
          "format": "uuid"
        },
        "recognitionModel": {
          "$ref": "#/definitions/RecognitionModel"
        },
        "faceRectangle": {
          "$ref": "#/definitions/FaceRectangle"
        },
        "faceLandmarks": {
          "$ref": "#/definitions/FaceLandmarks"
        },
        "faceAttributes": {
          "$ref": "#/definitions/FaceAttributes"
        }
      }
    },
    "FaceRectangle": {
      "type": "object",
      "required": [
        "width",
        "height",
        "left",
        "top"
      ],
      "description": "A rectangle within which a face can be found",
      "properties": {
        "width": {
          "type": "integer",
          "format": "int32",
          "description": "The width of the rectangle, in pixels."
        },
        "height": {
          "type": "integer",
          "format": "int32",
          "description": "The height of the rectangle, in pixels."
        },
        "left": {
          "type": "integer",
          "format": "int32",
          "description": "The distance from the left edge if the image to the left edge of the rectangle, in pixels."
        },
        "top": {
          "type": "integer",
          "format": "int32",
          "description": "The distance from the top edge if the image to the top edge of the rectangle, in pixels."
        }
      }
    },
    "Coordinate": {
      "type": "object",
      "required": [
        "x",
        "y"
      ],
      "description": "Coordinates within an image",
      "properties": {
        "x": {
          "type": "number",
          "description": "The horizontal component, in pixels."
        },
        "y": {
          "type": "number",
          "description": "The vertical component, in pixels."
        }
      }
    },
    "FaceLandmarks": {
      "type": "object",
      "description": "A collection of 27-point face landmarks pointing to the important positions of face components.",
      "properties": {
        "pupilLeft": {
          "$ref": "#/definitions/Coordinate"
        },
        "pupilRight": {
          "$ref": "#/definitions/Coordinate"
        },
        "noseTip": {
          "$ref": "#/definitions/Coordinate"
        },
        "mouthLeft": {
          "$ref": "#/definitions/Coordinate"
        },
        "mouthRight": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyebrowLeftOuter": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyebrowLeftInner": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyeLeftOuter": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyeLeftTop": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyeLeftBottom": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyeLeftInner": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyebrowRightInner": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyebrowRightOuter": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyeRightInner": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyeRightTop": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyeRightBottom": {
          "$ref": "#/definitions/Coordinate"
        },
        "eyeRightOuter": {
          "$ref": "#/definitions/Coordinate"
        },
        "noseRootLeft": {
          "$ref": "#/definitions/Coordinate"
        },
        "noseRootRight": {
          "$ref": "#/definitions/Coordinate"
        },
        "noseLeftAlarTop": {
          "$ref": "#/definitions/Coordinate"
        },
        "noseRightAlarTop": {
          "$ref": "#/definitions/Coordinate"
        },
        "noseLeftAlarOutTip": {
          "$ref": "#/definitions/Coordinate"
        },
        "noseRightAlarOutTip": {
          "$ref": "#/definitions/Coordinate"
        },
        "upperLipTop": {
          "$ref": "#/definitions/Coordinate"
        },
        "upperLipBottom": {
          "$ref": "#/definitions/Coordinate"
        },
        "underLipTop": {
          "$ref": "#/definitions/Coordinate"
        },
        "underLipBottom": {
          "$ref": "#/definitions/Coordinate"
        }
      }
    },
    "FaceAttributes": {
      "type": "object",
      "description": "Face Attributes",
      "properties": {
        "age": {
          "type": "number",
          "description": "Age in years"
        },
        "gender": {
          "type": "string",
          "description": "Possible gender of the face.",
          "x-ms-enum": {
            "name": "Gender",
            "modelAsString": false
          },
          "enum": [
            "male",
            "female"
          ]
        },
        "smile": {
          "description": "Smile intensity, a number between [0,1] ",
          "$ref": "#/definitions/Level"
        },
        "facialHair": {
          "description": "Properties describing facial hair attributes.",
          "$ref": "#/definitions/FacialHair"
        },
        "glasses": {
          "type": "string",
          "description": "Glasses type if any of the face.",
          "x-ms-enum": {
            "name": "GlassesType",
            "modelAsString": false
          },
          "enum": [
            "noGlasses",
            "readingGlasses",
            "sunglasses",
            "swimmingGoggles"
          ]
        },
        "headPose": {
          "description": "Properties indicating head pose of the face.",
          "$ref": "#/definitions/HeadPose"
        },
        "emotion": {
          "description": "Properties describing facial emotion in form of confidence ranging from 0 to 1.",
          "$ref": "#/definitions/Emotion"
        },
        "hair": {
          "description": "Properties describing hair attributes.",
          "$ref": "#/definitions/Hair"
        },
        "makeup": {
          "description": "Properties describing present makeups on a given face.",
          "$ref": "#/definitions/Makeup"
        },
        "occlusion": {
          "description": "Properties describing occlusions on a given face.",
          "$ref": "#/definitions/Occlusion"
        },
        "accessories": {
          "description": "Properties describing any accessories on a given face.",
          "$ref": "#/definitions/Accessories"
        },
        "blur": {
          "description": "Properties describing any presence of blur within the image.",
          "$ref": "#/definitions/Blur"
        },
        "exposure": {
          "description": "Properties describing exposure level of the image.",
          "$ref": "#/definitions/Exposure"
        },
        "noise": {
          "description": "Properties describing noise level of the image.",
          "$ref": "#/definitions/Noise"
        }
      }
    },
    "FacialHair": {
      "type": "object",
      "description": "Properties describing facial hair attributes.",
      "properties": {
        "moustache": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "beard": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "sideburns": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        }
      }
    },
    "HeadPose": {
      "type": "object",
      "description": "Properties indicating head pose of the face.",
      "properties": {
        "roll": {
          "type": "number",
          "x-nullable": false
        },
        "yaw": {
          "type": "number",
          "x-nullable": false
        },
        "pitch": {
          "type": "number",
          "x-nullable": false
        }
      }
    },
    "Emotion": {
      "type": "object",
      "description": "Properties describing facial emotion in form of confidence ranging from 0 to 1.",
      "properties": {
        "anger": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "contempt": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "disgust": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "fear": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "happiness": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "neutral": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "sadness": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "surprise": {
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        }
      }
    },
    "Hair": {
      "type": "object",
      "description": "Properties describing hair attributes.",
      "properties": {
        "bald": {
          "description": "A number describing confidence level of whether the person is bald.",
          "x-nullable": false,
          "$ref": "#/definitions/Confidence"
        },
        "invisible": {
          "type": "boolean",
          "description": "A boolean value describing whether the hair is visible in the image.",
          "x-nullable": false
        },
        "hairColor": {
          "description": "An array of candidate colors and confidence level in the presence of each.",
          "$ref": "#/definitions/HairColors"
        }
      }
    },
    "HairColors": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/HairColor"
      }
    },
    "HairColor": {
      "type": "object",
      "description": "Hair color and associated confidence",
      "properties": {
        "color": {
          "type": "string",
          "description": "Name of the hair color.",
          "x-nullable": false,
          "x-ms-enum": {
            "name": "HairColorType",
            "modelAsString": false
          },
          "enum": [
            "unknown",
            "white",
            "gray",
            "blond",
            "brown",
            "red",
            "black",
            "other"
          ]
        },
        "confidence": {
          "x-nullable": false,
          "description": "Confidence level of the color",
          "$ref": "#/definitions/Confidence"
        }
      }
    },
    "Makeup": {
      "type": "object",
      "description": "Properties describing present makeups on a given face.",
      "properties": {
        "eyeMakeup": {
          "type": "boolean",
          "description": "A boolean value describing whether eye makeup is present on a face.",
          "x-nullable": false
        },
        "lipMakeup": {
          "type": "boolean",
          "description": "A boolean value describing whether lip makeup is present on a face.",
          "x-nullable": false
        }
      }
    },
    "Occlusion": {
      "type": "object",
      "description": "Properties describing occlusions on a given face.",
      "properties": {
        "foreheadOccluded": {
          "type": "boolean",
          "description": "A boolean value indicating whether forehead is occluded.",
          "x-nullable": false
        },
        "eyeOccluded": {
          "type": "boolean",
          "description": "A boolean value indicating whether eyes are occluded.",
          "x-nullable": false
        },
        "mouthOccluded": {
          "type": "boolean",
          "description": "A boolean value indicating whether the mouth is occluded.",
          "x-nullable": false
        }
      }
    },
    "Accessories": {
      "type": "array",
      "description": "Properties describing any accessories on a given face.",
      "items": {
        "$ref": "#/definitions/Accessory"
      }
    },
    "Accessory": {
      "type": "object",
      "description": "Accessory item and corresponding confidence level.",
      "properties": {
        "type": {
          "type": "string",
          "description": "Type of an accessory",
          "x-nullable": false,
          "x-ms-enum": {
            "name": "AccessoryType",
            "modelAsString": false
          },
          "enum": [
            "headWear",
            "glasses",
            "mask"
          ]
        },
        "confidence": {
          "x-nullable": false,
          "description": "Confidence level of an accessory",
          "$ref": "#/definitions/Confidence"
        }
      }
    },
    "Blur": {
      "type": "object",
      "description": "Properties describing any presence of blur within the image.",
      "properties": {
        "blurLevel": {
          "type": "string",
          "description": "An enum value indicating level of blurriness.",
          "x-nullable": false,
          "x-ms-enum": {
            "name": "BlurLevel",
            "modelAsString": false
          },
          "enum": [
            "Low",
            "Medium",
            "High"
          ]
        },
        "value": {
          "description": "A number indicating level of blurriness ranging from 0 to 1.",
          "x-nullable": false,
          "$ref": "#/definitions/Level"
        }
      }
    },
    "Exposure": {
      "type": "object",
      "description": "Properties describing exposure level of the image.",
      "properties": {
        "exposureLevel": {
          "type": "string",
          "description": "An enum value indicating level of exposure.",
          "x-nullable": false,
          "x-ms-enum": {
            "name": "ExposureLevel",
            "modelAsString": false
          },
          "enum": [
            "UnderExposure",
            "GoodExposure",
            "OverExposure"
          ]
        },
        "value": {
          "description": "A number indicating level of exposure level ranging from 0 to 1. [0, 0.25) is under exposure. [0.25, 0.75) is good exposure. [0.75, 1] is over exposure.",
          "x-nullable": false,
          "$ref": "#/definitions/Level"
        }
      }
    },
    "Noise": {
      "type": "object",
      "description": "Properties describing noise level of the image.",
      "properties": {
        "noiseLevel": {
          "type": "string",
          "description": "An enum value indicating level of noise.",
          "x-nullable": false,
          "x-ms-enum": {
            "name": "NoiseLevel",
            "modelAsString": false
          },
          "enum": [
            "Low",
            "Medium",
            "High"
          ]
        },
        "value": {
          "description": "A number indicating level of noise level ranging from 0 to 1. [0, 0.25) is under exposure. [0.25, 0.75) is good exposure. [0.75, 1] is over exposure. [0, 0.3) is low noise level. [0.3, 0.7) is medium noise level. [0.7, 1] is high noise level.",
          "x-nullable": false,
          "$ref": "#/definitions/Level"
        }
      }
    },
    "FindSimilarRequest": {
      "type": "object",
      "required": [
        "faceId"
      ],
      "description": "Request body for find similar operation.",
      "properties": {
        "faceId": {
          "type": "string",
          "format": "uuid",
          "description": "FaceId of the query face. User needs to call Face - Detect first to get a valid faceId. Note that this faceId is not persisted and will expire 24 hours after the detection call"
        },
        "faceListId": {
          "type": "string",
          "description": "An existing user-specified unique candidate face list, created in Face List - Create a Face List. Face list contains a set of persistedFaceIds which are persisted and will never expire. Parameter faceListId, largeFaceListId and faceIds should not be provided at the same time.",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        },
        "largeFaceListId": {
          "type": "string",
          "description": "An existing user-specified unique candidate large face list, created in LargeFaceList - Create. Large face list contains a set of persistedFaceIds which are persisted and will never expire. Parameter faceListId, largeFaceListId and faceIds should not be provided at the same time.",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        },
        "faceIds": {
          "type": "array",
          "description": "An array of candidate faceIds. All of them are created by Face - Detect and the faceIds will expire 24 hours after the detection call. The number of faceIds is limited to 1000. Parameter faceListId, largeFaceListId and faceIds should not be provided at the same time.",
          "maxItems": 1000,
          "items": {
            "type": "string",
            "format": "uuid"
          }
        },
        "maxNumOfCandidatesReturned": {
          "type": "integer",
          "description": "The number of top similar faces returned. The valid range is [1, 1000].",
          "default": 20,
          "minimum": 1,
          "maximum": 1000
        },
        "mode": {
          "type": "string",
          "description": "Similar face searching mode. It can be \"matchPerson\" or \"matchFace\".",
          "default": "matchPerson",
          "x-nullable": false,
          "x-ms-enum": {
            "name": "FindSimilarMatchMode",
            "modelAsString": false
          },
          "enum": [
            "matchPerson",
            "matchFace"
          ]
        }
      }
    },
    "SimilarFaces": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/SimilarFace"
      }
    },
    "SimilarFace": {
      "type": "object",
      "required": [
        "confidence"
      ],
      "description": "Response body for find similar face operation.",
      "properties": {
        "faceId": {
          "type": "string",
          "format": "uuid",
          "description": "FaceId of candidate face when find by faceIds. faceId is created by Face - Detect and will expire 24 hours after the detection call"
        },
        "persistedFaceId": {
          "type": "string",
          "format": "uuid",
          "description": "PersistedFaceId of candidate face when find by faceListId. persistedFaceId in face list is persisted and will not expire. As showed in below response"
        },
        "confidence": {
          "description": "Similarity confidence of the candidate face. The higher confidence, the more similar. Range between [0,1].",
          "$ref": "#/definitions/Confidence"
        }
      }
    },
    "GroupRequest": {
      "type": "object",
      "required": [
        "faceIds"
      ],
      "description": "Request body for group request.",
      "properties": {
        "faceIds": {
          "type": "array",
          "description": "Array of candidate faceId created by Face - Detect. The maximum is 1000 faces",
          "maxItems": 1000,
          "items": {
            "type": "string",
            "format": "uuid",
            "x-nullable": false
          }
        }
      }
    },
    "GroupResult": {
      "type": "object",
      "required": [
        "groups"
      ],
      "description": "An array of face groups based on face similarity.",
      "properties": {
        "groups": {
          "type": "array",
          "description": "A partition of the original faces based on face similarity. Groups are ranked by number of faces",
          "items": {
            "type": "array",
            "items": {
              "type": "string",
              "format": "uuid",
              "x-nullable": false
            }
          }
        },
        "messyGroup": {
          "type": "array",
          "description": "Face ids array of faces that cannot find any similar faces from original faces.",
          "items": {
            "type": "string",
            "format": "uuid",
            "x-nullable": false
          }
        }
      }
    },
    "IdentifyRequest": {
      "type": "object",
      "required": [
        "faceIds"
      ],
      "description": "Request body for identify face operation.",
      "properties": {
        "faceIds": {
          "type": "array",
          "description": "Array of query faces faceIds, created by the Face - Detect. Each of the faces are identified independently. The valid number of faceIds is between [1, 10].",
          "maxItems": 10,
          "items": {
            "type": "string",
            "format": "uuid",
            "x-nullable": false
          }
        },
        "personGroupId": {
          "description": "PersonGroupId of the target person group, created by PersonGroup - Create. Parameter personGroupId and largePersonGroupId should not be provided at the same time.",
          "type": "string",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        },
        "largePersonGroupId": {
          "description": "LargePersonGroupId of the target large person group, created by LargePersonGroup - Create. Parameter personGroupId and largePersonGroupId should not be provided at the same time.",
          "type": "string",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        },
        "maxNumOfCandidatesReturned": {
          "type": "integer",
          "description": "The range of maxNumOfCandidatesReturned is between 1 and 5 (default is 1).",
          "default": 1,
          "minimum": 1,
          "maximum": 5
        },
        "confidenceThreshold": {
          "description": "Confidence threshold of identification, used to judge whether one face belong to one person. The range of confidenceThreshold is [0, 1] (default specified by algorithm).",
          "$ref": "#/definitions/Confidence"
        }
      }
    },
    "IdentifyResults": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/IdentifyResult"
      }
    },
    "IdentifyResult": {
      "type": "object",
      "required": [
        "faceId",
        "candidates"
      ],
      "description": "Response body for identify face operation.",
      "properties": {
        "faceId": {
          "type": "string",
          "format": "uuid",
          "description": "FaceId of the query face"
        },
        "candidates": {
          "type": "array",
          "description": "Identified person candidates for that face (ranked by confidence). Array size should be no larger than input maxNumOfCandidatesReturned. If no person is identified, will return an empty array.",
          "items": {
            "$ref": "#/definitions/IdentifyCandidate"
          }
        }
      }
    },
    "IdentifyCandidate": {
      "type": "object",
      "required": [
        "personId",
        "confidence"
      ],
      "description": "All possible faces that may qualify.",
      "properties": {
        "personId": {
          "type": "string",
          "format": "uuid",
          "description": "Id of candidate"
        },
        "confidence": {
          "description": "Confidence threshold of identification, used to judge whether one face belong to one person. The range of confidenceThreshold is [0, 1] (default specified by algorithm).",
          "$ref": "#/definitions/Confidence"
        }
      }
    },
    "VerifyFaceToPersonRequest": {
      "type": "object",
      "required": [
        "faceId",
        "personId"
      ],
      "description": "Request body for face to person verification.",
      "properties": {
        "faceId": {
          "type": "string",
          "format": "uuid",
          "description": "FaceId of the face, comes from Face - Detect"
        },
        "personGroupId": {
          "description": "Using existing personGroupId and personId for fast loading a specified person. personGroupId is created in PersonGroup - Create. Parameter personGroupId and largePersonGroupId should not be provided at the same time.",
          "type": "string",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        },
        "largePersonGroupId": {
          "description": "Using existing largePersonGroupId and personId for fast loading a specified person. largePersonGroupId is created in LargePersonGroup - Create. Parameter personGroupId and largePersonGroupId should not be provided at the same time.",
          "type": "string",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        },
        "personId": {
          "type": "string",
          "format": "uuid",
          "description": "Specify a certain person in a person group or a large person group. personId is created in PersonGroup Person - Create or LargePersonGroup Person - Create."
        }
      }
    },
    "VerifyFaceToFaceRequest": {
      "type": "object",
      "required": [
        "faceId1",
        "faceId2"
      ],
      "description": "Request body for face to face verification.",
      "properties": {
        "faceId1": {
          "type": "string",
          "format": "uuid",
          "description": "FaceId of the first face, comes from Face - Detect"
        },
        "faceId2": {
          "type": "string",
          "format": "uuid",
          "description": "FaceId of the second face, comes from Face - Detect"
        }
      }
    },
    "VerifyResult": {
      "type": "object",
      "required": [
        "isIdentical",
        "confidence"
      ],
      "description": "Result of the verify operation.",
      "properties": {
        "isIdentical": {
          "type": "boolean",
          "description": "True if the two faces belong to the same person or the face belongs to the person, otherwise false."
        },
        "confidence": {
          "description": "A number indicates the similarity confidence of whether two faces belong to the same person, or whether the face belongs to the person. By default, isIdentical is set to True if similarity confidence is greater than or equal to 0.5. This is useful for advanced users to override \"isIdentical\" and fine-tune the result on their own data.",
          "$ref": "#/definitions/Confidence"
        }
      }
    },
    "CompareFaceToFaceRequest": {
      "type": "object",
      "required": [
        "sourceImageUrl",
        "targetImageUrl"
      ],
      "description": "Request body for face to face comparison.",
      "properties": {
        "sourceImageUrl": {
          "type": "string",
          "description": "Publicly reachable URL of the source image"
        },
        "targetImageUrl": {
          "type": "string",
          "description": "Publicly reachable URL of the target image"
        }
      }
    },
    "CompareFaceToFaceFormRequest": {
      "type": "object",
      "required": [
        "source",
        "target"
      ],
      "description": "Form data with image files for face to face comparison.",
      "properties": {
        "source": {
          "type": "file",
          "description": "Source image file"
        },
        "target": {
          "type": "file",
          "description": "Target image file"
        }
      }
    },
    "CompareResult": {
      "type": "object",
      "required": [
        "targetImageFace",
        "sourceImageFace",
        "detectionModel",
        "recognitionModel"
      ],
      "description": "Result of the compare operation.",
      "properties": {
        "targetImageFace": {
          "description": "The face used for comparison from source image. along with confidence and isIdentical flag.",
          "$ref": "#/definitions/TargetImageFace"
        },
        "sourceImageFace": {
          "description": "The face used for comparison from target image.",
          "$ref": "#/definitions/SourceImageFace"
        },
        "detectionModel": {
          "description": "A number indicates the similarity confidence of whether two faces belong to the same person, or whether the face belongs to the person. By default, isIdentical is set to True if similarity confidence is greater than or equal to 0.5. This is useful for advanced users to override \"isIdentical\" and fine-tune the result on their own data.",
          "$ref": "#/definitions/DetectionModel"
        },
        "recognitionModel": {
          "description": "A number indicates the similarity confidence of whether two faces belong to the same person, or whether the face belongs to the person. By default, isIdentical is set to True if similarity confidence is greater than or equal to 0.5. This is useful for advanced users to override \"isIdentical\" and fine-tune the result on their own data.",
          "$ref": "#/definitions/RecognitionModel"
        }
      }
    },
    "TargetImageFace": {
      "type": "object",
      "required": [
        "faceRectangle",
        "confidence",
        "isIdentical"
      ],
      "description": "The face used for comparison from target image.",
      "properties": {
        "faceRectangle": {
          "$ref": "#/definitions/FaceRectangle"
        },
        "confidence": {
          "description": "A number indicates the similarity confidence two faces. By default, isIdentical is set to True if similarity confidence is greater than or equal to 0.5. This is useful for advanced users to override \"isIdentical\" and fine-tune the result on their own data.",
          "$ref": "#/definitions/Confidence"
        },
        "isIdentical": {
          "type": "boolean",
          "description": "True if the two faces belong to the same person, otherwise false."
        }
      }
    },
    "SourceImageFace": {
      "type": "object",
      "required": [
        "faceRectangle"
      ],
      "description": "The face used for comparison from source image.",
      "properties": {
        "faceRectangle": {
          "$ref": "#/definitions/FaceRectangle"
        }
      }
    },
    "FaceList": {
      "description": "Face list object.",
      "type": "object",
      "required": [
        "faceListId"
      ],
      "properties": {
        "faceListId": {
          "type": "string",
          "description": "FaceListId of the target face list.",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        },
        "persistedFaces": {
          "description": "Persisted faces within the face list.",
          "$ref": "#/definitions/PersistedFaces"
        }
      },
      "allOf": [
        {
          "$ref": "#/definitions/MetaDataContract"
        }
      ]
    },
    "FaceLists": {
      "type": "array",
      "description": "An array of face list results, with fields of faceListId, name and userData",
      "items": {
        "$ref": "#/definitions/FaceList"
      }
    },
    "PersonGroup": {
      "type": "object",
      "required": [
        "personGroupId"
      ],
      "description": "Person group object.",
      "properties": {
        "personGroupId": {
          "description": "PersonGroupId of the target person group.",
          "type": "string",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        }
      },
      "allOf": [
        {
          "$ref": "#/definitions/MetaDataContract"
        }
      ]
    },
    "PersonGroups": {
      "type": "array",
      "description": "An array of person groups.",
      "items": {
        "$ref": "#/definitions/PersonGroup"
      }
    },
    "Person": {
      "type": "object",
      "required": [
        "personId"
      ],
      "description": "Person object.",
      "properties": {
        "personId": {
          "type": "string",
          "format": "uuid",
          "description": "PersonId of the target face list."
        },
        "persistedFaceIds": {
          "type": "array",
          "description": "PersistedFaceIds of registered faces in the person. These persistedFaceIds are returned from Person - Add a Person Face, and will not expire.",
          "items": {
            "type": "string",
            "format": "uuid",
            "x-nullable": false
          }
        }
      },
      "allOf": [
        {
          "$ref": "#/definitions/NameAndUserDataContract"
        }
      ]
    },
    "Persons": {
      "type": "array",
      "description": "An array of Persons.",
      "items": {
        "$ref": "#/definitions/Person"
      }
    },
    "PersistedFace": {
      "type": "object",
      "required": [
        "persistedFaceId"
      ],
      "description": "PersonFace object.",
      "properties": {
        "persistedFaceId": {
          "type": "string",
          "format": "uuid",
          "description": "The persistedFaceId of the target face, which is persisted and will not expire. Different from faceId created by Face - Detect and will expire in 24 hours after the detection call."
        },
        "userData": {
          "type": "string",
          "description": "User-provided data attached to the face. The size limit is 1KB.",
          "maxLength": 1024
        }
      }
    },
    "PersistedFaces": {
      "type": "array",
      "description": "An array of persisted faces within the face list or large face list.",
      "items": {
        "$ref": "#/definitions/PersistedFace"
      }
    },
    "LargeFaceList": {
      "description": "Large face list object.",
      "type": "object",
      "required": [
        "largeFaceListId"
      ],
      "properties": {
        "largeFaceListId": {
          "type": "string",
          "description": "LargeFaceListId of the target large face list.",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        }
      },
      "allOf": [
        {
          "$ref": "#/definitions/MetaDataContract"
        }
      ]
    },
    "LargeFaceLists": {
      "type": "array",
      "description": "An array of large face list results, with fields of largeFaceListId, name and userData",
      "items": {
        "$ref": "#/definitions/LargeFaceList"
      }
    },
    "LargePersonGroup": {
      "type": "object",
      "required": [
        "largePersonGroupId"
      ],
      "description": "Large person group object.",
      "properties": {
        "largePersonGroupId": {
          "description": "LargePersonGroupId of the target large person groups",
          "type": "string",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        }
      },
      "allOf": [
        {
          "$ref": "#/definitions/MetaDataContract"
        }
      ]
    },
    "LargePersonGroups": {
      "type": "array",
      "description": "An array of large person groups.",
      "items": {
        "$ref": "#/definitions/LargePersonGroup"
      }
    },
    "UpdateFaceRequest": {
      "type": "object",
      "description": "Request to update face data.",
      "properties": {
        "userData": {
          "type": "string",
          "description": "User-provided data attached to the face. The size limit is 1KB.",
          "maxLength": 1024
        }
      }
    },
    "TrainingStatus": {
      "type": "object",
      "required": [
        "status",
        "createdDateTime"
      ],
      "description": "Training status object.",
      "properties": {
        "status": {
          "type": "string",
          "description": "Training status: notstarted, running, succeeded, failed. If the training process is waiting to perform, the status is notstarted. If the training is ongoing, the status is running. Status succeed means this person group or large person group is ready for Face - Identify, or this large face list is ready for Face - Find Similar. Status failed is often caused by no person or no persisted face exist in the person group or large person group, or no persisted face exist in the large face list.",
          "x-ms-enum": {
            "name": "TrainingStatusType",
            "modelAsString": false
          },
          "enum": [
            "nonstarted",
            "running",
            "succeeded",
            "failed"
          ]
        },
        "createdDateTime": {
          "type": "string",
          "format": "date-time",
          "description": "A combined UTC date and time string that describes the created time of the person group, large person group or large face list.",
          "x-ms-client-name": "created"
        },
        "lastActionDateTime": {
          "type": "string",
          "format": "date-time",
          "description": "A combined UTC date and time string that describes the last modify time of the person group, large person group or large face list, could be null value when the group is not successfully trained.",
          "x-ms-client-name": "lastAction"
        },
        "lastSuccessfulTrainingDateTime": {
          "type": "string",
          "format": "date-time",
          "description": "A combined UTC date and time string that describes the last successful training time of the person group, large person group or large face list.",
          "x-ms-client-name": "lastSuccessfulTraining"
        },
        "message": {
          "type": "string",
          "description": "Show failure message when training failed (omitted when training succeed)."
        }
      }
    },
    "Confidence": {
      "description": "A number ranging from 0 to 1 indicating a level of confidence associated with a property.",
      "type": "number",
      "minimum": 0,
      "maximum": 1
    },
    "Level": {
      "description": "A number ranging from 0 to 1 indicating the intensity level associated with a property.",
      "type": "number",
      "minimum": 0,
      "maximum": 1
    },
    "NameAndUserDataContract": {
      "type": "object",
      "description": "A combination of user defined name and user specified data for the person, largePersonGroup/personGroup, and largeFaceList/faceList.",
      "properties": {
        "name": {
          "type": "string",
          "description": "User defined name, maximum length is 128.",
          "maxLength": 128
        },
        "userData": {
          "type": "string",
          "description": "User specified data. Length should not exceed 16KB.",
          "maxLength": 16384
        }
      }
    },
    "MetaDataContract": {
      "type": "object",
      "description": "A combination of user defined name and user specified data and recognition model name for largePersonGroup/personGroup, and largeFaceList/faceList.",
      "properties": {
        "recognitionModel": {
          "$ref": "#/definitions/RecognitionModel"
        }
      },
      "allOf": [
        {
          "$ref": "#/definitions/NameAndUserDataContract"
        }
      ]
    },
    "DetectionModel": {
      "type": "string",
      "description": "Name of detection model. Detection model is used to detect faces from a given image. Dtection mdoel is returned by /compare API",
      "default": "detection_02",
      "x-nullable": false,
      "x-ms-enum": {
        "name": "DetectionModel",
        "modelAsString": true
      },
      "enum": [
        "detection_01",
        "detection_02"
      ]
    },
    "RecognitionModel": {
      "type": "string",
      "description": "Name of recognition model. Recognition model is used when the face features are extracted and associated with detected faceIds, (Large)FaceList or (Large)PersonGroup. A recognition model name can be provided when performing Face - Detect or (Large)FaceList - Create or (Large)PersonGroup - Create. The default value is 'recognition_01', if latest model needed, please explicitly specify the model you need.",
      "default": "recognition_01",
      "x-nullable": false,
      "x-ms-enum": {
        "name": "RecognitionModel",
        "modelAsString": true
      },
      "enum": [
        "recognition_01",
        "recognition_02",
        "recognition_03"
      ]
    },
    "ApplyScope": {
      "type": "array",
      "description": "Array of the target Face subscription ids for the snapshot, specified by the user who created the snapshot when calling Snapshot - Take. For each snapshot, only subscriptions included in the applyScope of Snapshot - Take can apply it.",
      "items": {
        "type": "string",
        "format": "uuid",
        "x-nullable": false
      }
    },
    "ApplySnapshotRequest": {
      "type": "object",
      "required": [
        "objectId"
      ],
      "description": "Request body for applying snapshot operation.",
      "properties": {
        "objectId": {
          "type": "string",
          "description": "User specified target object id to be created from the snapshot.",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        },
        "mode": {
          "type": "string",
          "description": "Snapshot applying mode. Currently only CreateNew is supported, which means the apply operation will fail if target subscription already contains an object of same type and using the same objectId. Users can specify the \"objectId\" in request body to avoid such conflicts.",
          "default": "CreateNew",
          "x-nullable": false,
          "x-ms-enum": {
            "name": "SnapshotApplyMode",
            "modelAsString": false
          },
          "enum": [
            "CreateNew"
          ]
        }
      }
    },
    "Snapshots": {
      "type": "array",
      "description": "An array of snapshots.",
      "items": {
        "$ref": "#/definitions/Snapshot"
      }
    },
    "Snapshot": {
      "type": "object",
      "required": [
        "id",
        "account",
        "type",
        "applyScope",
        "createdTime",
        "lastUpdateTime"
      ],
      "description": "Snapshot object.",
      "properties": {
        "id": {
          "type": "string",
          "format": "uuid",
          "description": "Snapshot id."
        },
        "account": {
          "type": "string",
          "description": "Azure Cognitive Service Face account id of the subscriber who created the snapshot by Snapshot - Take."
        },
        "type": {
          "type": "string",
          "description": "Type of the source object in the snapshot, specified by the subscriber who created the snapshot when calling Snapshot - Take. Currently FaceList, PersonGroup, LargeFaceList and LargePersonGroup are supported.",
          "x-ms-enum": {
            "name": "SnapshotObjectType",
            "modelAsString": false
          },
          "enum": [
            "FaceList",
            "LargeFaceList",
            "LargePersonGroup",
            "PersonGroup"
          ]
        },
        "applyScope": {
          "description": "Array of the target Face subscription ids for the snapshot, specified by the user who created the snapshot when calling Snapshot - Take. For each snapshot, only subscriptions included in the applyScope of Snapshot - Take can apply it.",
          "$ref": "#/definitions/ApplyScope"
        },
        "userData": {
          "type": "string",
          "description": "User specified data about the snapshot for any purpose. Length should not exceed 16KB.",
          "maxLength": 16384
        },
        "createdTime": {
          "type": "string",
          "format": "date-time",
          "description": "A combined UTC date and time string that describes the created time of the snapshot. E.g. 2018-12-25T11:41:02.2331413Z."
        },
        "lastUpdateTime": {
          "type": "string",
          "format": "date-time",
          "description": "A combined UTC date and time string that describes the last time when the snapshot was created or updated by Snapshot - Update. E.g. 2018-12-25T11:51:27.8705696Z."
        }
      }
    },
    "TakeSnapshotRequest": {
      "type": "object",
      "required": [
        "type",
        "objectId",
        "applyScope"
      ],
      "description": "Request body for taking snapshot operation.",
      "properties": {
        "type": {
          "type": "string",
          "description": "User specified type for the source object to take snapshot from. Currently FaceList, PersonGroup, LargeFaceList and LargePersonGroup are supported.",
          "x-ms-enum": {
            "name": "SnapshotObjectType",
            "modelAsString": false
          },
          "enum": [
            "FaceList",
            "LargeFaceList",
            "LargePersonGroup",
            "PersonGroup"
          ]
        },
        "objectId": {
          "type": "string",
          "description": "User specified source object id to take snapshot from.",
          "maxLength": 64,
          "pattern": "^[a-z0-9-_]+$"
        },
        "applyScope": {
          "description": "User specified array of target Face subscription ids for the snapshot. For each snapshot, only subscriptions included in the applyScope of Snapshot - Take can apply it.",
          "$ref": "#/definitions/ApplyScope"
        },
        "userData": {
          "type": "string",
          "description": "User specified data about the snapshot for any purpose. Length should not exceed 16KB.",
          "maxLength": 16384
        }
      }
    },
    "UpdateSnapshotRequest": {
      "type": "object",
      "description": "Request body for updating a snapshot, with a combination of user defined apply scope and user specified data.",
      "properties": {
        "applyScope": {
          "description": "Array of the target Face subscription ids for the snapshot, specified by the user who created the snapshot when calling Snapshot - Take. For each snapshot, only subscriptions included in the applyScope of Snapshot - Take can apply it.",
          "$ref": "#/definitions/ApplyScope"
        },
        "userData": {
          "type": "string",
          "description": "User specified data about the snapshot for any purpose. Length should not exceed 16KB.",
          "maxLength": 16384
        }
      }
    },
    "OperationStatus": {
      "type": "object",
      "required": [
        "status",
        "createdTime"
      ],
      "description": "Operation status object. Operation refers to the asynchronous backend task including taking a snapshot and applying a snapshot.",
      "properties": {
        "status": {
          "type": "string",
          "description": "Operation status: notstarted, running, succeeded, failed. If the operation is requested and waiting to perform, the status is notstarted. If the operation is ongoing in backend, the status is running. Status succeeded means the operation is completed successfully, specifically for snapshot taking operation, it illustrates the snapshot is well taken and ready to apply, and for snapshot applying operation, it presents the target object has finished creating by the snapshot and ready to be used. Status failed is often caused by editing the source object while taking the snapshot or editing the target object while applying the snapshot before completion, see the field \"message\" to check the failure reason.",
          "x-ms-enum": {
            "name": "OperationStatusType",
            "modelAsString": false
          },
          "enum": [
            "notstarted",
            "running",
            "succeeded",
            "failed"
          ]
        },
        "createdTime": {
          "type": "string",
          "format": "date-time",
          "description": "A combined UTC date and time string that describes the time when the operation (take or apply a snapshot) is requested. E.g. 2018-12-25T11:41:02.2331413Z."
        },
        "lastActionTime": {
          "type": "string",
          "format": "date-time",
          "description": "A combined UTC date and time string that describes the last time the operation (take or apply a snapshot) is actively migrating data. The lastActionTime will keep increasing until the operation finishes. E.g. 2018-12-25T11:51:27.8705696Z."
        },
        "resourceLocation": {
          "type": "string",
          "description": "When the operation succeeds successfully, for snapshot taking operation the snapshot id will be included in this field, and for snapshot applying operation, the path to get the target object will be returned in this field."
        },
        "message": {
          "type": "string",
          "description": "Show failure message when operation fails (omitted when operation succeeds)."
        }
      }
    }
  },
  "parameters": {
    "returnFaceAttributes": {
      "name": "returnFaceAttributes",
      "in": "query",
      "description": "Analyze and return the one or more specified face attributes in the comma-separated string like \"returnFaceAttributes=age,gender\". Supported face attributes include age, gender, headPose, smile, facialHair, glasses and emotion. Note that each face attribute analysis has additional computational and time cost.",
      "type": "array",
      "x-ms-parameter-location": "method",
      "required": false,
      "collectionFormat": "csv",
      "items": {
        "type": "string",
        "x-nullable": false,
        "x-ms-enum": {
          "name": "FaceAttributeType",
          "modelAsString": false
        },
        "enum": [
          "age",
          "gender",
          "headPose",
          "smile",
          "facialHair",
          "glasses",
          "emotion",
          "hair",
          "makeup",
          "occlusion",
          "accessories",
          "blur",
          "exposure",
          "noise"
        ]
      }
    },
    "targetFace": {
      "name": "targetFace",
      "in": "query",
      "description": "A face rectangle to specify the target face to be added to a person in the format of \"targetFace=left,top,width,height\". E.g. \"targetFace=10,10,100,100\". If there is more than one face in the image, targetFace is required to specify which face to add. No targetFace means there is only one face detected in the entire image.",
      "type": "array",
      "x-ms-parameter-location": "method",
      "required": false,
      "collectionFormat": "csv",
      "items": {
        "type": "integer",
        "format": "int32",
        "x-nullable": false,
        "maxItems": 4,
        "minItems": 4
      }
    },
    "faceUserData": {
      "name": "userData",
      "in": "query",
      "description": "User-specified data about the face for any purpose. The maximum length is 1KB.",
      "type": "string",
      "maxLength": 1024,
      "x-ms-parameter-location": "method",
      "required": false,
      "collectionFormat": "csv"
    },
    "faceListId": {
      "name": "faceListId",
      "in": "path",
      "description": "Id referencing a particular face list.",
      "required": true,
      "type": "string",
      "x-ms-parameter-location": "method",
      "collectionFormat": "csv",
      "maxLength": 64,
      "pattern": "^[a-z0-9-_]+$"
    },
    "personGroupId": {
      "name": "personGroupId",
      "in": "path",
      "description": "Id referencing a particular person group.",
      "required": true,
      "type": "string",
      "x-ms-parameter-location": "method",
      "collectionFormat": "csv",
      "maxLength": 64,
      "pattern": "^[a-z0-9-_]+$"
    },
    "largeFaceListId": {
      "name": "largeFaceListId",
      "in": "path",
      "description": "Id referencing a particular large face list.",
      "required": true,
      "type": "string",
      "x-ms-parameter-location": "method",
      "collectionFormat": "csv",
      "maxLength": 64,
      "pattern": "^[a-z0-9-_]+$"
    },
    "largePersonGroupId": {
      "name": "largePersonGroupId",
      "in": "path",
      "description": "Id referencing a particular large person group.",
      "required": true,
      "type": "string",
      "x-ms-parameter-location": "method",
      "collectionFormat": "csv",
      "maxLength": 64,
      "pattern": "^[a-z0-9-_]+$"
    },
    "personId": {
      "name": "personId",
      "in": "path",
      "description": "Id referencing a particular person.",
      "required": true,
      "type": "string",
      "format": "uuid",
      "x-ms-parameter-location": "method",
      "collectionFormat": "csv"
    },
    "persistedFaceId": {
      "name": "persistedFaceId",
      "in": "path",
      "description": "Id referencing a particular persistedFaceId of an existing face.",
      "required": true,
      "type": "string",
      "format": "uuid",
      "x-ms-parameter-location": "method",
      "collectionFormat": "csv"
    },
    "applyScope": {
      "name": "applyScope",
      "description": "User specified snapshot apply scopes as a search filter. ApplyScope is an array of the target Azure subscription ids for the snapshot, specified by the user who created the snapshot by Snapshot - Take.",
      "in": "query",
      "required": false,
      "type": "array",
      "x-ms-parameter-location": "method",
      "collectionFormat": "csv",
      "items": {
        "type": "string",
        "format": "uuid",
        "x-nullable": false
      }
    },
    "operationId": {
      "name": "operationId",
      "in": "path",
      "description": "Id referencing a particular take/apply snapshot operation.",
      "required": true,
      "type": "string",
      "format": "uuid",
      "x-ms-parameter-location": "method",
      "collectionFormat": "csv"
    },
    "snapshotId": {
      "name": "snapshotId",
      "in": "path",
      "description": "Id referencing a particular snapshot.",
      "required": true,
      "type": "string",
      "format": "uuid",
      "x-ms-parameter-location": "method",
      "collectionFormat": "csv"
    },
    "Endpoint": {
      "name": "Endpoint",
      "description": "Supported Cognitive Services endpoints (protocol and hostname, for example: https://westus.api.cognitive.microsoft.com).",
      "x-ms-parameter-location": "client",
      "required": true,
      "type": "string",
      "in": "path",
      "x-ms-skip-url-encoding": true
    },
    "recognitionModel": {
      "name": "recognitionModel",
      "description": "Name of recognition model. Recognition model is used when the face features are extracted and associated with detected faceIds, (Large)FaceList or (Large)PersonGroup. A recognition model name can be provided when performing Face - Detect or (Large)FaceList - Create or (Large)PersonGroup - Create. The default value is 'recognition_01', if latest model needed, please explicitly specify the model you need.",
      "default": "recognition_01",
      "required": false,
      "type": "string",
      "in": "query",
      "x-ms-parameter-location": "method",
      "x-nullable": false,
      "x-ms-enum": {
        "name": "RecognitionModel",
        "modelAsString": true
      },
      "enum": [
        "recognition_01",
        "recognition_02",
        "recognition_03"
      ]
    },
    "returnRecognitionModel": {
      "name": "returnRecognitionModel",
      "description": "A value indicating whether the operation should return 'recognitionModel' in response.",
      "default": false,
      "required": false,
      "type": "boolean",
      "in": "query",
      "x-ms-parameter-location": "method"
    },
    "detectionModel": {
      "name": "detectionModel",
      "description": "Name of detection model. Detection model is used to detect faces in the submitted image. A detection model name can be provided when performing Face - Detect or (Large)FaceList - Add Face or (Large)PersonGroup - Add Face. The default value is 'detection_01', if another model is needed, please explicitly specify it.",
      "default": "detection_01",
      "required": false,
      "type": "string",
      "in": "query",
      "x-ms-parameter-location": "method",
      "x-nullable": false,
      "x-ms-enum": {
        "name": "DetectionModel",
        "modelAsString": true
      },
      "enum": [
        "detection_01",
        "detection_02"
      ]
    }
  }
}
