openapi: 3.0.0
info:
  title: Azure OpenAI API
  version: 2023-05-15
  description: Azure OpenAI APIs for completions and search
tags: []
paths:
  /deployments/{deploymentId}/chat/completions:
    post:
      operationId: getChatCompletions
      description: >-
        Gets chat completions for the provided chat messages.

        Completions support a wide variety of tasks and generate text that continues from or "completes"

        provided prompt data.
      parameters:
        - $ref: '#/components/parameters/Azure.Core.Foundations.ApiVersionParameter'
        - name: deploymentId
          in: path
          required: true
          description: Specifies either the model deployment name (when using Azure
            OpenAI) or model name (when using non-Azure OpenAI) to use for this
            request.
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatCompletions'
        default:
          description: An unexpected error response.
          headers:
            x-ms-error-code:
              required: false
              description: String error code indicating what went wrong.
              schema:
                type: string
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Azure.Core.Foundations.ErrorResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatCompletionsOptions'
  /deployments/{deploymentId}/completions:
    post:
      operationId: getCompletions
      description: >-
        Gets completions for the provided input prompts.

        Completions support a wide variety of tasks and generate text that continues from or "completes"

        provided prompt data.
      parameters:
        - $ref: '#/components/parameters/Azure.Core.Foundations.ApiVersionParameter'
        - name: deploymentId
          in: path
          required: true
          description: Specifies either the model deployment name (when using Azure
            OpenAI) or model name (when using non-Azure OpenAI) to use for this
            request.
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Completions'
        default:
          description: An unexpected error response.
          headers:
            x-ms-error-code:
              required: false
              description: String error code indicating what went wrong.
              schema:
                type: string
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Azure.Core.Foundations.ErrorResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompletionsOptions'
  /deployments/{deploymentId}/embeddings:
    post:
      operationId: getEmbeddings
      description: Return the embeddings for a given prompt.
      parameters:
        - $ref: '#/components/parameters/Azure.Core.Foundations.ApiVersionParameter'
        - name: deploymentId
          in: path
          required: true
          description: Specifies either the model deployment name (when using Azure
            OpenAI) or model name (when using non-Azure OpenAI) to use for this
            request.
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Embeddings'
        default:
          description: An unexpected error response.
          headers:
            x-ms-error-code:
              required: false
              description: String error code indicating what went wrong.
              schema:
                type: string
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Azure.Core.Foundations.ErrorResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/EmbeddingsOptions'
security:
  - ApiKeyAuth: []
  - OAuth2Auth:
      - https://cognitiveservices.azure.com/.default
components:
  parameters:
    Azure.Core.Foundations.ApiVersionParameter:
      name: api-version
      in: query
      required: true
      description: The API version to use for this operation.
      schema:
        type: string
        minLength: 1
  schemas:
    Azure.Core.Foundations.Error:
      type: object
      description: The error object.
      required:
        - code
        - message
      properties:
        code:
          type: string
          description: One of a server-defined set of error codes.
        message:
          type: string
          description: A human-readable representation of the error.
        target:
          type: string
          description: The target of the error.
        details:
          type: array
          items:
            $ref: '#/components/schemas/Azure.Core.Foundations.Error'
          description: An array of details about specific errors that led to this reported
            error.
        innererror:
          allOf:
            - $ref: '#/components/schemas/Azure.Core.Foundations.InnerError'
          description: An object containing more specific information than the current
            object about the error.
    Azure.Core.Foundations.ErrorResponse:
      type: object
      description: A response containing error details.
      required:
        - error
      properties:
        error:
          allOf:
            - $ref: '#/components/schemas/Azure.Core.Foundations.Error'
          description: The error object.
    Azure.Core.Foundations.InnerError:
      type: object
      description: An object containing more specific information about the error. As
        per Microsoft One API guidelines -
        https://github.com/Microsoft/api-guidelines/blob/vNext/Guidelines.md#7102-error-condition-responses.
      properties:
        code:
          type: string
          description: One of a server-defined set of error codes.
        innererror:
          allOf:
            - $ref: '#/components/schemas/Azure.Core.Foundations.InnerError'
          description: Inner error.
    AzureOpenAIOperationState:
      type: string
      description: The state of a job or item.
      enum:
        - notRunning
        - running
        - succeeded
        - canceled
        - failed
    ChatChoice:
      type: object
      description: >-
        The representation of a single prompt completion as part of an overall
        chat completions request.

        Generally, `n` choices are generated per provided prompt with a default value of 1.

        Token limits and other settings may limit the number of choices generated.
      required:
        - index
        - finish_reason
      properties:
        message:
          allOf:
            - $ref: '#/components/schemas/ChatMessage'
          description: The chat message for a given chat completions prompt.
        index:
          type: integer
          format: int32
          description: The ordered index associated with this chat completions choice.
        finish_reason:
          oneOf:
            - $ref: '#/components/schemas/CompletionsFinishReason'
          nullable: true
          description: The reason that this chat completions choice completed its generated.
        delta:
          allOf:
            - $ref: '#/components/schemas/ChatMessage'
          description: The delta message content for a streaming response.
    ChatCompletions:
      type: object
      description: >-
        Representation of the response data from a chat completions request.

        Completions support a wide variety of tasks and generate text that continues from or "completes"

        provided prompt data.
      required:
        - id
        - created
        - choices
        - usage
      properties:
        id:
          type: string
          description: A unique identifier associated with this chat completions response.
        created:
          type: integer
          format: unixtime
          description: >-
            The first timestamp associated with generation activity for this
            completions response,

            represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.
        choices:
          type: array
          items:
            $ref: '#/components/schemas/ChatChoice'
          description: >-
            The collection of completions choices associated with this
            completions response.

            Generally, `n` choices are generated per provided prompt with a default value of 1.

            Token limits and other settings may limit the number of choices generated.
        usage:
          allOf:
            - $ref: '#/components/schemas/CompletionsUsage'
          description: Usage information for tokens processed and generated as part of
            this completions operation.
    ChatCompletionsOptions:
      type: object
      description: >-
        The configuration information for a chat completions request.

        Completions support a wide variety of tasks and generate text that continues from or "completes"

        provided prompt data.
      required:
        - messages
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/ChatMessage'
          description: >-
            The collection of context messages associated with this chat
            completions request.

            Typical usage begins with a chat message for the System role that provides instructions for

            the behavior of the assistant, followed by alternating messages between the User and

            Assistant roles.
        max_tokens:
          type: integer
          format: int32
          description: The maximum number of tokens to generate.
        temperature:
          type: number
          format: float
          description: >-
            The sampling temperature to use that controls the apparent
            creativity of generated completions.

            Higher values will make output more random while lower values will make results more focused

            and deterministic.

            It is not recommended to modify temperature and top_p for the same completions request as the

            interaction of these two settings is difficult to predict.
        top_p:
          type: number
          format: float
          description: >-
            An alternative to sampling with temperature called nucleus sampling.
            This value causes the

            model to consider the results of tokens with the provided probability mass. As an example, a

            value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be

            considered.

            It is not recommended to modify temperature and top_p for the same completions request as the

            interaction of these two settings is difficult to predict.
        logit_bias:
          type: object
          description: >-
            A map between GPT token IDs and bias scores that influences the
            probability of specific tokens

            appearing in a completions response. Token IDs are computed via external tokenizer tools, while

            bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to

            a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias

            score varies by model.
          additionalProperties:
            type: integer
            format: int32
        user:
          type: string
          description: >-
            An identifier for the caller or end user of the operation. This may
            be used for tracking

            or rate-limiting purposes.
        n:
          type: integer
          format: int32
          description: >-
            The number of chat completions choices that should be generated for
            a chat completions

            response.

            Because this setting can generate many completions, it may quickly consume your token quota.

            Use carefully and ensure reasonable settings for max_tokens and stop.
        stop:
          type: array
          items:
            type: string
          description: A collection of textual sequences that will end completions
            generation.
        presence_penalty:
          type: number
          format: float
          description: >-
            A value that influences the probability of generated tokens
            appearing based on their existing

            presence in generated text.

            Positive values will make tokens less likely to appear when they already exist and increase the

            model's likelihood to output new topics.
        frequency_penalty:
          type: number
          format: float
          description: >-
            A value that influences the probability of generated tokens
            appearing based on their cumulative

            frequency in generated text.

            Positive values will make tokens less likely to appear as their frequency increases and

            decrease the likelihood of the model repeating the same statements verbatim.
        stream:
          type: boolean
          description: A value indicating whether chat completions should be streamed for
            this request.
        model:
          type: string
          description: >-
            The model name to provide as part of this completions request.

            Not applicable to Azure OpenAI, where deployment information should be included in the Azure

            resource URI that's connected to.
    ChatMessage:
      type: object
      description: A single, role-attributed message within a chat completion interaction.
      required:
        - role
        - content
      properties:
        role:
          allOf:
            - $ref: '#/components/schemas/ChatRole'
          description: The role associated with this message payload.
        content:
          type: string
          nullable: true
          description: The text associated with this message payload.
    ChatRole:
      type: string
      description: A description of the intended purpose of a message within a chat
        completions interaction.
      enum:
        - system
        - assistant
        - user
    Choice:
      type: object
      description: >-
        The representation of a single prompt completion as part of an overall
        completions request.

        Generally, `n` choices are generated per provided prompt with a default value of 1.

        Token limits and other settings may limit the number of choices generated.
      required:
        - text
        - index
        - logprobs
        - finish_reason
      properties:
        text:
          type: string
          description: The generated text for a given completions prompt.
        index:
          type: integer
          format: int32
          description: The ordered index associated with this completions choice.
        logprobs:
          type: object
          allOf:
            - $ref: '#/components/schemas/CompletionsLogProbabilityModel'
          nullable: true
          description: The log probabilities model for tokens associated with this
            completions choice.
        finish_reason:
          oneOf:
            - $ref: '#/components/schemas/CompletionsFinishReason'
          nullable: true
          description: Reason for finishing
    Completions:
      type: object
      description: >-
        Representation of the response data from a completions request.

        Completions support a wide variety of tasks and generate text that continues from or "completes"

        provided prompt data.
      required:
        - id
        - created
        - choices
        - usage
      properties:
        id:
          type: string
          description: A unique identifier associated with this completions response.
        created:
          type: integer
          format: unixtime
          description: >-
            The first timestamp associated with generation activity for this
            completions response,

            represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.
        choices:
          type: array
          items:
            $ref: '#/components/schemas/Choice'
          description: >-
            The collection of completions choices associated with this
            completions response.

            Generally, `n` choices are generated per provided prompt with a default value of 1.

            Token limits and other settings may limit the number of choices generated.
        usage:
          allOf:
            - $ref: '#/components/schemas/CompletionsUsage'
          description: Usage information for tokens processed and generated as part of
            this completions operation.
    CompletionsFinishReason:
      type: string
      description: Representation of the manner in which a completions response concluded.
      enum:
        - stop
        - length
        - content_filter
    CompletionsLogProbabilityModel:
      type: object
      description: Representation of a log probabilities model for a completions generation.
      required:
        - tokens
        - token_logprobs
        - top_logprobs
        - text_offset
      properties:
        tokens:
          type: array
          items:
            type: string
          description: The textual forms of tokens evaluated in this probability model.
        token_logprobs:
          type: array
          items:
            type: number
            format: float
            nullable: true
          description: A collection of log probability values for the tokens in this
            completions data.
        top_logprobs:
          type: array
          items:
            type: object
            additionalProperties:
              type: number
              format: float
              nullable: true
          description: A mapping of tokens to maximum log probability values in this
            completions data.
        text_offset:
          type: array
          items:
            type: integer
            format: int32
          description: The text offsets associated with tokens in this completions data.
    CompletionsOptions:
      type: object
      description: >-
        The configuration information for a completions request.

        Completions support a wide variety of tasks and generate text that continues from or "completes"

        provided prompt data.
      required:
        - prompt
      properties:
        prompt:
          type: array
          items:
            type: string
          description: The prompts to generate completions from.
        max_tokens:
          type: integer
          format: int32
          description: The maximum number of tokens to generate.
        temperature:
          type: number
          format: float
          description: >-
            The sampling temperature to use that controls the apparent
            creativity of generated completions.

            Higher values will make output more random while lower values will make results more focused

            and deterministic.

            It is not recommended to modify temperature and top_p for the same completions request as the

            interaction of these two settings is difficult to predict.
        top_p:
          type: number
          format: float
          description: >-
            An alternative to sampling with temperature called nucleus sampling.
            This value causes the

            model to consider the results of tokens with the provided probability mass. As an example, a

            value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be

            considered.

            It is not recommended to modify temperature and top_p for the same completions request as the

            interaction of these two settings is difficult to predict.
        logit_bias:
          type: object
          description: >-
            A map between GPT token IDs and bias scores that influences the
            probability of specific tokens

            appearing in a completions response. Token IDs are computed via external tokenizer tools, while

            bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to

            a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias

            score varies by model.
          additionalProperties:
            type: integer
            format: int32
        user:
          type: string
          description: >-
            An identifier for the caller or end user of the operation. This may
            be used for tracking

            or rate-limiting purposes.
        n:
          type: integer
          format: int32
          description: >-
            The number of completions choices that should be generated per
            provided prompt as part of an

            overall completions response.

            Because this setting can generate many completions, it may quickly consume your token quota.

            Use carefully and ensure reasonable settings for max_tokens and stop.
        logprobs:
          type: integer
          format: int32
          description: >-
            A value that controls the emission of log probabilities for the
            provided number of most likely

            tokens within a completions response.
        echo:
          type: boolean
          description: >-
            A value specifying whether completions responses should include
            input prompts as prefixes to

            their generated output.
        stop:
          type: array
          items:
            type: string
          description: A collection of textual sequences that will end completions
            generation.
        presence_penalty:
          type: number
          format: float
          description: >-
            A value that influences the probability of generated tokens
            appearing based on their existing

            presence in generated text.

            Positive values will make tokens less likely to appear when they already exist and increase the

            model's likelihood to output new topics.
        frequency_penalty:
          type: number
          format: float
          description: >-
            A value that influences the probability of generated tokens
            appearing based on their cumulative

            frequency in generated text.

            Positive values will make tokens less likely to appear as their frequency increases and

            decrease the likelihood of the model repeating the same statements verbatim.
        best_of:
          type: integer
          format: int32
          description: >-
            A value that controls how many completions will be internally
            generated prior to response

            formulation.

            When used together with n, best_of controls the number of candidate completions and must be

            greater than n.

            Because this setting can generate many completions, it may quickly consume your token quota.

            Use carefully and ensure reasonable settings for max_tokens and stop.
        stream:
          type: boolean
          description: A value indicating whether chat completions should be streamed for
            this request.
        model:
          type: string
          description: >-
            The model name to provide as part of this completions request.

            Not applicable to Azure OpenAI, where deployment information should be included in the Azure

            resource URI that's connected to.
    CompletionsUsage:
      type: object
      description: >-
        Representation of the token counts processed for a completions request.

        Counts consider all tokens across prompts, choices, choice alternates, best_of generations, and

        other consumers.
      required:
        - completion_tokens
        - prompt_tokens
        - total_tokens
      properties:
        completion_tokens:
          type: integer
          format: int32
          description: The number of tokens generated across all completions emissions.
        prompt_tokens:
          type: integer
          format: int32
          description: The number of tokens in the provided prompts for the completions
            request.
        total_tokens:
          type: integer
          format: int32
          description: The total number of tokens processed for the completions request
            and response.
    Deployment:
      type: object
      description: A specific deployment
      required:
        - deploymentId
      properties:
        deploymentId:
          type: string
          description: Specifies either the model deployment name (when using Azure
            OpenAI) or model name (when using non-Azure OpenAI) to use for this
            request.
          readOnly: true
    EmbeddingItem:
      type: object
      description: Representation of a single embeddings relatedness comparison.
      required:
        - embedding
        - index
      properties:
        embedding:
          type: array
          items:
            type: number
            format: float
          description: >-
            List of embeddings value for the input prompt. These represent a
            measurement of the

            vector-based relatedness of the provided input.
        index:
          type: integer
          format: int32
          description: Index of the prompt to which the EmbeddingItem corresponds.
    Embeddings:
      type: object
      description: >-
        Representation of the response data from an embeddings request.

        Embeddings measure the relatedness of text strings and are commonly used for search, clustering,

        recommendations, and other similar scenarios.
      required:
        - data
        - usage
      properties:
        data:
          type: array
          items:
            $ref: '#/components/schemas/EmbeddingItem'
          description: Embedding values for the prompts submitted in the request.
        usage:
          allOf:
            - $ref: '#/components/schemas/EmbeddingsUsage'
          description: Usage counts for tokens input using the embeddings API.
    EmbeddingsOptions:
      type: object
      description: >-
        The configuration information for an embeddings request.

        Embeddings measure the relatedness of text strings and are commonly used for search, clustering,

        recommendations, and other similar scenarios.
      required:
        - input
      properties:
        user:
          type: string
          description: >-
            An identifier for the caller or end user of the operation. This may
            be used for tracking

            or rate-limiting purposes.
        model:
          type: string
          description: >-
            The model name to provide as part of this embeddings request.

            Not applicable to Azure OpenAI, where deployment information should be included in the Azure

            resource URI that's connected to.
        input:
          type: array
          items:
            type: string
          description: >-
            Input texts to get embeddings for, encoded as a an array of strings.

            Each input must not exceed 2048 tokens in length.


            Unless you are embedding code, we suggest replacing newlines (\n) in your input with a single space,

            as we have observed inferior results when newlines are present.
    EmbeddingsUsage:
      type: object
      description: Measurement of the amount of tokens used in this request and response.
      required:
        - prompt_tokens
        - total_tokens
      properties:
        prompt_tokens:
          type: integer
          format: int32
          description: Number of tokens sent in the original request.
        total_tokens:
          type: integer
          format: int32
          description: Total number of tokens transacted in this request/response.
    ServiceApiVersions:
      type: string
      enum:
        - 2022-12-01
        - 2023-05-15
        - 2023-06-01-preview
        - 2023-07-01-preview
        - 2023-08-01-preview
        - 2023-09-01-preview
  securitySchemes:
    ApiKeyAuth:
      type: apiKey
      in: header
      name: api-key
    OAuth2Auth:
      type: oauth2
      flows:
        implicit:
          authorizationUrl: https://login.microsoftonline.com/common/oauth2/v2.0/authorize
          scopes:
            https://cognitiveservices.azure.com/.default: ''
servers:
  - url: '{endpoint}/openai'
    description: Azure OpenAI APIs for completions and search
    variables:
      endpoint:
        default: ''
        description: >-
          Supported Cognitive Services endpoints (protocol and hostname, for
          example:

          https://westus.api.cognitive.microsoft.com).
