{
 "swagger": "2.0",
 "info": {
  "version": "1.0",
  "title": "Computer Vision",
  "description": "The Computer Vision API provides state-of-the-art algorithms to process images and return information. For example, it can be used to determine if an image contains mature content, or it can be used to find all the faces in an image.  It also has other features like estimating dominant and accent colors, categorizing the content of images, and describing an image with complete English sentences.  Additionally, it can also intelligently generate images thumbnails for displaying large images effectively."
 },
 "x-ms-parameterized-host": {
  "hostTemplate": "{azureRegion}.api.cognitive.microsoft.com",
  "parameters": [
   {
    "$ref": "#/parameters/AzureRegion"
   }
  ]
 },
 "basePath": "/vision/v1.0",
 "schemes": [
  "https"
 ],
 "paths": {
  "/models": {
   "get": {
    "description": "This operation returns the list of domain-specific models that are supported by the Computer Vision API.  Currently, the API only supports one domain-specific model: a celebrity recognizer. A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.",
    "operationId": "List Domain Specific Models",
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     }
    ],
    "responses": {
     "200": {
      "schema": {
       "$ref": "#/definitions/ListModelsResult"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/analyze?overload=url": {
   "post": {
    "description": "This operation extracts a rich set of visual features based on the image content. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL.  Within your request, there is an optional parameter to allow you to choose which features to return.  By default, image categories are returned in the response.",
    "operationId": "Analyze Image",
    "consumes": [
     "application/json"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "name": "visualFeatures",
      "in": "query",
      "description": "A string indicating what visual feature types to return. Multiple values should be comma-separated. Valid visual feature types include:Categories - categorizes image content according to a taxonomy defined in documentation. Tags - tags the image with a detailed list of words related to the image content. Description - describes the image content with a complete English sentence. Faces - detects if faces are present. If present, generate coordinates, gender and age. ImageType - detects if image is clipart or a line drawing. Color - determines the accent color, dominant color, and whether an image is black&white.Adult - detects if the image is pornographic in nature (depicts nudity or a sex act).  Sexually suggestive content is also detected.",
      "type": "array",
      "required": false,
      "collectionFormat": "csv",
      "items": {
       "type": "string",
       "enum": [
        "ImageType",
        "Faces",
        "Adult",
        "Categories",
        "Color",
        "Tags",
        "Description"
       ]
      }
     },
     {
      "name": "details",
      "in": "query",
      "description": "A string indicating which domain-specific details to return. Multiple values should be comma-separated. Valid visual feature types include:Celebrities - identifies celebrities if detected in the image.",
      "type": "string",
      "required": false,
      "enum": [
       "Celebrities"
      ]
     },
     {
      "name": "language",
      "in": "query",
      "description": "A string indicating which language to return. The service will return recognition results in specified language. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default.zh - Simplified Chinese.",
      "type": "string",
      "required": false,
      "default": "en",
      "enum": [
       "en",
       "zh"
      ]
     },
     {
      "$ref": "#/parameters/ImageUrl"
     }
    ],
    "responses": {
     "200": {
      "description": "The response include the extracted features in JSON format.Here is the definitions for enumeration typesClipartTypeNon-clipart = 0,  ambiguous = 1, normal-clipart = 2, good-clipart = 3.LineDrawingTypeNon-LineDrawing = 0,LineDrawing = 1.",
      "schema": {
       "$ref": "#/definitions/ImageAnalysis"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/generateThumbnail": {
   "post": {
    "description": "This operation generates a thumbnail image with the user-specified width and height. By default, the service analyzes the image, identifies the region of interest (ROI), and generates smart cropping coordinates based on the ROI. Smart cropping helps when you specify an aspect ratio that differs from that of the input image. A successful response contains the thumbnail image binary. If the request failed, the response contains an error code and a message to help determine what went wrong.",
    "operationId": "GenerateThumbnail",
    "consumes": [
     "application/json"
    ],
    "produces": [
     "application/octet-stream",
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "name": "width",
      "type": "integer",
      "in": "query",
      "required": true,
      "minimum": 1,
      "maximum": 1023,
      "description": "Width of the thumbnail. It must be between 1 and 1024. Recommended minimum of 50."
     },
     {
      "name": "height",
      "type": "integer",
      "in": "query",
      "required": true,
      "minimum": 1,
      "maximum": 1023,
      "description": "Height of the thumbnail. It must be between 1 and 1024. Recommended minimum of 50."
     },
     {
      "$ref": "#/parameters/ImageUrl"
     },
     {
      "name": "smartCropping",
      "type": "boolean",
      "in": "query",
      "required": false,
      "default": false,
      "description": "Boolean flag for enabling smart cropping."
     }
    ],
    "responses": {
     "200": {
      "schema": {
       "type": "file"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/ocr": {
   "post": {
    "description": "Optical Character Recognition (OCR) detects printed text in an image and extracts the recognized characters into a machine-usable character stream.   Upon success, the OCR results will be returned. Upon failure, the error code together with an error message will be returned. The error code can be one of InvalidImageUrl, InvalidImageFormat, InvalidImageSize, NotSupportedImage,  NotSupportedLanguage, or InternalServerError.",
    "operationId": "RecognizePrintedText",
    "consumes": [
     "application/json"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "$ref": "#/parameters/DetectOrientation"
     },
     {
      "$ref": "#/parameters/ImageUrl"
     },
     {
      "$ref": "#/parameters/Language"
     }
    ],
    "responses": {
     "200": {
      "description": "The OCR results in the hierarchy of region/line/word. The results include text, bounding box for regions, lines and words.textAngleThe angle, in degrees, of the detected text with respect to the closest horizontal or vertical direction. After rotating the input image clockwise by this angle, the recognized text lines become horizontal or vertical. In combination with the orientation property it can be used to overlay recognition results correctly on the original image, by rotating either the original image or recognition results by a suitable angle around the center of the original image. If the angle cannot be confidently detected, this property is not present. If the image contains text at different angles, only part of the text will be recognized correctly.<img src=\"https://oxfordportal.blob.core.windows.net/vision/doc-vision-overview-ocr01.png\"/>orientationOrientation of the text recognized in the image. The value (up,down,left, or right) refers to the direction that the top of the recognized text is facing, after the image has been rotated around its center according to the detected text angle (see textAngle property).languageThe BCP-47 language code (user-provided or auto-detected) of the text detected in the image.regionsAn array of objects, where each object represents a region of recognized text. A region consists of multiple lines (e.g. a column of text in a multi-column document).linesAn array of objects, where each object represents a line of recognized text.wordsAn array of objects, where each object represents a recognized word.boundingBoxBounding box of a recognized region, line, or word, depending on the parent object. The four integers represent the x-coordinate of the left edge, the y-coordinate of the top edge, width, and height of the bounding box, in the coordinate system of the input image, after it has been rotated around its center according to the detected text angle (see textAngle property), with the origin at the top-left corner, and the y-axis pointing down.textString value of a recognized word.",
      "schema": {
       "$ref": "#/definitions/OcrResult"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/describe": {
   "post": {
    "description": "This operation generates a description of an image in human readable language with complete sentences.  The description is based on a collection of content tags, which are also returned by the operation. More than one description can be generated for each image.  Descriptions are ordered by their confidence score. All descriptions are in English. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL.A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.",
    "operationId": "DescribeImage",
    "consumes": [
     "application/json"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "name": "maxCandidates",
      "in": "query",
      "description": "Maximum number of candidate descriptions to be returned.  The default is 1.",
      "type": "string",
      "required": false,
      "default": "1"
     },
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "$ref": "#/parameters/ImageUrl"
     }
    ],
    "responses": {
     "200": {
      "schema": {
       "$ref": "#/definitions/ImageDescription"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/tag": {
   "post": {
    "description": "This operation generates a list of words, or tags, that are relevant to the content of the supplied image. The Computer Vision API can return tags based on objects, living beings, scenery or actions found in images. Unlike categories, tags are not organized according to a hierarchical classification system, but correspond to image content. Tags may contain hints to avoid ambiguity or provide context, for example the tag “cello” may be accompanied by the hint “musical instrument”. All tags are in English.",
    "operationId": "TagImage",
    "consumes": [
     "application/json"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "$ref": "#/parameters/ImageUrl"
     }
    ],
    "responses": {
     "200": {
      "schema": {
       "$ref": "#/definitions/TagResult"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/models/{model}/analyze": {
   "post": {
    "description": "This operation recognizes content within an image by applying a domain-specific model.  The list of domain-specific models that are supported by the Computer Vision API can be retrieved using the /models GET request.  Currently, the API only provides a single domain-specific model: celebrities. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL. A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.",
    "operationId": "Recognize Domain Specific Content",
    "consumes": [
     "application/json"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "name": "model",
      "in": "path",
      "description": "The domain-specific content to recognize.",
      "required": true,
      "type": "string"
     },
     {
      "$ref": "#/parameters/ImageUrl"
     }
    ],
    "responses": {
     "200": {
      "schema": {
       "$ref": "#/definitions/DomainModelResults"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/recognizeText": {
   "post": {
    "description": "Recognize Text operation. When you use the Recognize Text interface, the response contains a field called “Operation-Location”. The “Operation-Location” field contains the URL that you must use for your Get Handwritten Text Operation Result operation.\r\n<br>\r\n<br>\r\nFor the result of a Recognize Handwritten Text operation to be available, it requires an amount of time that depends on the length of the text. So, you may need to wait before using this Get Handwritten Text Operation Result interface. The time you need to wait may be up to a number of seconds.\r\n<br>\r\n<br>\r\nNote: this technology is currently in preview and is only available for English text.",
    "operationId": "RecognizeText",
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "$ref": "#/parameters/ImageUrl"
     },
     {
      "$ref": "#/parameters/HandwritingBoolean"
     }
    ],
    "consumes": [
     "application/json"
    ],
    "produces": [
     "application/json"
    ],
    "responses": {
     "202": {
      "description": "The service has accepted the request and will start processing later.\r\n<br>\r\nIt will return Accepted immediately and include an <b>“Operation-Location”</b> header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.\r\n<br>\r\n<table class=\"element table\">\r\n            <thead>\r\n            </thead>\r\n            <tbody>\r\n            <tr><td><b>Operation-Location</b></td><td>Client side should use this URL to query operation status/result. <br/> Example: https://cognitiveservice/vision/v1.0/textOperations/49a36324-fc4b-4387-aa06-090cfbf0064f\r\n.</td></tr>\r\n             </tbody>\r\n</table>",
      "headers": {
       "Operation-Location": {
        "description": "URL to query for status of the operation. The operation ID will expire in 48 hours. ",
        "type": "string"
       }
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/textOperations/{operationId}": {
   "get": {
    "description": "This interface is used for getting text operation result. The URL to this interface should be retrieved from <b>“Operation-Location”</b> field returned from Recognize Text interface.",
    "operationId": "GetTextOperationResult",
    "parameters": [
     {
      "name": "operationId",
      "in": "path",
      "description": "Id of the text operation returned in the response of the <a href=\"/docs/services/56f91f2d778daf23d8ec6739/operations/587f2c6a154055056008f200\">Recognize Handwritten Text</a> interface.",
      "required": true,
      "type": "string"
     },
     {
      "$ref": "#/parameters/SubscriptionKey"
     }
    ],
    "produces": [
     "application/json"
    ],
    "responses": {
     "200": {
      "description": "Returns the operation status. Possible values of \"status\" field are:\r\n      <table class=\"element table\">\r\n      <thead>\r\n      <tr><th>Status Code</th><th>Description</th></tr>\r\n      </thead>\r\n      <tbody>\r\n      <tr><td>Not started</td><td>The text recognition process has not started.</td></tr>\r\n      <tr><td>Running</td><td>The text recognition is being processed.</td></tr> \r\n\t  <tr><td>Failed</td><td>The text recognition process failed.</td></tr>\r\n\t  <tr><td>Succeeded</td><td>The text recognition process succeeded.</td></tr>\r\n\t  </tbody>\r\n      </table>\r\n\r\n\r\nIf the status is \"Succeeded\", the response JSON will include the text recognition result. The result is organized in the hierarchy of Line/Word/Text. Result fields include lines, words, bounding box and text:\r\n<br>\r\n<br><b>Lines</b>\r\n<br>\r\nAn array of objects, where each object represents a line of recognized text.\r\n<br><b>Words</b>\r\n<br>\r\nAn array of objects, where each object represents a recognized word.\r\n<br><b>BoundingBox</b>\r\n<br>\r\nBounding box of a recognized region, line, or word, depending on the parent object. The eight integers represent the four points (x-coordinate, y-coordinate) of the detected rectangle from the left-top corner and clockwise. \r\n<br><b>Text</b>\r\n<br>\r\nString value of a recognized word/line.\r\n<br>",
      "schema": {
       "$ref": "#/definitions/TextOperationResult"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  }
 },
 "x-ms-paths": {
  "/analyze?overload=stream": {
   "post": {
    "description": "This operation extracts a rich set of visual features based on the image content.                         Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL.  Within your request, there is an optional parameter to allow you to choose which features to return.  By default, image categories are returned in the response. A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong. ",
    "operationId": "Analyze Image In Stream",
    "consumes": [
     "application/octet-stream"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "name": "visualFeatures",
      "in": "query",
      "description": "A string indicating what visual feature types to return. Multiple values should be comma-separated. Valid visual feature types include:Categories - categorizes image content according to a taxonomy defined in documentation. Tags - tags the image with a detailed list of words related to the image content. Description - describes the image content with a complete English sentence. Faces - detects if faces are present. If present, generate coordinates, gender and age. ImageType - detects if image is clipart or a line drawing. Color - determines the accent color, dominant color, and whether an image is black&white.Adult - detects if the image is pornographic in nature (depicts nudity or a sex act).  Sexually suggestive content is also detected.",
      "type": "array",
      "required": false,
      "collectionFormat": "csv",
      "items": {
       "type": "string",
       "enum": [
        "ImageType",
        "Faces",
        "Adult",
        "Categories",
        "Color",
        "Tags",
        "Description"
       ]
      }
     },
     {
      "name": "details",
      "in": "query",
      "description": "A string indicating which domain-specific details to return. Multiple values should be comma-separated. Valid visual feature types include:Celebrities - identifies celebrities if detected in the image.",
      "type": "string",
      "required": false,
      "enum": [
       "Celebrities"
      ]
     },
     {
      "name": "language",
      "in": "query",
      "description": "A string indicating which language to return. The service will return recognition results in specified language. If this parameter is not specified, the default value is &quot;en&quot;.Supported languages:en - English, Default.zh - Simplified Chinese.",
      "type": "string",
      "required": false,
      "default": "en",
      "enum": [
       "en",
       "zh"
      ]
     },
     {
      "$ref": "#/parameters/ImageStream"
     }
    ],
    "responses": {
     "200": {
      "description": "The response include the extracted features in JSON format. Here is the definitions for enumeration types clipart = 0, ambiguous = 1, normal-clipart = 2, good-clipart = 3. Non-LineDrawing = 0,LineDrawing = 1.",
      "schema": {
       "$ref": "#/definitions/ImageAnalysis"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/generateThumbnail?overload=stream": {
   "post": {
    "description": "This operation generates a thumbnail image with the user-specified width and height. By default, the service analyzes the image, identifies the region of interest (ROI), and generates smart cropping coordinates based on the ROI. Smart cropping helps when you specify an aspect ratio that differs from that of the input image. A successful response contains the thumbnail image binary. If the request failed, the response contains an error code and a message to help determine what went wrong.",
    "operationId": "GenerateThumbnailFromStream",
    "consumes": [
     "application/octet-stream"
    ],
    "produces": [
     "application/octet-stream",
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "name": "width",
      "type": "integer",
      "in": "query",
      "required": true,
      "minimum": 1,
      "maximum": 1023,
      "description": "Width of the thumbnail. It must be between 1 and 1024. Recommended minimum of 50."
     },
     {
      "name": "height",
      "type": "integer",
      "in": "query",
      "required": true,
      "minimum": 1,
      "maximum": 1023,
      "description": "Height of the thumbnail. It must be between 1 and 1024. Recommended minimum of 50."
     },
     {
      "$ref": "#/parameters/ImageStream"
     },
     {
      "name": "smartCropping",
      "type": "boolean",
      "in": "query",
      "required": false,
      "default": false,
      "description": "Boolean flag for enabling smart cropping."
     }
    ],
    "responses": {
     "200": {
      "schema": {
       "type": "file"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/ocr?overload=stream": {
   "post": {
    "description": "Optical Character Recognition (OCR) detects printed text in an image and extracts the recognized characters into a machine-usable character stream.   Upon success, the OCR results will be returned. Upon failure, the error code together with an error message will be returned. The error code can be one of InvalidImageUrl, InvalidImageFormat, InvalidImageSize, NotSupportedImage,  NotSupportedLanguage, or InternalServerError.",
    "operationId": "RecognizePrintedTextInStream",
    "consumes": [
     "application/octet-stream"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "$ref": "#/parameters/Language"
     },
     {
      "$ref": "#/parameters/DetectOrientation"
     },
     {
      "$ref": "#/parameters/ImageStream"
     }
    ],
    "responses": {
     "200": {
      "description": "The OCR results in the hierarchy of region/line/word. The results include text, bounding box for regions, lines and words.textAngleThe angle, in degrees, of the detected text with respect to the closest horizontal or vertical direction. After rotating the input image clockwise by this angle, the recognized text lines become horizontal or vertical. In combination with the orientation property it can be used to overlay recognition results correctly on the original image, by rotating either the original image or recognition results by a suitable angle around the center of the original image. If the angle cannot be confidently detected, this property is not present. If the image contains text at different angles, only part of the text will be recognized correctly.<img src=\"https://oxfordportal.blob.core.windows.net/vision/doc-vision-overview-ocr01.png\"/>orientationOrientation of the text recognized in the image. The value (up,down,left, or right) refers to the direction that the top of the recognized text is facing, after the image has been rotated around its center according to the detected text angle (see textAngle property).languageThe BCP-47 language code (user-provided or auto-detected) of the text detected in the image.regionsAn array of objects, where each object represents a region of recognized text. A region consists of multiple lines (e.g. a column of text in a multi-column document).linesAn array of objects, where each object represents a line of recognized text.wordsAn array of objects, where each object represents a recognized word.boundingBoxBounding box of a recognized region, line, or word, depending on the parent object. The four integers represent the x-coordinate of the left edge, the y-coordinate of the top edge, width, and height of the bounding box, in the coordinate system of the input image, after it has been rotated around its center according to the detected text angle (see textAngle property), with the origin at the top-left corner, and the y-axis pointing down.textString value of a recognized word.",
      "schema": {
       "$ref": "#/definitions/OcrResult"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/describe?overload=stream": {
   "post": {
    "description": "This operation generates a description of an image in human readable language with complete sentences.  The description is based on a collection of content tags, which are also returned by the operation. More than one description can be generated for each image.  Descriptions are ordered by their confidence score. All descriptions are in English. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL.A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.",
    "operationId": "DescribeImageInStream",
    "consumes": [
     "application/octet-stream"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "name": "maxCandidates",
      "in": "query",
      "description": "Maximum number of candidate descriptions to be returned.  The default is 1.",
      "type": "string",
      "required": false,
      "default": "1"
     },
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "$ref": "#/parameters/ImageStream"
     }
    ],
    "responses": {
     "200": {
      "schema": {
       "$ref": "#/definitions/ImageDescription"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/tag?overload=stream": {
   "post": {
    "description": "This operation generates a list of words, or tags, that are relevant to the content of the supplied image. The Computer Vision API can return tags based on objects, living beings, scenery or actions found in images. Unlike categories, tags are not organized according to a hierarchical classification system, but correspond to image content. Tags may contain hints to avoid ambiguity or provide context, for example the tag “cello” may be accompanied by the hint “musical instrument”. All tags are in English.",
    "operationId": "TagImageInStream",
    "consumes": [
     "application/octet-stream"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "$ref": "#/parameters/ImageStream"
     }
    ],
    "responses": {
     "200": {
      "schema": {
       "$ref": "#/definitions/TagResult"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/models/{model}/analyze?overload=stream": {
   "post": {
    "description": "This operation recognizes content within an image by applying a domain-specific model.  The list of domain-specific models that are supported by the Computer Vision API can be retrieved using the /models GET request.  Currently, the API only provides a single domain-specific model: celebrities. Two input methods are supported -- (1) Uploading an image or (2) specifying an image URL. A successful response will be returned in JSON.  If the request failed, the response will contain an error code and a message to help understand what went wrong.",
    "operationId": "Recognize Domain Specific Content In Stream",
    "consumes": [
     "application/octet-stream"
    ],
    "produces": [
     "application/json"
    ],
    "parameters": [
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "name": "model",
      "in": "path",
      "description": "The domain-specific content to recognize.",
      "required": true,
      "type": "string"
     },
     {
      "$ref": "#/parameters/ImageStream"
     }
    ],
    "responses": {
     "200": {
      "schema": {
       "$ref": "#/definitions/DomainModelResults"
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  },
  "/recognizeText?overload=stream": {
   "post": {
    "description": "Recognize Text operation. When you use the Recognize Text interface, the response contains a field called “Operation-Location”. The “Operation-Location” field contains the URL that you must use for your Get Handwritten Text Operation Result operation.\r\n<br>\r\n<br>\r\nFor the result of a Recognize Handwritten Text operation to be available, it requires an amount of time that depends on the length of the text. So, you may need to wait before using this Get Handwritten Text Operation Result interface. The time you need to wait may be up to a number of seconds.\r\n<br>\r\n<br>\r\nNote: this technology is currently in preview and is only available for English text.",
    "operationId": "RecognizeTextInStream",
    "parameters": [
     {
      "$ref": "#/parameters/HandwritingBoolean"
     },
     {
      "$ref": "#/parameters/SubscriptionKey"
     },
     {
      "$ref": "#/parameters/ImageStream"
     }
    ],
    "consumes": [
     "application/octet-stream"
    ],
    "produces": [
     "application/json"
    ],
    "responses": {
     "202": {
      "description": "The service has accepted the request and will start processing later.\r\n<br>\r\nIt will return Accepted immediately and include an <b>“Operation-Location”</b> header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.\r\n<br>\r\n<table class=\"element table\">\r\n            <thead>\r\n            </thead>\r\n            <tbody>\r\n            <tr><td><b>Operation-Location</b></td><td>Client side should use this URL to query operation status/result. <br/> Example: https://cognitiveservice/vision/v1.0/textOperations/49a36324-fc4b-4387-aa06-090cfbf0064f\r\n.</td></tr>\r\n             </tbody>\r\n</table>",
      "headers": {
       "Operation-Location": {
        "description": "URL to query for status of the operation. The operation ID will expire in 48 hours. ",
        "type": "string"
       }
      }
     },
     "default": {
      "schema": {
       "$ref": "#/definitions/ComputerVisionError"
      }
     }
    }
   }
  }
 },
 "definitions": {
  "TextOperationResult": {
   "type": "object",
   "properties": {
    "status": {
     "type": "string",
     "description": "",
     "enum": [
      "Not Started",
      "Running",
      "Failed",
      "Succeeded"
     ],
     "x-ms-enum": {
      "name": "TextOperationStatusCodes",
      "modelAsString": false
     },
     "x-nullable": false
    },
    "recognitionResult": {
     "$ref": "#/definitions/RecognitionResult"
    }
   }
  },
  "RecognitionResult": {
   "type": "object",
   "properties": {
    "lines": {
     "type": "array",
     "items": {
      "$ref": "#/definitions/Line"
     }
    }
   }
  },
  "Line": {
   "type": "object",
   "properties": {
    "boundingBox": {
     "$ref": "#/definitions/BoundingBox"
    },
    "text": {
     "type": "string"
    },
    "words": {
     "type": "array",
     "items": {
      "$ref": "#/definitions/Word"
     }
    }
   }
  },
  "Word": {
   "type": "object",
   "properties": {
    "boundingBox": {
     "$ref": "#/definitions/BoundingBox"
    },
    "text": {
     "type": "string"
    }
   }
  },
  "BoundingBox": {
   "type": "array",
   "items": {
    "type": "integer",
    "x-nullable": false
   }
  },
  "ImageAnalysis": {
   "type": "object",
   "description": "",
   "properties": {
    "categories": {
     "type": "array",
     "description": "",
     "items": {
      "$ref": "#/definitions/Category"
     }
    },
    "adult": {
     "description": "",
     "$ref": "#/definitions/AdultInfo"
    },
    "color": {
     "description": "",
     "$ref": "#/definitions/ColorInfo"
    },
    "imageType": {
     "description": "",
     "$ref": "#/definitions/ImageType"
    },
    "tags": {
     "type": "array",
     "description": "A list of tags with confidence level.",
     "items": {
      "$ref": "#/definitions/ImageTag"
     }
    },
    "description": {
     "description": "",
     "$ref": "#/definitions/ImageDescriptionDetails"
    },
    "faces": {
     "type": "array",
     "description": "",
     "items": {
      "$ref": "#/definitions/FaceDescription"
     }
    },
    "requestId": {
     "type": "string",
     "description": ""
    },
    "metadata": {
     "description": "Image metadata",
     "$ref": "#/definitions/ImageMetadata"
    }
   }
  },
  "ImageUrl": {
   "type": "object",
   "required": [
    "url"
   ],
   "properties": {
    "url": {
     "type": "string"
    }
   }
  },
  "OcrResult": {
   "type": "object",
   "properties": {
    "language": {
     "type": "string",
     "description": "The BCP-47 language code (user-provided or auto-detected) of the text detected in the image.",
     "enum": [
      "unk",
      "zh-Hans",
      "zh-Hant",
      "cs",
      "da",
      "nl",
      "en",
      "fi",
      "fr",
      "de",
      "el",
      "hu",
      "it",
      "ja",
      "ko",
      "nb",
      "pl",
      "pt",
      "ru",
      "es",
      "sv",
      "tr"
     ]
    },
    "textAngle": {
     "type": "number",
     "format": "double",
     "description": "The angle, in degrees, of the detected text with respect to the closest horizontal or vertical direction. After rotating the input image clockwise by this angle, the recognized text lines become horizontal or vertical. In combination with the orientation property it can be used to overlay recognition results correctly on the original image, by rotating either the original image or recognition results by a suitable angle around the center of the original image. If the angle cannot be confidently detected, this property is not present. If the image contains text at different angles, only part of the text will be recognized correctly."
    },
    "orientation": {
     "type": "string",
     "description": "Orientation of the text recognized in the image. The value (up,down,left, or right) refers to the direction that the top of the recognized text is facing, after the image has been rotated around its center according to the detected text angle (see textAngle property)."
    },
    "regions": {
     "type": "array",
     "description": "An array of objects, where each object represents a region of recognized text.",
     "items": {
      "$ref": "#/definitions/OcrRegion"
     }
    }
   }
  },
  "OcrRegion": {
   "type": "object",
   "description": "A region consists of multiple lines (e.g. a column of text in a multi-column document).",
   "properties": {
    "boundingBox": {
     "type": "string",
     "description": "Bounding box of a recognized region. The four integers represent the x-coordinate of the left edge, the y-coordinate of the top edge, width, and height of the bounding box, in the coordinate system of the input image, after it has been rotated around its center according to the detected text angle (see textAngle property), with the origin at the top-left corner, and the y-axis pointing down."
    },
    "lines": {
     "type": "array",
     "items": {
      "$ref": "#/definitions/OcrLine"
     }
    }
   }
  },
  "OcrLine": {
   "type": "object",
   "description": "",
   "properties": {
    "boundingBox": {
     "type": "string",
     "description": "Bounding box of a recognized line. The four integers represent the x-coordinate of the left edge, the y-coordinate of the top edge, width, and height of the bounding box, in the coordinate system of the input image, after it has been rotated around its center according to the detected text angle (see textAngle property), with the origin at the top-left corner, and the y-axis pointing down."
    },
    "words": {
     "type": "array",
     "description": "An array of objects, where each object represents a recognized word.",
     "items": {
      "$ref": "#/definitions/OcrWord"
     }
    }
   }
  },
  "OcrWord": {
   "type": "object",
   "description": "Information on a recognized word.",
   "properties": {
    "boundingBox": {
     "type": "string",
     "description": "Bounding box of a recognized word. The four integers represent the x-coordinate of the left edge, the y-coordinate of the top edge, width, and height of the bounding box, in the coordinate system of the input image, after it has been rotated around its center according to the detected text angle (see textAngle property), with the origin at the top-left corner, and the y-axis pointing down."
    },
    "text": {
     "type": "string",
     "description": "String value of a recognized word."
    }
   }
  },
  "ListModelsResult": {
   "type": "object",
   "description": "",
   "properties": {
    "models": {
     "type": "array",
     "description": "",
     "items": {
      "$ref": "#/definitions/ModelDescription"
     }
    }
   }
  },
  "ModelDescription": {
   "type": "object",
   "description": "",
   "properties": {
    "name": {
     "type": "string"
    },
    "categories": {
     "type": "array",
     "items": {
      "type": "string"
     }
    }
   }
  },
  "DomainModelResults": {
   "type": "object",
   "description": "",
   "properties": {
    "result": {
     "description": "",
     "x-ms-client-flatten": true,
     "$ref": "#/definitions/DomainModelResult"
    },
    "requestId": {
     "type": "string",
     "description": ""
    },
    "metadata": {
     "description": "Image metadata",
     "$ref": "#/definitions/ImageMetadata"
    }
   }
  },
  "DomainModelResult": {
   "type": "object",
   "properties": {
    "celebrities": {
     "type": "array",
     "description": "",
     "items": {
      "$ref": "#/definitions/CelebritiesModel"
     }
    }
   }
  },
  "ImageDescription": {
   "type": "object",
   "description": "A collection of content tags, along with a list of captions sorted by confidence level, and image metadata.",
   "properties": {
    "description": {
     "description": "",
     "x-ms-client-flatten": true,
     "$ref": "#/definitions/ImageDescriptionDetails"
    }
   }
  },
  "TagResult": {
   "type": "object",
   "description": "The results of a image tag operation, including any tags and image metadata.",
   "properties": {
    "tags": {
     "type": "array",
     "description": "A list of tags with confidence level.",
     "items": {
      "$ref": "#/definitions/ImageTag"
     }
    },
    "requestId": {
     "type": "string",
     "description": ""
    },
    "metadata": {
     "description": "Image metadata",
     "$ref": "#/definitions/ImageMetadata"
    }
   }
  },
  "ImageDescriptionDetails": {
   "type": "object",
   "description": "A collection of content tags, along with a list of captions sorted by confidence level, and image metadata.",
   "properties": {
    "tags": {
     "type": "array",
     "description": "A collection of image tags.",
     "items": {
      "type": "string"
     }
    },
    "captions": {
     "type": "array",
     "description": "A list of captions, sorted by confidence level.",
     "items": {
      "$ref": "#/definitions/ImageCaption"
     }
    },
    "requestId": {
     "type": "string",
     "description": ""
    },
    "metadata": {
     "description": "Image metadata",
     "$ref": "#/definitions/ImageMetadata"
    }
   }
  },
  "ImageCaption": {
   "type": "object",
   "description": "An image caption, i.e. a brief description of what the image depicts.",
   "properties": {
    "text": {
     "type": "string",
     "description": "The text of the caption"
    },
    "confidence": {
     "type": "number",
     "format": "double",
     "description": "The level of confidence the service has in the caption"
    }
   }
  },
  "ImageTag": {
   "type": "object",
   "description": "An image caption, i.e. a brief description of what the image depicts.",
   "properties": {
    "name": {
     "type": "string",
     "description": "The tag value"
    },
    "confidence": {
     "type": "number",
     "format": "double",
     "description": "The level of confidence the service has in the caption"
    }
   }
  },
  "ImageMetadata": {
   "type": "object",
   "description": "Image metadata",
   "properties": {
    "width": {
     "type": "integer",
     "format": "int32",
     "description": "Image width"
    },
    "height": {
     "type": "integer",
     "format": "int32",
     "description": "Image height"
    },
    "format": {
     "type": "string",
     "description": "Image format"
    }
   }
  },
  "CelebritiesModel": {
   "type": "object",
   "description": "",
   "properties": {
    "name": {
     "type": "string",
     "description": ""
    },
    "confidence": {
     "type": "number",
     "format": "double",
     "description": ""
    },
    "faceRectangle": {
     "$ref": "#/definitions/FaceRectangle"
    }
   }
  },
  "FaceRectangle": {
   "type": "object",
   "description": "",
   "properties": {
    "left": {
     "type": "integer",
     "description": ""
    },
    "right": {
     "type": "integer",
     "description": ""
    },
    "width": {
     "type": "integer",
     "description": ""
    },
    "height": {
     "type": "integer",
     "description": ""
    }
   }
  },
  "FaceDescription": {
   "type": "object",
   "description": "",
   "properties": {
    "age": {
     "type": "integer",
     "description": ""
    },
    "gender": {
     "type": "string",
     "description": "",
     "enum": [
      "Male",
      "Female"
     ]
    },
    "faceRectangle": {
     "$ref": "#/definitions/FaceRectangle"
    }
   }
  },
  "ImageType": {
   "type": "object",
   "description": "",
   "properties": {
    "clipArtType": {
     "type": "integer",
     "description": ""
    },
    "lineDrawingType": {
     "type": "integer",
     "description": ""
    }
   }
  },
  "ColorInfo": {
   "type": "object",
   "description": "",
   "properties": {
    "dominantColorForeground": {
     "type": "string",
     "description": ""
    },
    "dominantColorBackground": {
     "type": "string",
     "description": ""
    },
    "dominantColors": {
     "type": "array",
     "items": {
      "type": "string",
      "description": ""
     }
    },
    "accentColor": {
     "type": "string",
     "description": ""
    },
    "isBWImg": {
     "type": "boolean",
     "description": ""
    }
   }
  },
  "AdultInfo": {
   "type": "object",
   "description": "",
   "properties": {
    "isAdultContent": {
     "type": "boolean",
     "description": ""
    },
    "isRacyContent": {
     "type": "boolean",
     "description": ""
    },
    "adultScore": {
     "type": "number",
     "format": "double",
     "description": ""
    },
    "racyScore": {
     "type": "number",
     "format": "double",
     "description": ""
    }
   }
  },
  "Category": {
   "type": "object",
   "description": "",
   "properties": {
    "name": {
     "type": "string",
     "description": ""
    },
    "score": {
     "type": "number",
     "format": "double",
     "description": ""
    },
    "detail": {
     "type": "object",
     "description": "",
     "$ref": "#/definitions/CategoryDetail"
    }
   }
  },
  "CategoryDetail": {
   "type": "object",
   "description": "",
   "properties": {
    "celebrities": {
     "type": "array",
     "description": "",
     "items": {
      "$ref": "#/definitions/CelebritiesModel"
     }
    }
   }
  },
  "ComputerVisionError": {
   "type": "object",
   "required": [
    "code",
    "message"
   ],
   "properties": {
    "code": {
     "type": "string",
     "description": "The error code.",
     "enum": [
      "InvalidImageUrl",
      "InvalidImageFormat",
      "InvalidImageSize",
      "NotSupportedVisualFeature",
      "NotSupportedImage",
      "InvalidDetails",
      "NotSupportedLanguage",
      "BadArgument",
      "FailedToProcess",
      "Timeout",
      "InternalServerError",
      "Unspecified",
      "StorageException"
     ],
     "x-ms-enum": {
      "name": "ComputerVisionErrorCodes",
      "modelAsString": false
     }
    },
    "message": {
     "type": "string",
     "description": "A message explaining the error reported by the service."
    },
    "requestId": {
     "type": "string",
     "description": "A unique request identifier."
    }
   }
  }
 },
 "parameters": {
  "AzureRegion": {
   "name": "azureRegion",
   "description": "Supported Azure regions for Computer Vision endpoints",
   "x-ms-parameter-location": "client",
   "required": true,
   "type": "string",
   "in": "path",
   "x-ms-skip-url-encoding": true,
   "x-ms-enum": {
    "name": "AzureRegion",
    "modelAsString": true
   },
   "enum": [
    "westus",
    "westeurope",
    "southeastasia",
    "eastus2",
    "westcentralus"
   ]
  },
  "SubscriptionKey": {
   "name": "Ocp-Apim-Subscription-Key",
   "x-ms-client-name": "SubscriptionKey",
   "in": "header",
   "required": true,
   "description": "Subscription key in header",
   "type": "string"
  },
  "Language": {
   "name": "language",
   "in": "query",
   "description": "The BCP-47 language code of the text to be detected in the image.The default value is &quot;unk&quot;, then the service will auto detect the language of the text in the image.        Supported languages:    <ul style=\"margin-left:.375in;direction:ltr;unicode-bidi:embed; margin-top:0in;margin-bottom:0in\" type=\"disc\">        unk (AutoDetect)        zh-Hans (ChineseSimplified)        zh-Hant (ChineseTraditional)        cs (Czech)        da (Danish)        nl (Dutch)        en (English)        fi (Finnish)        fr (French)        de (German)        el (Greek)        hu (Hungarian)        it (Italian)        Ja (Japanese)        ko (Korean)        nb (Norwegian)        pl (Polish)        pt (Portuguese,        ru (Russian)        es (Spanish)        sv (Swedish)        tr (Turkish)    ",
   "type": "string",
   "required": false,
   "x-ms-parameter-location": "method",
   "default": "unk",
   "enum": [
    "unk",
    "zh-Hans",
    "zh-Hant",
    "cs",
    "da",
    "nl",
    "en",
    "fi",
    "fr",
    "de",
    "el",
    "hu",
    "it",
    "ja",
    "ko",
    "nb",
    "pl",
    "pt",
    "ru",
    "es",
    "sv",
    "tr"
   ]
  },
  "DetectOrientation": {
   "name": "detectOrientation",
   "in": "query",
   "description": "Whether detect the text orientation in the image. With detectOrientation=true the OCR service tries to detect the image orientation and correct it before further processing (e.g. if it's upside-down). ",
   "required": true,
   "x-ms-parameter-location": "method",
   "type": "boolean",
   "default": true
  },
  "ImageStream": {
   "name": "Image",
   "in": "body",
   "required": true,
   "x-ms-parameter-location": "method",
   "description": "An image stream.",
   "schema": {
    "type": "file"
   }
  },
  "ImageUrl": {
   "name": "ImageUrl",
   "in": "body",
   "required": true,
   "x-ms-parameter-location": "method",
   "x-ms-client-flatten": true,
   "description": "A JSON document with a URL pointing to the image that is to be analyzed.",
   "schema": {
    "$ref": "#/definitions/ImageUrl"
   }
  },
  "HandwritingBoolean": {
   "name": "detectHandwriting",
   "in": "query",
   "description": "If “true” is specified, handwriting recognition is performed. If this parameter is set to “false” or is not specified, printed text recognition is performed.",
   "required": false,
   "x-ms-parameter-location": "method",
   "type": "boolean",
   "default": "false",
   "enum": [
    "true",
    "false"
   ]
  }
 }
}