import "@typespec/openapi";
import "../common/models.tsp";
import "../chat/models.tsp";
import "../responses/models.tsp";
using TypeSpec.OpenAPI;
namespace OpenAI;
/** A CompletionsRunDataSource object describing a model sampling configuration. */
@summary("CompletionsRunDataSource")
@extension(
  "x-oaiMeta",
  #{
    name: "The completions data source object used to configure an individual run",
    group: "eval runs",
    example: "{\n  \"name\": \"gpt-4o-mini-2024-07-18\",\n  \"data_source\": {\n    \"type\": \"completions\",\n    \"input_messages\": {\n      \"type\": \"item_reference\",\n      \"item_reference\": \"item.input\"\n    },\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"source\": {\n      \"type\": \"stored_completions\",\n      \"model\": \"gpt-4o-mini-2024-07-18\"\n    }\n  }\n}\n",
  }
)
model CreateEvalCompletionsRunDataSource {
  /** The type of run data source. Always `completions`. */
  type: "completions" = "completions";

  /** Used when sampling from a model. Dictates the structure of the messages passed into the model. Can either be a reference to a prebuilt trajectory (ie, `item.input_trajectory`), or a template with variable references to the `item` namespace. */
  input_messages?: {
    type: "template";
    template: (EasyInputMessage | EvalItem)[];
  } | {
    type: "item_reference";
    item_reference: string;
  };

  sampling_params?: {
    temperature?: numeric = 1;
    max_completion_tokens?: integer;
    top_p?: numeric = 1;
    seed?: integer = 42;
    response_format?: ResponseFormatText | ResponseFormatJsonSchema | ResponseFormatJsonObject;
    tools?: ChatCompletionTool[];
  };

  /** The name of the model to use for generating completions (e.g. "o3-mini"). */
  `model`?: string;

  /** Determines what populates the `item` namespace in this run's data source. */
  source: EvalJsonlFileContentSource | EvalJsonlFileIdSource | EvalStoredCompletionsSource;
}

/**A message input to the model with a role indicating instruction following
hierarchy. Instructions given with the `developer` or `system` role take
precedence over instructions given with the `user` role. Messages with the
`assistant` role are presumed to have been generated by the model in previous
interactions.*/
@summary("Eval message object")
model EvalItem {
  /**The role of the message input. One of `user`, `assistant`, `system`, or
  `developer`.*/
  role: "user" | "assistant" | "system" | "developer";

  /** Inputs to the model - can contain template strings. */
  content:
    | string
    | InputTextContent
    | {
        @extension("x-stainless-const", true)
        type: "output_text";

        text: string;
      }
    | {
        @extension("x-stainless-const", true)
        type: "input_image";

        image_url: string;
        detail?: string;
      }
    | InputAudio
    | unknown[];

  /** The type of the message input. Always `message`. */
  @extension("x-stainless-const", true)
  type?: "message";
}

@summary("EvalJsonlFileContentSource")
model EvalJsonlFileContentSource {
  /** The type of jsonl source. Always `file_content`. */
  @extension("x-stainless-const", true)
  type: "file_content" = "file_content";

  /** The content of the jsonl file. */
  content: {
    item: {};
    sample?: {};
  }[];
}

@summary("EvalJsonlFileIdSource")
model EvalJsonlFileIdSource {
  /** The type of jsonl source. Always `file_id`. */
  @extension("x-stainless-const", true)
  type: "file_id" = "file_id";

  /** The identifier of the file. */
  id: string;
}

/** A StoredCompletionsRunDataSource configuration describing a set of filters */
@summary("StoredCompletionsRunDataSource")
@extension(
  "x-oaiMeta",
  #{
    name: "The stored completions data source object used to configure an individual run",
    group: "eval runs",
    example: "{\n  \"type\": \"stored_completions\",\n  \"model\": \"gpt-4o\",\n  \"created_after\": 1668124800,\n  \"created_before\": 1668124900,\n  \"limit\": 100,\n  \"metadata\": {}\n}\n",
  }
)
model EvalStoredCompletionsSource {
  /** The type of source. Always `stored_completions`. */
  @extension("x-stainless-const", true)
  type: "stored_completions" = "stored_completions";

  metadata?: Metadata;

  /** An optional model to filter by (e.g., 'gpt-4o'). */
  `model`?: string | null;

  /** An optional Unix timestamp to filter items created after this time. */
  created_after?: integer | null;

  /** An optional Unix timestamp to filter items created before this time. */
  created_before?: integer | null;

  /** An optional maximum number of items to return. */
  limit?: integer | null;
}

/**A CustomDataSourceConfig object that defines the schema for the data source used for the evaluation runs.
This schema is used to define the shape of the data that will be:
- Used to define your testing criteria and
- What data is required when creating a run*/
@summary("CustomDataSourceConfig")
@extension(
  "x-oaiMeta",
  #{
    name: "The eval file data source config object",
    group: "evals",
    example: "{\n  \"type\": \"custom\",\n  \"item_schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"name\": {\"type\": \"string\"},\n      \"age\": {\"type\": \"integer\"}\n    },\n    \"required\": [\"name\", \"age\"]\n  },\n  \"include_sample_schema\": true\n}\n",
  }
)
model CreateEvalCustomDataSourceConfig {
  /** The type of data source. Always `custom`. */
  @extension("x-stainless-const", true)
  type: "custom" = "custom";

  /** The json schema for each row in the data source. */
  item_schema: {};

  /** Whether the eval should expect you to populate the sample namespace (ie, by generating responses off of your data source) */
  include_sample_schema?: boolean;
}

/** A JsonlRunDataSource object with that specifies a JSONL file that matches the eval */
@summary("JsonlRunDataSource")
@extension(
  "x-oaiMeta",
  #{
    name: "The file data source object for the eval run configuration",
    group: "evals",
    example: "{\n \"type\": \"jsonl\",\n \"source\": {\n   \"type\": \"file_id\",\n   \"id\": \"file-9GYS6xbkWgWhmE7VoLUWFg\"\n }\n}\n",
  }
)
model CreateEvalJsonlRunDataSource {
  /** The type of data source. Always `jsonl`. */
  @extension("x-stainless-const", true)
  type: "jsonl" = "jsonl";

  /** Determines what populates the `item` namespace in the data source. */
  source: EvalJsonlFileContentSource | EvalJsonlFileIdSource;
}

/**A LabelModelGrader object which uses a model to assign labels to each item
in the evaluation.*/
@summary("LabelModelGrader")
@extension(
  "x-oaiMeta",
  #{
    name: "The eval label model grader object",
    group: "evals",
    example: "{\n  \"type\": \"label_model\",\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"input\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Classify the sentiment of the following statement as one of 'positive', 'neutral', or 'negative'\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Statement: {{item.response}}\"\n    }\n  ],\n  \"passing_labels\": [\"positive\"],\n  \"labels\": [\"positive\", \"neutral\", \"negative\"],\n  \"name\": \"Sentiment label grader\"\n}\n",
  }
)
model CreateEvalLabelModelGrader {
  /** The object type, which is always `label_model`. */
  @extension("x-stainless-const", true)
  type: "label_model";

  /** The name of the grader. */
  name: string;

  /** The model to use for the evaluation. Must support structured outputs. */
  `model`: string;

  /** A list of chat messages forming the prompt or context. May include variable references to the `item` namespace, ie {{item.name}}. */
  input: CreateEvalItem[];

  /** The labels to classify to each item in the evaluation. */
  labels: string[];

  /** The labels that indicate a passing result. Must be a subset of labels. */
  passing_labels: string[];
}

/** A chat message that makes up the prompt or context. May include variable references to the `item` namespace, ie {{item.name}}. */
@summary("CreateEvalItem")
@extension(
  "x-oaiMeta",
  #{ name: "The chat message object used to configure an individual run" }
)
union CreateEvalItem {
  {
    role: string,
    content: string,
  },
  EvalItem,
}

/**A data source config which specifies the metadata property of your logs query.
This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc.*/
@summary("LogsDataSourceConfig")
@extension(
  "x-oaiMeta",
  #{
    name: "The logs data source object for evals",
    group: "evals",
    example: "{\n  \"type\": \"logs\",\n  \"metadata\": {\n    \"use_case\": \"customer_support_agent\"\n  }\n}\n",
  }
)
model CreateEvalLogsDataSourceConfig {
  /** The type of data source. Always `logs`. */
  @extension("x-stainless-const", true)
  type: "logs" = "logs";

  /** Metadata filters for the logs data source. */
  metadata?: {};
}

@summary("CreateEvalRequest")
model CreateEvalRequest {
  /** The name of the evaluation. */
  name?: string;

  metadata?: Metadata;

  /** The configuration for the data source used for the evaluation runs. Dictates the schema of the data used in the evaluation. */
  data_source_config: CreateEvalCustomDataSourceConfig | CreateEvalLogsDataSourceConfig | CreateEvalStoredCompletionsDataSourceConfig;

  /** A list of graders for all eval runs in this group. Graders can reference variables in the data source using double curly braces notation, like `{{item.variable_name}}`. To reference the model's output, use the `sample` namespace (ie, `{{sample.output_text}}`). */
  testing_criteria: (
    | CreateEvalLabelModelGrader
    | EvalGraderStringCheck
    | EvalGraderTextSimilarity
    | EvalGraderPython
    | EvalGraderScoreModel)[];
}

/** Deprecated in favor of LogsDataSourceConfig. */
@summary("StoredCompletionsDataSourceConfig")
@extension(
  "x-oaiMeta",
  #{
    name: "The stored completions data source object for evals",
    group: "evals",
    example: "{\n  \"type\": \"stored_completions\",\n  \"metadata\": {\n    \"use_case\": \"customer_support_agent\"\n  }\n}\n",
  }
)
model CreateEvalStoredCompletionsDataSourceConfig {
  /** The type of data source. Always `stored_completions`. */
  @extension("x-stainless-const", true)
  type: "stored_completions" = "stored_completions";

  /** Metadata filters for the stored completions data source. */
  metadata?: {};
}

@summary("StringCheckGrader")
model EvalGraderStringCheck {
  /** The object type, which is always `string_check`. */
  @extension("x-stainless-const", true)
  type: "string_check";

  /** The name of the grader. */
  name: string;

  /** The input text. This may include template strings. */
  input: string;

  /** The reference text. This may include template strings. */
  reference: string;

  /** The string check operation to perform. One of `eq`, `ne`, `like`, or `ilike`. */
  operation: "eq" | "ne" | "like" | "ilike";
}

@summary("TextSimilarityGrader")
model EvalGraderTextSimilarity {
  /** The type of grader. */
  @extension("x-stainless-const", true)
  type: "text_similarity" = "text_similarity";

  /** The name of the grader. */
  name: string;

  /** The text being graded. */
  input: string;

  /** The text being graded against. */
  reference: string;

  /**The evaluation metric to use. One of `cosine`, `fuzzy_match`, `bleu`,
  `gleu`, `meteor`, `rouge_1`, `rouge_2`, `rouge_3`, `rouge_4`, `rouge_5`,
  or `rouge_l`.*/
  evaluation_metric:
    | "cosine"
    | "fuzzy_match"
    | "bleu"
    | "gleu"
    | "meteor"
    | "rouge_1"
    | "rouge_2"
    | "rouge_3"
    | "rouge_4"
    | "rouge_5"
    | "rouge_l";

  /** The threshold for the score. */
  pass_threshold: numeric;
}

@summary("PythonGrader")
model EvalGraderPython {
  /** The object type, which is always `python`. */
  @extension("x-stainless-const", true)
  type: "python";

  /** The name of the grader. */
  name: string;

  /** The source code of the python script. */
  source: string;

  /** The image tag to use for the python script. */
  image_tag?: string;

  /** The threshold for the score. */
  pass_threshold?: numeric;
}

@summary("ScoreModelGrader")
model EvalGraderScoreModel {
  /** The object type, which is always `score_model`. */
  @extension("x-stainless-const", true)
  type: "score_model";

  /** The name of the grader. */
  name: string;

  /** The model to use for the evaluation. */
  `model`: string;

  /** The sampling parameters for the model. */
  sampling_params?: {};

  /** The input text. This may include template strings. */
  input: EvalItem[];

  /** The range of the score. Defaults to `[0, 1]`. */
  range?: numeric[];

  /** The threshold for the score. */
  pass_threshold?: numeric;
}

/** A ResponsesRunDataSource object describing a model sampling configuration. */
@summary("ResponsesRunDataSource")
@extension(
  "x-oaiMeta",
  #{
    name: "The completions data source object used to configure an individual run",
    group: "eval runs",
    example: "{\n  \"name\": \"gpt-4o-mini-2024-07-18\",\n  \"data_source\": {\n    \"type\": \"responses\",\n    \"input_messages\": {\n      \"type\": \"item_reference\",\n      \"item_reference\": \"item.input\"\n    },\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"source\": {\n      \"type\": \"responses\",\n      \"model\": \"gpt-4o-mini-2024-07-18\"\n    }\n  }\n}\n",
  }
)
model CreateEvalResponsesRunDataSource {
  /** The type of run data source. Always `responses`. */
  type: "responses" = "responses";

  /** Used when sampling from a model. Dictates the structure of the messages passed into the model. Can either be a reference to a prebuilt trajectory (ie, `item.input_trajectory`), or a template with variable references to the `item` namespace. */
  input_messages?: {
    type: "template";
    template: ({
      role: string;
      content: string;
    } | EvalItem)[];
  } | {
    type: "item_reference";
    item_reference: string;
  };

  sampling_params?: {
    temperature?: numeric = 1;
    max_completion_tokens?: integer;
    top_p?: numeric = 1;
    seed?: integer = 42;
    tools?: Tool[];
    text?: {
      format?: TextResponseFormatConfiguration;
    };
  };

  /** The name of the model to use for generating completions (e.g. "o3-mini"). */
  `model`?: string;

  /** Determines what populates the `item` namespace in this run's data source. */
  source: EvalJsonlFileContentSource | EvalJsonlFileIdSource | EvalResponsesSource;
}

/** A EvalResponsesSource object describing a run data source configuration. */
@summary("EvalResponsesSource")
@extension(
  "x-oaiMeta",
  #{
    name: "The run data source object used to configure an individual run",
    group: "eval runs",
    example: "{\n  \"type\": \"responses\",\n  \"model\": \"gpt-4o-mini-2024-07-18\",\n  \"temperature\": 0.7,\n  \"top_p\": 1.0,\n  \"users\": [\"user1\", \"user2\"],\n  \"tools\": [\"tool1\", \"tool2\"],\n  \"instructions_search\": \"You are a coding assistant\"\n}\n",
  }
)
model EvalResponsesSource {
  /** The type of run data source. Always `responses`. */
  type: "responses";

  /** Metadata filter for the responses. This is a query parameter used to select responses. */
  metadata?: {} | null;

  /** The name of the model to find responses for. This is a query parameter used to select responses. */
  `model`?: string | null;

  /** Optional string to search the 'instructions' field. This is a query parameter used to select responses. */
  instructions_search?: string | null;

  /** Only include items created after this timestamp (inclusive). This is a query parameter used to select responses. */
  @minValue(0)
  created_after?: integer | null;

  /** Only include items created before this timestamp (inclusive). This is a query parameter used to select responses. */
  @minValue(0)
  created_before?: integer | null;

  /** Optional reasoning effort parameter. This is a query parameter used to select responses. */
  reasoning_effort?: ReasoningEffort;

  /** Sampling temperature. This is a query parameter used to select responses. */
  temperature?: numeric | null;

  /** Nucleus sampling parameter. This is a query parameter used to select responses. */
  top_p?: numeric | null;

  /** List of user identifiers. This is a query parameter used to select responses. */
  users?: string[] | null;

  /** List of tool names. This is a query parameter used to select responses. */
  tools?: string[] | null;
}

@summary("CreateEvalRunRequest")
model CreateEvalRunRequest {
  /** The name of the run. */
  name?: string;

  metadata?: Metadata;

  /** Details about the run's data source. */
  data_source: CreateEvalJsonlRunDataSource | CreateEvalCompletionsRunDataSource | CreateEvalResponsesRunDataSource;
}

/**An Eval object with a data source config and testing criteria.
An Eval represents a task to be done for your LLM integration.
Like:
- Improve the quality of my chatbot
- See how well my chatbot handles customer support
- Check if o4-mini is better at my usecase than gpt-4o*/
@summary("Eval")
@extension(
  "x-oaiMeta",
  #{
    name: "The eval object",
    group: "evals",
    example: "{\n  \"object\": \"eval\",\n  \"id\": \"eval_67abd54d9b0081909a86353f6fb9317a\",\n  \"data_source_config\": {\n    \"type\": \"custom\",\n    \"item_schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"label\": {\"type\": \"string\"},\n      },\n      \"required\": [\"label\"]\n    },\n    \"include_sample_schema\": true\n  },\n  \"testing_criteria\": [\n    {\n      \"name\": \"My string check grader\",\n      \"type\": \"string_check\",\n      \"input\": \"{{sample.output_text}}\",\n      \"reference\": \"{{item.label}}\",\n      \"operation\": \"eq\",\n    }\n  ],\n  \"name\": \"External Data Eval\",\n  \"created_at\": 1739314509,\n  \"metadata\": {\n    \"test\": \"synthetics\",\n  }\n}\n",
  }
)
model Eval {
  /** The object type. */
  @extension("x-stainless-const", true)
  object: "eval" = "eval";

  /** Unique identifier for the evaluation. */
  id: string;

  /** The name of the evaluation. */
  name: string;

  /** Configuration of data sources used in runs of the evaluation. */
  data_source_config: EvalCustomDataSourceConfig | EvalLogsDataSourceConfig | EvalStoredCompletionsDataSourceConfig;

  /** A list of testing criteria. */
  testing_criteria: (
    | EvalGraderLabelModel
    | EvalGraderStringCheck
    | EvalGraderTextSimilarity
    | EvalGraderPython
    | EvalGraderScoreModel)[];

  /** The Unix timestamp (in seconds) for when the eval was created. */
  created_at: integer;

  metadata: Metadata;
}

/**A CustomDataSourceConfig which specifies the schema of your `item` and optionally `sample` namespaces.
The response schema defines the shape of the data that will be:
- Used to define your testing criteria and
- What data is required when creating a run*/
@summary("CustomDataSourceConfig")
@extension(
  "x-oaiMeta",
  #{
    name: "The eval custom data source config object",
    group: "evals",
    example: "{\n  \"type\": \"custom\",\n  \"schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"item\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"label\": {\"type\": \"string\"},\n        },\n        \"required\": [\"label\"]\n      }\n    },\n    \"required\": [\"item\"]\n  }\n}\n",
  }
)
model EvalCustomDataSourceConfig {
  /** The type of data source. Always `custom`. */
  @extension("x-stainless-const", true)
  type: "custom" = "custom";

  /**The json schema for the run data source items.
  Learn how to build JSON schemas [here](https://json-schema.org/).*/
  schema: {};
}

/**A LogsDataSourceConfig which specifies the metadata property of your logs query.
This is usually metadata like `usecase=chatbot` or `prompt-version=v2`, etc.
The schema returned by this data source config is used to defined what variables are available in your evals.
`item` and `sample` are both defined when using this data source config.*/
@summary("LogsDataSourceConfig")
@extension(
  "x-oaiMeta",
  #{
    name: "The logs data source object for evals",
    group: "evals",
    example: "{\n  \"type\": \"logs\",\n  \"metadata\": {\n    \"language\": \"english\"\n  },\n  \"schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"item\": {\n        \"type\": \"object\"\n      },\n      \"sample\": {\n        \"type\": \"object\"\n      }\n    },\n    \"required\": [\n      \"item\",\n      \"sample\"\n    }\n}\n",
  }
)
model EvalLogsDataSourceConfig {
  /** The type of data source. Always `logs`. */
  @extension("x-stainless-const", true)
  type: "logs" = "logs";

  metadata?: Metadata;

  /**The json schema for the run data source items.
  Learn how to build JSON schemas [here](https://json-schema.org/).*/
  schema: {};
}

/** Deprecated in favor of LogsDataSourceConfig. */
@summary("StoredCompletionsDataSourceConfig")
@extension(
  "x-oaiMeta",
  #{
    name: "The stored completions data source object for evals",
    group: "evals",
    example: "{\n  \"type\": \"stored_completions\",\n  \"metadata\": {\n    \"language\": \"english\"\n  },\n  \"schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"item\": {\n        \"type\": \"object\"\n      },\n      \"sample\": {\n        \"type\": \"object\"\n      }\n    },\n    \"required\": [\n      \"item\",\n      \"sample\"\n    }\n}\n",
  }
)
model EvalStoredCompletionsDataSourceConfig {
  /** The type of data source. Always `stored_completions`. */
  @extension("x-stainless-const", true)
  type: "stored_completions" = "stored_completions";

  metadata?: Metadata;

  /**The json schema for the run data source items.
  Learn how to build JSON schemas [here](https://json-schema.org/).*/
  schema: {};
}

@summary("LabelModelGrader")
model EvalGraderLabelModel {
  /** The object type, which is always `label_model`. */
  @extension("x-stainless-const", true)
  type: "label_model";

  /** The name of the grader. */
  name: string;

  /** The model to use for the evaluation. Must support structured outputs. */
  `model`: string;

  input: EvalItem[];

  /** The labels to assign to each item in the evaluation. */
  labels: string[];

  /** The labels that indicate a passing result. Must be a subset of labels. */
  passing_labels: string[];
}

/** An object representing an error response from the Eval API. */
@summary("EvalApiError")
@extension(
  "x-oaiMeta",
  #{
    name: "The API error object",
    group: "evals",
    example: "{\n  \"code\": \"internal_error\",\n  \"message\": \"The eval run failed due to an internal error.\"\n}\n",
  }
)
model EvalApiError {
  /** The error code. */
  code: string;

  /** The error message. */
  message: string;
}

/** An object representing a list of evals. */
@summary("EvalList")
@extension(
  "x-oaiMeta",
  #{
    name: "The eval list object",
    group: "evals",
    example: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"eval\",\n      \"id\": \"eval_67abd54d9b0081909a86353f6fb9317a\",\n      \"data_source_config\": {\n        \"type\": \"custom\",\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"item\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"input\": {\n                  \"type\": \"string\"\n                },\n                \"ground_truth\": {\n                  \"type\": \"string\"\n                }\n              },\n              \"required\": [\n                \"input\",\n                \"ground_truth\"\n              ]\n            }\n          },\n          \"required\": [\n            \"item\"\n          ]\n        }\n      },\n      \"testing_criteria\": [\n        {\n          \"name\": \"String check\",\n          \"id\": \"String check-2eaf2d8d-d649-4335-8148-9535a7ca73c2\",\n          \"type\": \"string_check\",\n          \"input\": \"{{item.input}}\",\n          \"reference\": \"{{item.ground_truth}}\",\n          \"operation\": \"eq\"\n        }\n      ],\n      \"name\": \"External Data Eval\",\n      \"created_at\": 1739314509,\n      \"metadata\": {},\n    }\n  ],\n  \"first_id\": \"eval_67abd54d9b0081909a86353f6fb9317a\",\n  \"last_id\": \"eval_67abd54d9b0081909a86353f6fb9317a\",\n  \"has_more\": true\n}\n",
  }
)
model EvalList {
  /** The type of this object. It is always set to "list". */
  @extension("x-stainless-const", true)
  object: "list" = "list";

  /** An array of eval objects. */
  data: Eval[];

  /** The identifier of the first eval in the data array. */
  first_id: string;

  /** The identifier of the last eval in the data array. */
  last_id: string;

  /** Indicates whether there are more evals available. */
  has_more: boolean;
}

/** A schema representing an evaluation run. */
@summary("EvalRun")
@extension(
  "x-oaiMeta",
  #{
    name: "The eval run object",
    group: "evals",
    example: "{\n  \"object\": \"eval.run\",\n  \"id\": \"evalrun_67e57965b480819094274e3a32235e4c\",\n  \"eval_id\": \"eval_67e579652b548190aaa83ada4b125f47\",\n  \"report_url\": \"https://platform.openai.com/evaluations/eval_67e579652b548190aaa83ada4b125f47?run_id=evalrun_67e57965b480819094274e3a32235e4c\",\n  \"status\": \"queued\",\n  \"model\": \"gpt-4o-mini\",\n  \"name\": \"gpt-4o-mini\",\n  \"created_at\": 1743092069,\n  \"result_counts\": {\n    \"total\": 0,\n    \"errored\": 0,\n    \"failed\": 0,\n    \"passed\": 0\n  },\n  \"per_model_usage\": null,\n  \"per_testing_criteria_results\": null,\n  \"data_source\": {\n    \"type\": \"completions\",\n    \"source\": {\n      \"type\": \"file_content\",\n      \"content\": [\n        {\n          \"item\": {\n            \"input\": \"Tech Company Launches Advanced Artificial Intelligence Platform\",\n            \"ground_truth\": \"Technology\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Central Bank Increases Interest Rates Amid Inflation Concerns\",\n            \"ground_truth\": \"Markets\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"International Summit Addresses Climate Change Strategies\",\n            \"ground_truth\": \"World\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Major Retailer Reports Record-Breaking Holiday Sales\",\n            \"ground_truth\": \"Business\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"National Team Qualifies for World Championship Finals\",\n            \"ground_truth\": \"Sports\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Stock Markets Rally After Positive Economic Data Released\",\n            \"ground_truth\": \"Markets\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Global Manufacturer Announces Merger with Competitor\",\n            \"ground_truth\": \"Business\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Breakthrough in Renewable Energy Technology Unveiled\",\n            \"ground_truth\": \"Technology\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"World Leaders Sign Historic Climate Agreement\",\n            \"ground_truth\": \"World\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Professional Athlete Sets New Record in Championship Event\",\n            \"ground_truth\": \"Sports\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Financial Institutions Adapt to New Regulatory Requirements\",\n            \"ground_truth\": \"Business\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Tech Conference Showcases Advances in Artificial Intelligence\",\n            \"ground_truth\": \"Technology\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Global Markets Respond to Oil Price Fluctuations\",\n            \"ground_truth\": \"Markets\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"International Cooperation Strengthened Through New Treaty\",\n            \"ground_truth\": \"World\"\n          }\n        },\n        {\n          \"item\": {\n            \"input\": \"Sports League Announces Revised Schedule for Upcoming Season\",\n            \"ground_truth\": \"Sports\"\n          }\n        }\n      ]\n    },\n    \"input_messages\": {\n      \"type\": \"template\",\n      \"template\": [\n        {\n          \"type\": \"message\",\n          \"role\": \"developer\",\n          \"content\": {\n            \"type\": \"input_text\",\n            \"text\": \"Categorize a given news headline into one of the following topics: Technology, Markets, World, Business, or Sports.\\n\\n# Steps\\n\\n1. Analyze the content of the news headline to understand its primary focus.\\n2. Extract the subject matter, identifying any key indicators or keywords.\\n3. Use the identified indicators to determine the most suitable category out of the five options: Technology, Markets, World, Business, or Sports.\\n4. Ensure only one category is selected per headline.\\n\\n# Output Format\\n\\nRespond with the chosen category as a single word. For instance: \\\"Technology\\\", \\\"Markets\\\", \\\"World\\\", \\\"Business\\\", or \\\"Sports\\\".\\n\\n# Examples\\n\\n**Input**: \\\"Apple Unveils New iPhone Model, Featuring Advanced AI Features\\\"  \\n**Output**: \\\"Technology\\\"\\n\\n**Input**: \\\"Global Stocks Mixed as Investors Await Central Bank Decisions\\\"  \\n**Output**: \\\"Markets\\\"\\n\\n**Input**: \\\"War in Ukraine: Latest Updates on Negotiation Status\\\"  \\n**Output**: \\\"World\\\"\\n\\n**Input**: \\\"Microsoft in Talks to Acquire Gaming Company for $2 Billion\\\"  \\n**Output**: \\\"Business\\\"\\n\\n**Input**: \\\"Manchester United Secures Win in Premier League Football Match\\\"  \\n**Output**: \\\"Sports\\\" \\n\\n# Notes\\n\\n- If the headline appears to fit into more than one category, choose the most dominant theme.\\n- Keywords or phrases such as \\\"stocks\\\", \\\"company acquisition\\\", \\\"match\\\", or technological brands can be good indicators for classification.\\n\"\n          }\n        },\n        {\n          \"type\": \"message\",\n          \"role\": \"user\",\n          \"content\": {\n            \"type\": \"input_text\",\n            \"text\": \"{{item.input}}\"\n          }\n        }\n      ]\n    },\n    \"model\": \"gpt-4o-mini\",\n    \"sampling_params\": {\n      \"seed\": 42,\n      \"temperature\": 1.0,\n      \"top_p\": 1.0,\n      \"max_completions_tokens\": 2048\n    }\n  },\n  \"error\": null,\n  \"metadata\": {}\n}\n",
  }
)
model EvalRun {
  /** The type of the object. Always "eval.run". */
  @extension("x-stainless-const", true)
  object: "eval.run" = "eval.run";

  /** Unique identifier for the evaluation run. */
  id: string;

  /** The identifier of the associated evaluation. */
  eval_id: string;

  /** The status of the evaluation run. */
  status: string;

  /** The model that is evaluated, if applicable. */
  `model`: string;

  /** The name of the evaluation run. */
  name: string;

  /** Unix timestamp (in seconds) when the evaluation run was created. */
  created_at: integer;

  /** The URL to the rendered evaluation run report on the UI dashboard. */
  report_url: string;

  /** Counters summarizing the outcomes of the evaluation run. */
  result_counts: {
    total: integer;
    errored: integer;
    failed: integer;
    passed: integer;
  };

  /** Usage statistics for each model during the evaluation run. */
  per_model_usage: {
    @extension(
      "x-stainless-naming",
      #{ python: #{ property_name: "run_model_name" } }
    )
    model_name: string;

    invocation_count: integer;
    prompt_tokens: integer;
    completion_tokens: integer;
    total_tokens: integer;
    cached_tokens: integer;
  }[];

  /** Results per testing criteria applied during the evaluation run. */
  per_testing_criteria_results: {
    testing_criteria: string;
    passed: integer;
    failed: integer;
  }[];

  /** Information about the run's data source. */
  data_source: CreateEvalJsonlRunDataSource | CreateEvalCompletionsRunDataSource | CreateEvalResponsesRunDataSource;

  metadata: Metadata;
  error: EvalApiError;
}

/** An object representing a list of runs for an evaluation. */
@summary("EvalRunList")
@extension(
  "x-oaiMeta",
  #{
    name: "The eval run list object",
    group: "evals",
    example: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"eval.run\",\n      \"id\": \"evalrun_67b7fbdad46c819092f6fe7a14189620\",\n      \"eval_id\": \"eval_67b7fa9a81a88190ab4aa417e397ea21\",\n      \"report_url\": \"https://platform.openai.com/evaluations/eval_67b7fa9a81a88190ab4aa417e397ea21?run_id=evalrun_67b7fbdad46c819092f6fe7a14189620\",\n      \"status\": \"completed\",\n      \"model\": \"o3-mini\",\n      \"name\": \"Academic Assistant\",\n      \"created_at\": 1740110812,\n      \"result_counts\": {\n        \"total\": 171,\n        \"errored\": 0,\n        \"failed\": 80,\n        \"passed\": 91\n      },\n      \"per_model_usage\": null,\n      \"per_testing_criteria_results\": [\n        {\n          \"testing_criteria\": \"String check grader\",\n          \"passed\": 91,\n          \"failed\": 80\n        }\n      ],\n      \"run_data_source\": {\n        \"type\": \"completions\",\n        \"template_messages\": [\n          {\n            \"type\": \"message\",\n            \"role\": \"system\",\n            \"content\": {\n              \"type\": \"input_text\",\n              \"text\": \"You are a helpful assistant.\"\n            }\n          },\n          {\n            \"type\": \"message\",\n            \"role\": \"user\",\n            \"content\": {\n              \"type\": \"input_text\",\n              \"text\": \"Hello, can you help me with my homework?\"\n            }\n          }\n        ],\n        \"datasource_reference\": null,\n        \"model\": \"o3-mini\",\n        \"max_completion_tokens\": null,\n        \"seed\": null,\n        \"temperature\": null,\n        \"top_p\": null\n      },\n      \"error\": null,\n      \"metadata\": {\"test\": \"synthetics\"}\n    }\n  ],\n  \"first_id\": \"evalrun_67abd54d60ec8190832b46859da808f7\",\n  \"last_id\": \"evalrun_67abd54d60ec8190832b46859da808f7\",\n  \"has_more\": false\n}\n",
  }
)
model EvalRunList {
  /** The type of this object. It is always set to "list". */
  @extension("x-stainless-const", true)
  object: "list" = "list";

  /** An array of eval run objects. */
  data: EvalRun[];

  /** The identifier of the first eval run in the data array. */
  first_id: string;

  /** The identifier of the last eval run in the data array. */
  last_id: string;

  /** Indicates whether there are more evals available. */
  has_more: boolean;
}

/** A schema representing an evaluation run output item. */
@summary("EvalRunOutputItem")
@extension(
  "x-oaiMeta",
  #{
    name: "The eval run output item object",
    group: "evals",
    example: "{\n  \"object\": \"eval.run.output_item\",\n  \"id\": \"outputitem_67abd55eb6548190bb580745d5644a33\",\n  \"run_id\": \"evalrun_67abd54d60ec8190832b46859da808f7\",\n  \"eval_id\": \"eval_67abd54d9b0081909a86353f6fb9317a\",\n  \"created_at\": 1739314509,\n  \"status\": \"pass\",\n  \"datasource_item_id\": 137,\n  \"datasource_item\": {\n      \"teacher\": \"To grade essays, I only check for style, content, and grammar.\",\n      \"student\": \"I am a student who is trying to write the best essay.\"\n  },\n  \"results\": [\n    {\n      \"name\": \"String Check Grader\",\n      \"type\": \"string-check-grader\",\n      \"score\": 1.0,\n      \"passed\": true,\n    }\n  ],\n  \"sample\": {\n    \"input\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are an evaluator bot...\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"You are assessing...\"\n      }\n    ],\n    \"output\": [\n      {\n        \"role\": \"assistant\",\n        \"content\": \"The rubric is not clear nor concise.\"\n      }\n    ],\n    \"finish_reason\": \"stop\",\n    \"model\": \"gpt-4o-2024-08-06\",\n    \"usage\": {\n      \"total_tokens\": 521,\n      \"completion_tokens\": 2,\n      \"prompt_tokens\": 519,\n      \"cached_tokens\": 0\n    },\n    \"error\": null,\n    \"temperature\": 1.0,\n    \"max_completion_tokens\": 2048,\n    \"top_p\": 1.0,\n    \"seed\": 42\n  }\n}\n",
  }
)
model EvalRunOutputItem {
  /** The type of the object. Always "eval.run.output_item". */
  @extension("x-stainless-const", true)
  object: "eval.run.output_item" = "eval.run.output_item";

  /** Unique identifier for the evaluation run output item. */
  id: string;

  /** The identifier of the evaluation run associated with this output item. */
  run_id: string;

  /** The identifier of the evaluation group. */
  eval_id: string;

  /** Unix timestamp (in seconds) when the evaluation run was created. */
  created_at: integer;

  /** The status of the evaluation run. */
  status: string;

  /** The identifier for the data source item. */
  datasource_item_id: integer;

  /** Details of the input data source item. */
  datasource_item: {};

  /** A list of results from the evaluation run. */
  results: {}[];

  /** A sample containing the input and output of the evaluation run. */
  sample: {
    input: {
      role: string;
      content: string;
    }[];
    output: {
      role?: string;
      content?: string;
    }[];
    finish_reason: string;
    `model`: string;
    usage: {
      total_tokens: integer;
      completion_tokens: integer;
      prompt_tokens: integer;
      cached_tokens: integer;
    };
    error: EvalApiError;
    temperature: numeric;
    max_completion_tokens: integer;
    top_p: numeric;
    seed: integer;
  };
}

/** An object representing a list of output items for an evaluation run. */
@summary("EvalRunOutputItemList")
@extension(
  "x-oaiMeta",
  #{
    name: "The eval run output item list object",
    group: "evals",
    example: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"eval.run.output_item\",\n      \"id\": \"outputitem_67abd55eb6548190bb580745d5644a33\",\n      \"run_id\": \"evalrun_67abd54d60ec8190832b46859da808f7\",\n      \"eval_id\": \"eval_67abd54d9b0081909a86353f6fb9317a\",\n      \"created_at\": 1739314509,\n      \"status\": \"pass\",\n      \"datasource_item_id\": 137,\n      \"datasource_item\": {\n          \"teacher\": \"To grade essays, I only check for style, content, and grammar.\",\n          \"student\": \"I am a student who is trying to write the best essay.\"\n      },\n      \"results\": [\n        {\n          \"name\": \"String Check Grader\",\n          \"type\": \"string-check-grader\",\n          \"score\": 1.0,\n          \"passed\": true,\n        }\n      ],\n      \"sample\": {\n        \"input\": [\n          {\n            \"role\": \"system\",\n            \"content\": \"You are an evaluator bot...\"\n          },\n          {\n            \"role\": \"user\",\n            \"content\": \"You are assessing...\"\n          }\n        ],\n        \"output\": [\n          {\n            \"role\": \"assistant\",\n            \"content\": \"The rubric is not clear nor concise.\"\n          }\n        ],\n        \"finish_reason\": \"stop\",\n        \"model\": \"gpt-4o-2024-08-06\",\n        \"usage\": {\n          \"total_tokens\": 521,\n          \"completion_tokens\": 2,\n          \"prompt_tokens\": 519,\n          \"cached_tokens\": 0\n        },\n        \"error\": null,\n        \"temperature\": 1.0,\n        \"max_completion_tokens\": 2048,\n        \"top_p\": 1.0,\n        \"seed\": 42\n      }\n    },\n  ],\n  \"first_id\": \"outputitem_67abd55eb6548190bb580745d5644a33\",\n  \"last_id\": \"outputitem_67abd55eb6548190bb580745d5644a33\",\n  \"has_more\": false\n}\n",
  }
)
model EvalRunOutputItemList {
  /** The type of this object. It is always set to "list". */
  @extension("x-stainless-const", true)
  object: "list" = "list";

  /** An array of eval run output item objects. */
  data: EvalRunOutputItem[];

  /** The identifier of the first eval run output item in the data array. */
  first_id: string;

  /** The identifier of the last eval run output item in the data array. */
  last_id: string;

  /** Indicates whether there are more eval run output items available. */
  has_more: boolean;
}
