import "@typespec/http";
import "@typespec/rest";
import "@azure-tools/typespec-autorest";
import "@typespec/versioning";
import "@azure-tools/typespec-azure-core";
import "../common/models.tsp";
import "../main.tsp";
import "@typespec/openapi";
import "./chat_messages.tsp";

using TypeSpec.Rest;
using TypeSpec.Versioning;
using Azure.Core.Foundations;

namespace Azure.AI.Projects;

@doc("Evaluator Configuration")
@added(Versions.v2025_05_15_preview)
model EvaluatorConfiguration {
  @doc("Identifier of the evaluator.")
  id: string;

  @doc("Initialization parameters of the evaluator.")
  @removed(Versions.v2025_07_31_preview)
  initParams?: Record<unknown>;

  @doc("Initial parameters of the evaluator.")
  @added(Versions.v2025_07_31_preview)
  @renamedFrom(Versions.v2025_07_31_preview, "initParams")
  initialParameters?: Record<unknown>;

  @doc("Data parameters of the evaluator.")
  dataMapping?: Record<string>;
}

#suppress "@azure-tools/typespec-azure-core/no-string-discriminator"
@doc("Abstract data class.")
@discriminator("type")
@added(Versions.v2025_05_15_preview)
model InputData {
  @doc("Type of the data")
  type: string;
}

@doc("Dataset as source for evaluation.")
@added(Versions.v2025_05_15_preview)
model InputDataset extends InputData {
  type: "dataset";

  @doc("Evaluation input data")
  id: string;
}

@doc("Unique identifier of a dataset registered in AI Foundry workspace. The identifier follows the format: azureai://accounts/{resourceName}/projects/{projectName}/data/{datasetName}/versions/{versionNumber}")
@added(Versions.v2025_07_31_preview)
scalar DatasetId extends string;

@doc("Specifies the type of data source used for evaluation. Different types support various data input methods and formats.")
@added(Versions.v2025_07_31_preview)
union EvaluationDataSourceType {
  @doc("Use inline JSON data provided directly in the request")
  InlineData: "inlineData",

  @doc("Use a dataset that has been registered and stored in AI Foundry workspace")
  Dataset: "dataset",

  @doc("Use data generated by running a model deployment against inline queries provided in the request")
  FoundryModelInline: "FoundryModelInline",

  @doc("Use data generated by running a model deployment against a registered dataset")
  FoundryModelDataset: "FoundryModelDataset",

  string,
}

@doc("Controls the level of data redaction applied to evaluation results to protect sensitive information and ensure compliance with privacy requirements.")
@added(Versions.v2025_07_31_preview)
union RedactionLevel {
  @doc("Apply redaction to sensitive data in evaluation results. This is the recommended setting for production environments to ensure data privacy.")
  Sensitive: "sensitive",

  @doc("Do not apply any data redaction to evaluation results. Use this setting only in development or testing environments where data privacy is not a concern.")
  None: "none",

  string,
}

#suppress "@azure-tools/typespec-azure-core/no-string-discriminator"
@doc("Base class for different types of evaluation data sources. Use the discriminator field 'type' to specify the specific data source implementation.")
@discriminator("type")
@added(Versions.v2025_07_31_preview)
model EvaluationDataSource {
  @doc("Specifies the type of data source being used for evaluation")
  type: EvaluationDataSourceType;
}

@doc("Represents a message that contains a query and its corresponding response.")
@added(Versions.v2025_07_31_preview)
model QueryResponseMessage {
  @doc("The query message sent to the model.")
  query: string;

  @doc("The response generated by the model in reply to the query.")
  response?: string;

  @doc("Optional context information that may include additional details about the query or response, such as metadata or processing instructions.")
  context?: string;

  @doc("Optional ground truth value for the query, which can be used to compare against the model's response during evaluation.")
  ground_truth?: string;
}

@doc("The type of inline messages used for evaluation. This union allows for different inline message formats to be specified.")
@added(Versions.v2025_07_31_preview)
union InlineMessagesType {
  @doc("Inline messages that consist of query-response pairs for evaluation. Each pair includes a query string and the corresponding response generated by the model.")
  queryResponseMessage: "queryResponseMessage",

  @doc("role content type message")
  roleContentType: "roleContentType",

  string,
}

@doc("Available formats for structuring inline evaluation data.")
@added(Versions.v2025_07_31_preview)
union InlineDataFormat {
  @doc("Query-response pairs for Q&A and chatbot evaluations")
  queryResponseMessage: "queryResponseMessage",

  @doc("Multi-turn conversations with role-based messages and tool definitions")
  chatMessages: "chatMessages",

  @doc("Flexible JSON format for custom data structures")
  inlineJson: "inlineJson",

  string,
}

@added(Versions.v2025_07_31_preview)
@doc("Base class for inline evaluation data with format discrimination.")
@discriminator("dataFormat")
model InlineData {
  @doc("Format of the inline data structure")
  dataFormat: InlineDataFormat;
}

@doc("Query-response pairs for evaluating Q&A systems and response accuracy.")
@added(Versions.v2025_07_31_preview)
model QueryResponseInlineMessages extends InlineData {
  @doc("Specifies query-response format")
  dataFormat: InlineDataFormat.queryResponseMessage;

  @doc("Array of query-response pairs with optional context and ground truth")
  messages: QueryResponseMessage[];
}

@added(Versions.v2025_07_31_preview)
@doc("Multi-turn conversations for evaluating dialogue systems and context awareness.")
model ChatMessages extends InlineData {
  @doc("Specifies chat messages format")
  dataFormat: InlineDataFormat.chatMessages;

  @doc("Array of messages representing representing queries")
  query: Message[];

  @doc("Array of messages representing responses")
  response: Message[];

  @doc("Array of tool definitions that are used in the conversation")
  toolDefinitions: AgentToolDefinition[];
}

@added(Versions.v2025_07_31_preview)
@doc("Custom JSON format for complex evaluation scenarios requiring flexible data structures.")
model InlineJson extends InlineData {
  @doc("Specifies JSON format")
  dataFormat: InlineDataFormat.inlineJson;

  @doc("Array of JSON strings with custom fields and metadata")
  messages: string[];
}

@doc("Data source using inline data provided directly in the request.")
@added(Versions.v2025_07_31_preview)
model InlineDataSource extends EvaluationDataSource {
  @doc("Specifies inline JSON data source")
  type: EvaluationDataSourceType.InlineData;

  @doc("Optional unique identifier for the inline data source. This can be an agent id or a custom identifier to distinguish between different inline data sources.")
  id?: string;

  @doc("Inline data structured according to the specified format")
  data: InlineData;
}

@doc("Data source that uses a dataset registered and stored in AI Foundry workspace. This is the recommended approach for large datasets or reusable evaluation data.")
@added(Versions.v2025_07_31_preview)
model FoundryDatasetDataSource extends EvaluationDataSource {
  @doc("Specifies that this data source uses a registered dataset")
  type: EvaluationDataSourceType.Dataset;

  @doc("Unique identifier of the dataset registered in AI Foundry workspace")
  datasetId: DatasetId;
}

@doc("Comprehensive configuration for a model deployment used in evaluation scenarios. This defines how the model will process inputs and generate responses for evaluation.")
@added(Versions.v2025_07_31_preview)
model EvaluationModelConfiguration {
  @doc("The model deployment to be evaluated. Accepts either the deployment name alone or with the connection name as '{connectionName}/modelDeploymentName'.")
  modelDeploymentName: string;

  @doc("Optional model-specific parameters to fine-tune behavior during evaluation. These may include temperature, max tokens, top-p, frequency penalty, and other model configuration options supported by the deployment.")
  modelParameters?: Record<unknown>;
}

@doc("Common properties shared across model deployment configurations used in evaluations.")
@added(Versions.v2025_07_31_preview)
model EvaluationModelSourceCommon {
  @doc("Configuration for the model deployment used in evaluation.")
  modelConfiguration: EvaluationModelConfiguration;

  @doc("A list of messages comprising the conversation so far. Each message can be a json string with role and content to specify the conversation context.")
  baseMessages: Message[];
}

@doc("Data source that uses a model deployment with inline queries. The specified model processes each query to generate responses, which are then evaluated against the configured evaluators.")
@added(Versions.v2025_07_31_preview)
model FoundryModelInlineSource extends EvaluationDataSource {
  @doc("Specifies that this data source uses a model deployment with inline queries")
  type: EvaluationDataSourceType.FoundryModelInline;

  ...EvaluationModelSourceCommon;

  @doc("Inline queries to be processed by the model deployment. The response is then evaluated against the configured evaluators.")
  queries: Array<string>;
}

@doc("Data source that uses a model deployment with a dataset containing prompts. The model processes each prompt from the dataset to generate responses, which are then evaluated against the configured evaluators.")
@added(Versions.v2025_07_31_preview)
model FoundryModelDatasetSource extends EvaluationDataSource {
  @doc("Specifies that this data source uses a model deployment with a dataset")
  type: EvaluationDataSourceType.FoundryModelDataset;

  ...EvaluationModelSourceCommon;

  @doc("Unique identifier of the dataset containing prompts that will be processed by the model deployment")
  datasetId: DatasetId;

  @doc("Name of the column in the dataset that contains the queries to be processed by the model deployment. This allows specifying which column should be used as input for evaluation. eg.. queryField: '{{item.messages}}'")
  queryField: string;
}

// @doc("Statistical aggregation methods available for summarizing evaluation results across multiple data points.")
// @added(Versions.v2025_07_31_preview)
// union AggregationMethod {
//   @doc("Calculate the sum of all values")
//   sum: "sum",

//   @doc("Calculate the arithmetic mean of all values")
//   average: "average",

//   @doc("Find the maximum value")
//   max: "max",

//   @doc("Find the minimum value")
//   min: "min",

//   @doc("Count the total number of values")
//   count: "count",

//   @doc("Calculate the nth percentile value. Needs a parameter to specify the percentile level (e.g., level: 95 for 95th percentile analysis)")
//   percentile: "percentile",

//   @doc("Calculate the standard deviation")
//   standardDeviation: "standardDeviation",

//   string,
// }

// @doc("Configuration for a specific aggregation method, allowing customization of how the aggregation is calculated.")
// @added(Versions.v2025_07_31_preview)
// model AggregationConfiguration {
//   @doc("The statistical aggregation method to apply (e.g., 'average', 'percentile', 'standardDeviation')")
//   method: AggregationMethod;

//   @doc("Optional parameters specific to the aggregation method. For example, 'populationVariance: true' for standard deviation, or 'interpolation: linear' for percentiles.")
//   parameters?: Record<unknown>;
// }

@doc("Represents system Metadata about the evaluation execution environment and resource usage.")
@added(Versions.v2025_07_31_preview)
model EvaluatorResultSystemData {
  @doc("Total number of input tokens consumed during the evaluation process. This helps track resource usage and costs.")
  inputTokenCount: int64;

  @doc("Total number of output tokens generated during the evaluation process. This helps track resource usage and costs.")
  outputTokenCount: int64;

  @doc("Total time taken to complete the evaluation, measured in milliseconds. This includes processing time and any network latency.")
  duration: int64;

  @doc("Additional details about the evaluator result")
  additionalDetails: Record<unknown>;
}

@doc("Common properties shared across all evaluator results, including metadata about the evaluation execution and resource consumption.")
@added(Versions.v2025_07_31_preview)
model EvaluatorResultCommon {
  @doc("Unique identifier of the evaluator that produced this result")
  evaluatorId: string;

  @doc("Metadata about the evaluation execution environment and resource usage.")
  systemData?: EvaluatorResultSystemData;

  @doc("Current state of the evaluation execution. Tracks progress from initiation to completion or failure.")
  state: OperationState;
}

@added(Versions.v2025_07_31_preview)
@doc("Represents an aggregated score calculated using a specific statistical method across all rows of data.")
model AggregatedScore {
  @doc("The calculated aggregated value based on the specified method. This value is undefined in case aggregation is not applicable or the method does not produce a numeric result.")
  value?: float32;

  @doc("Additional metadata about the aggregation calculation")
  metadata?: Record<unknown>;
}

@added(Versions.v2025_07_31_preview)
@doc("Statistical summary of evaluation results including aggregated scores and pass/fail rates.")
model EvaluationSummaryStatistics {
  @doc("Total number of rows that were evaluated.")
  sampleCount: int64;

  @doc("Proportion of evaluation results that failed to meet the specified criteria or threshold, expressed as a value between 0 and 1.")
  defectRate?: float32;

  @doc("Lowest score value observed across all evaluation results.")
  minScore?: float32;

  @doc("Highest score value observed across all evaluation results.")
  maxScore?: float32;

  @doc("Arithmetic mean of all score values in the evaluation results.")
  averageScore?: float32;

  @doc("Standard deviation of score values, measuring the variability or spread of the evaluation results.")
  standardDeviation?: float32;

  @doc("Lower bound of the 90% confidence interval for the evaluation scores, indicating statistical reliability of the results.")
  confidenceIntervalLower?: float32;

  @doc("Upper bound of the 90% confidence interval for the evaluation scores, indicating statistical reliability of the results.")
  confidenceIntervalUpper?: float32;
}

@doc("Aggregated summary of evaluation results for a specific evaluator across all data rows. This provides a high-level overview of the evaluator's performance.")
@added(Versions.v2025_07_31_preview)
model EvaluatorSummaryResult {
  ...EvaluatorResultCommon;

  @doc("Statistical summary of evaluation results where key is the metric id.")
  statistics?: Record<EvaluationSummaryStatistics>;
}

@doc("Specifies the type of external storage destination where evaluation results can be exported for further analysis or long-term retention.")
@added(Versions.v2025_07_31_preview)
union EvaluationDestinationConfigurationType {
  @doc("Export evaluation results to Azure Application Insights for monitoring and analytics")
  AppInsightsStorage: "appInsightsStorage",

  @doc("Export evaluation results to Azure Blob Storage for data archival and custom processing")
  StorageAccountStorage: "storageAccountStorage",

  string,
}

@doc("Base configuration for exporting evaluation results to external storage destinations. This enables integration with external analytics and monitoring systems.")
@added(Versions.v2025_07_31_preview)
@discriminator("type")
model EvaluationDestinationConfiguration {
  @doc("Specifies the type of external storage destination")
  type: EvaluationDestinationConfigurationType;

  @doc("Name of the connection resource configured in the AI Foundry workspace. This connection must be properly configured with appropriate credentials before use.")
  connectionName: string;

  @doc("Current state of the export operation to this destination. This field is managed by the service and cannot be modified by clients.")
  @visibility(Lifecycle.Read)
  state?: OperationState;
}

@doc("Configuration for exporting evaluation results to Azure Application Insights. This enables monitoring, alerting, and analytics on evaluation performance and trends.")
@added(Versions.v2025_07_31_preview)
model AppInsightsDestinationConfiguration
  extends EvaluationDestinationConfiguration {
  @doc("Specifies Azure Application Insights as the export destination")
  type: EvaluationDestinationConfigurationType.AppInsightsStorage;
}

@doc("Configuration for exporting evaluation results to Azure Blob Storage. This provides long-term storage and enables custom data processing workflows.")
@added(Versions.v2025_07_31_preview)
model StorageAccountDestinationConfiguration
  extends EvaluationDestinationConfiguration {
  @doc("Specifies Azure Blob Storage as the export destination")
  type: EvaluationDestinationConfigurationType.StorageAccountStorage;

  @doc("Name of the container within the Azure Blob Storage account where evaluation results will be stored")
  containerName: string;
}

@doc("Configuration settings that control how evaluation results are processed, stored, and exported. These settings affect data privacy, retention, and integration with external systems.")
@added(Versions.v2025_07_31_preview)
model EvaluationResultSettings {
  @doc("Level of data redaction to apply to evaluation results. Defaults to 'sensitive' in production environments to protect confidential information.")
  redactionLevel?: RedactionLevel;

  @doc("List of external storage destinations where evaluation results should be exported in addition to the default AI Foundry storage")
  @added(Versions.v2025_07_31_preview)
  additionalDestinations?: Array<EvaluationDestinationConfiguration>;
}

@doc("One Evaluation Definition")
@added(Versions.v2025_07_31_preview)
model OneEvaluation {
  @doc("Input data to evaluate. Single row support only.")
  dataSource: InlineDataSource;

  @doc("Evaluators to be used for the evaluation.")
  evaluators: Record<EvaluatorConfiguration>;

  @doc("External storage destinations where evaluation results can be exported.")
  additionalDestinations?: EvaluationDestinationConfiguration;

  @doc("Evaluation's properties. Unlike tags, properties are add-only. Once added, a property cannot be removed.")
  properties?: Record<string>;
}

@doc("Evaluation Definition")
@resource("runs")
@added(Versions.v2025_05_15_preview)
model Evaluation {
  @doc("Identifier of the evaluation.")
  @key("name")
  @encodedName("application/json", "id")
  @visibility(Lifecycle.Read)
  name: string;

  @doc("Data for evaluation.")
  @removed(Versions.v2025_07_31_preview) // Replaced by `dataSource`
  data: InputData;

  @doc("Data source configuration that specifies where the evaluation data comes from. This replaces the legacy 'data' field and provides more flexible data source options.")
  @added(Versions.v2025_07_31_preview)
  dataSource: EvaluationDataSource;

  @doc("Configuration settings that control how evaluation results are processed, stored, and exported")
  @added(Versions.v2025_07_31_preview)
  resultSettings?: EvaluationResultSettings;

  @doc("Display Name for evaluation. It helps to find the evaluation easily in AI Foundry. It does not need to be unique.")
  displayName?: string;

  @doc("Description of the evaluation. It can be used to store additional information about the evaluation and is mutable.")
  description?: string;

  @doc("Status of the evaluation. It is set by service and is read-only.")
  @visibility(Lifecycle.Read)
  @removed(Versions.v2025_07_31_preview) // Replaced by `state` in 2025-07-31-preview
  status?: string;

  @doc("Current operational state of the evaluation. This field is managed by the service and reflects the evaluation's progress from initiation to completion.")
  @visibility(Lifecycle.Read)
  @added(Versions.v2025_07_31_preview)
  state: OperationState;

  @doc("Evaluation's tags. Unlike properties, tags are fully mutable.")
  tags?: Record<string>;

  @doc("Evaluation's properties. Unlike tags, properties are add-only. Once added, a property cannot be removed.")
  properties?: Record<string>;

  @doc("Evaluators to be used for the evaluation.")
  evaluators: Record<EvaluatorConfiguration>;

  @doc("Specifies the type and configuration of the entity used for this evaluation.")
  @removed(Versions.v2025_07_31_preview)
  target?: EvaluationTarget;

  @doc("Aggregated summary of evaluation results for each configured evaluator. This provides a high-level overview of performance across all data rows and is available once the evaluation completes.")
  @visibility(Lifecycle.Read)
  @added(Versions.v2025_07_31_preview)
  summary?: Record<EvaluatorSummaryResult>;

  @doc("""
     Unique identifier of the dataset containing detailed evaluation results. This dataset is created automatically upon evaluation completion and contains row-by-row results for analysis.
     The identifier follows the format: azureai://accounts/{resourceName}/projects/{projectName}/datasets/{datasetName}/versions/{versionNumber}.
     Note: This dataset is only available when the result retention policy permits detailed result storage.
    """)
  @added(Versions.v2025_07_31_preview)
  @visibility(Lifecycle.Read)
  resultDatasetId?: DatasetId;

  @doc("System-generated metadata containing internal service information for debugging and operational purposes. This field is managed by the service and cannot be modified by clients.")
  @visibility(Lifecycle.Read)
  @added(Versions.v2025_07_31_preview)
  systemData?: Record<string>;
}

@doc("Definition for sampling strategy.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluationSamplingConfiguration {
  @doc("Name of the sampling strategy.")
  name: string;

  @doc("Percentage of sampling per hour (0-100).")
  samplingPercent: float32;

  @doc("Maximum request rate per hour (0 to 1000).")
  maxRequestRate: float32;
}

@doc("The redaction configuration will allow the user to control what is redacted.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluationRedactionConfiguration {
  @doc("Redact score properties. If not specified, the default is to redact in production.")
  redactScoreProperties?: boolean;
}

@doc("Evaluation request for agent run.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluationRequest {
  @doc("Identifier of the agent run.")
  runId: string;

  @doc("Identifier of the agent thread. This field is mandatory currently, but it will be optional in the future.")
  threadId?: string;

  @doc("Evaluators to be used for the evaluation.")
  evaluators: Record<EvaluatorConfiguration>;

  @doc("Sampling configuration for the evaluation.")
  samplingConfiguration?: AgentEvaluationSamplingConfiguration;

  @doc("Redaction configuration for the evaluation.")
  redactionConfiguration?: AgentEvaluationRedactionConfiguration;

  @doc("Pass the AppInsights connection string to the agent evaluation for the evaluation results and the errors logs.")
  appInsightsConnectionString: string;
}

@doc("Result for the agent evaluation evaluator run.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluationResult {
  @doc("Evaluator's name. This is the name of the evaluator that was used to evaluate the agent's completion.")
  evaluator: string;

  @doc("Identifier of the evaluator.")
  evaluatorId: string;

  @doc("Score of the given evaluator. No restriction on range.")
  score: float32;

  @doc("Status of the evaluator result. Options: Running, Completed, Failed, NotApplicable.")
  status: string;

  @doc("Reasoning for the evaluation result.")
  reason?: string;

  @doc("Version of the evaluator that was used to evaluate the agent's completion.")
  version?: string;

  @doc("The unique identifier of the thread.")
  threadId?: string;

  @doc("The unique identifier of the run.")
  runId: string;

  @doc("A string explaining why there was an error, if applicable.")
  error?: string;

  @doc("Additional properties relevant to the evaluator. These will differ between evaluators.")
  additionalDetails?: Record<string>;
}

@doc("Evaluation response for agent evaluation run.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluation {
  @doc("Identifier of the agent evaluation run.")
  id: string;

  @doc("Status of the agent evaluation. Options: Running, Completed, Failed.")
  status: string;

  @doc("The reason of the request failure for the long running process, if applicable.")
  error?: string;

  @doc("The agent evaluation result.")
  result?: Array<AgentEvaluationResult>;
}

@doc("Allowed types of evaluation targets.")
@added(Versions.v2025_05_15_preview)
@removed(Versions.v2025_07_31_preview)
union EvaluationTargetType {
  @doc("Evaluation target that uses a model for response generation.")
  modelResponseGeneration: "modelResponseGeneration",

  string,
}

@doc("Abstract base model for defining evaluation targets.")
@discriminator("type")
@added(Versions.v2025_05_15_preview)
@removed(Versions.v2025_07_31_preview)
model EvaluationTarget {
  @doc("Discriminator that defines the type of the evaluation target.")
  type: EvaluationTargetType;
}

@doc("Evaluation target for generating responses using a given model and dataset.")
@added(Versions.v2025_05_15_preview)
@removed(Versions.v2025_07_31_preview)
model modelResponseGenerationTarget extends EvaluationTarget {
  @doc("The type of evaluation target. Always 'modelResponseGeneration'.")
  type: EvaluationTargetType.modelResponseGeneration;

  @doc("A list of messages comprising the conversation so far.")
  baseMessages: Message[];

  @doc("The model deployment to be evaluated. Accepts either the deployment name alone or with the connection name as '{connectionName}/modelDeploymentName'.")
  modelDeploymentName: string;

  @doc("Optional parameters passed to the model for evaluation.")
  modelParams: Record<unknown>;
}

@doc("Defines the possible outcomes of an evaluation result, indicating whether the evaluated data meets the specified criteria or standards.")
@added(Versions.v2025_07_31_preview)
union EvaluationResultOutcome {
  @doc("The evaluation completed successfully and met the specified criteria or threshold")
  pass: "pass",

  @doc("The evaluation completed but did not meet the specified criteria or threshold")
  fail: "fail",

  @doc("The evaluation has not been processed yet or is still in progress")
  notEvaluated: "notEvaluated",

  @doc("The evaluation criteria do not apply to this particular data row or context")
  notApplicable: "notApplicable",

  string,
}

@doc("Detailed result of a specific evaluator's assessment on a single data row. Contains the evaluation outcome, score, reasoning, and additional metrics.")
@added(Versions.v2025_07_31_preview)
model EvaluatorResult {
  ...EvaluatorResultCommon;

  @doc("Score for the evaluation result")
  score?: Record<EvaluationScore>;
}

@doc("Describes the desirable direction for the evaluation score. This indicates whether a higher or lower score is preferred for this evaluator.")
@added(Versions.v2025_07_31_preview)
union EvaluatorDesirableDirection {
  @doc("Indicates that a higher score is desirable for this evaluator")
  increase: "increase",

  @doc("Indicates that a lower score is desirable for this evaluator")
  decrease: "decrease",

  @doc("Indicates that the score should be neutral, meaning it does not have a preferred direction")
  neutral: "neutral",

  string,
}

@added(Versions.v2025_07_31_preview)
@doc("Type of score assigned by the evaluator. This indicates whether the score is boolean, continuous, or ordinal.")
union EvaluatorScoreType {
  @doc("Boolean score type, where the score is either true or false")
  boolean: "boolean",

  @doc("Continuous score type, where the score is a floating-point number representing a continuous value")
  continuous: "continuous",

  @doc("Ordinal score type, where the score represents an ordered category or rank")
  ordinal: "ordinal",

  string,
}

@added(Versions.v2025_07_31_preview)
@doc("Score assigned by the evaluator to a specific data row")
model EvaluationScore {
  #suppress "@azure-tools/typespec-autorest/union-unsupported" "External API shape is defined in OpenAPI 3.0 as oneOf."
  @doc("Score of the evaluation.")
  label?: boolean | string | float32;

  @doc("Justification for the score, providing context on how it was derived or what it represents")
  justification: string;

  @doc("Threshold value that this score is compared against to determine the evaluation outcome")
  threshold?: float32;

  @doc("Direction of the score that indicates whether a higher or lower score is desirable for this evaluator")
  desirableDirection?: EvaluatorDesirableDirection;

  @doc("Type of the score that indicates whether it is a boolean, continuous, or ordinal score")
  scoreType?: EvaluatorScoreType;

  @doc("Outcome of the evaluation based on the score and threshold. Indicates whether the score meets, exceeds, or falls below expectations")
  outcome?: EvaluationResultOutcome;

  @doc("Optional additional details that may include metadata or other relevant information about the score")
  additionalDetails?: Record<unknown>;
}

@doc("Comprehensive evaluation results for a single data row, including the input data, processing state, and results from all configured evaluators.")
@added(Versions.v2025_07_31_preview)
@resource("runs/{evaluationId}/results")
model EvaluationResult {
  @doc("Unique identifier for this evaluation result row")
  @key("id")
  @visibility(Lifecycle.Read)
  id: string;

  @doc("Current processing state of this data row. Tracks progress from initial processing through completion or failure.")
  state: OperationState;

  @doc("Error message describing why the evaluation failed for this data row, if applicable")
  error?: string;

  @doc("Original input data for this row in JSON string format. This preserves the exact data that was evaluated for reference and debugging.")
  @added(Versions.v2025_07_31_preview)
  inputDataJson?: string;

  @doc("Evaluation results from all the configured evaluators for this data row.")
  evaluatorResults: Record<EvaluatorResult>;
}
