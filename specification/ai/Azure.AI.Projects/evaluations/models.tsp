import "@typespec/http";
import "@typespec/rest";
import "@azure-tools/typespec-autorest";
import "@typespec/versioning";
import "@azure-tools/typespec-azure-core";
import "../common/models.tsp";
import "../main.tsp";
import "@typespec/openapi";
import "./chat_messages.tsp";

using TypeSpec.Rest;
using TypeSpec.Versioning;
using Azure.Core.Foundations;

namespace Azure.AI.Projects;

@doc("Evaluator Configuration")
@added(Versions.v2025_05_15_preview)
model EvaluatorConfiguration {
  @doc("Identifier of the evaluator.")
  id: string;

  @doc("Name of the evaluator. It can be used for identify the evaluator in the evaluation results.")
  @added(Versions.v2025_07_31_preview)
  name?: string;

  @doc("Initialization parameters of the evaluator.")
  @removed(Versions.v2025_07_31_preview)
  initParams?: Record<unknown>;

  @doc("Initialization parameters of the evaluator.")
  @added(Versions.v2025_07_31_preview)
  @renamedFrom(Versions.v2025_07_31_preview, "initParams")
  initializationParameters?: Record<unknown>;

  @doc("Data parameters of the evaluator.")
  dataMapping?: Record<string>;
}

#suppress "@azure-tools/typespec-azure-core/no-string-discriminator"
@doc("Abstract data class.")
@discriminator("type")
@added(Versions.v2025_05_15_preview)
model InputData {
  @doc("Type of the data")
  type: string;
}

@doc("Dataset as source for evaluation.")
@added(Versions.v2025_05_15_preview)
model InputDataset extends InputData {
  type: "dataset";

  @doc("Evaluation input data")
  id: string;
}

@doc("Unique identifier of a dataset registered in AI Foundry workspace. The identifier follows the format: azureai://accounts/{resourceName}/projects/{projectName}/data/{datasetName}/versions/{versionNumber}")
@added(Versions.v2025_07_31_preview)
scalar DatasetId extends string;

@doc("Specifies the type of data source used for evaluation. Different types support various data input methods and formats.")
@added(Versions.v2025_07_31_preview)
union EvaluationDataSourceType {
  @doc("Use inline JSON data provided directly in the request")
  InlineData: "inlineData",

  @doc("Use a dataset that has been registered and stored in AI Foundry workspace")
  Dataset: "dataset",

  @doc("Use data generated by running a model deployment against inline queries provided in the request")
  FoundryModelInline: "FoundryModelInline",

  @doc("Use data generated by running a model deployment against a registered dataset")
  FoundryModelDataset: "FoundryModelDataset",

  string,
}

#suppress "@azure-tools/typespec-azure-core/no-string-discriminator"
@doc("Base class for different types of evaluation data sources. Use the discriminator field 'type' to specify the specific data source implementation.")
@discriminator("type")
@added(Versions.v2025_07_31_preview)
model EvaluationDataSource {
  @doc("Specifies the type of data source being used for evaluation")
  type: EvaluationDataSourceType;
}

@doc("Represents a message that contains a query and its corresponding response.")
@added(Versions.v2025_07_31_preview)
model QueryResponseMessage {
  @doc("The query message sent to the model.")
  query: string;

  @doc("The response generated by the model in reply to the query.")
  response?: string;

  @doc("Optional context information that may include additional details about the query or response, such as metadata or processing instructions.")
  context?: string;

  @doc("Optional ground truth value for the query, which can be used to compare against the model's response during evaluation.")
  ground_truth?: string;
}

@doc("Available formats for structuring inline evaluation data.")
@added(Versions.v2025_07_31_preview)
union InlineDataFormat {
  @doc("Query-response pairs for Q&A and chatbot evaluations")
  queryResponseMessageFormat: "queryResponseMessageFormat",

  @doc("Multi-turn conversations with role-based messages and tool definitions")
  evaluationMessagesFormat: "evaluationMessagesFormat",

  @doc("Flexible JSON format for custom data structures")
  inlineJsonFormat: "inlineJsonFormat",

  string,
}

@added(Versions.v2025_07_31_preview)
@doc("Base class for inline evaluation data with format discrimination.")
@discriminator("dataFormat")
model InlineData {
  @doc("Format of the inline data structure")
  dataFormat: InlineDataFormat;
}

@doc("Query-response pairs for evaluating Q&A systems and response accuracy.")
@added(Versions.v2025_07_31_preview)
model QueryResponseInlineMessages extends InlineData {
  @doc("Specifies query-response format")
  dataFormat: InlineDataFormat.queryResponseMessageFormat;

  @doc("Array of query-response pairs with optional context and ground truth")
  messages: QueryResponseMessage[];
}

@added(Versions.v2025_07_31_preview)
@doc("Multi-turn conversations for evaluating dialogue systems and context awareness.")
model EvaluatorMessages extends InlineData {
  @doc("Specifies evaluation messages format")
  dataFormat: InlineDataFormat.evaluationMessagesFormat;

  @doc("Array of messages representing representing queries")
  query: Message[];

  @doc("Array of messages representing responses")
  response: Message[];

  @doc("Array of tool definitions that are used in the conversation")
  toolDefinitions: AgentToolDefinition[];
}

@added(Versions.v2025_07_31_preview)
@doc("Custom JSON format for complex evaluation scenarios requiring flexible data structures.")
model InlineJson extends InlineData {
  @doc("Specifies JSON format")
  dataFormat: InlineDataFormat.inlineJsonFormat;

  @doc("Array of JSON strings with custom fields and metadata")
  messages: string[];
}

@doc("Data source using inline data provided directly in the request. Size limit is 2 MB")
@added(Versions.v2025_07_31_preview)
model InlineDataSource extends EvaluationDataSource {
  @doc("Specifies inline JSON data source")
  type: EvaluationDataSourceType.InlineData;

  @doc("Optional unique identifier for the inline data source. This can be an agent id or a custom identifier to distinguish between different inline data sources.")
  id?: string;

  @doc("Inline data structured according to the specified format")
  @visibility(Lifecycle.Create)
  data?: InlineData;

  @doc("Dataset id for the uploaded inline data.")
  @visibility(Lifecycle.Read)
  inlineDatasetId: DatasetId;
}

@doc("Data source that uses a dataset registered and stored in AI Foundry workspace. This is the recommended approach for large datasets or reusable evaluation data.")
@added(Versions.v2025_07_31_preview)
model FoundryDatasetDataSource extends EvaluationDataSource {
  @doc("Specifies that this data source uses a registered dataset")
  type: EvaluationDataSourceType.Dataset;

  @doc("Unique identifier of the dataset registered in AI Foundry workspace")
  datasetId: DatasetId;
}

alias EvaluationModelSourceCommon = {
  @doc("Configuration for the model deployment used in evaluation.")
  modelTarget: TargetConfig;

  @doc("A list of messages comprising the conversation so far. Each message can be a json string with role and content to specify the conversation context.")
  baseMessages: Message[];
};

@doc("Data source that uses a model deployment with inline queries. The specified model processes each query to generate responses, which are then evaluated against the configured evaluators.")
@added(Versions.v2025_07_31_preview)
model FoundryModelInlineSource extends EvaluationDataSource {
  @doc("Specifies that this data source uses a model deployment with inline queries")
  type: EvaluationDataSourceType.FoundryModelInline;

  ...EvaluationModelSourceCommon;

  @doc("Inline queries to be processed by the model deployment. The response is then evaluated against the configured evaluators.")
  queries: Array<string>;
}

@doc("Data source that uses a model deployment with a dataset containing prompts. The model processes each prompt from the dataset to generate responses, which are then evaluated against the configured evaluators.")
@added(Versions.v2025_07_31_preview)
model FoundryModelDatasetSource extends EvaluationDataSource {
  @doc("Specifies that this data source uses a model deployment with a dataset")
  type: EvaluationDataSourceType.FoundryModelDataset;

  ...EvaluationModelSourceCommon;

  @doc("Unique identifier of the dataset containing prompts that will be processed by the model deployment")
  datasetId: DatasetId;

  @doc("Name of the column in the dataset that contains the queries to be processed by the model deployment. This allows specifying which column should be used as input for evaluation. eg.. queryField: '\${data.query}'")
  queryField: string;
}

@added(Versions.v2025_07_31_preview)
@doc("Confidence interval for the metric.")
model ConfidenceInterval {
  @doc("Lower bound of the confidence interval for the average score, providing a range within which the true average is likely to fall.")
  lowerBound: float32;

  @doc("Upper bound of the confidence interval for the average score, providing a range within which the true average is likely to fall.")
  upperBound: float32;
}

@added(Versions.v2025_07_31_preview)
@doc("Statistical summary of evaluation results including aggregated scores and pass/fail rates.")
model EvaluationSummaryStatistics {
  @doc("Total number of rows that were evaluated.")
  sampleCount: safeint;

  @doc("Proportion of evaluation results that passed the specified criteria or threshold, expressed as a value between 0 and 1.")
  passRate?: float32;

  @doc("Lowest score value observed across all evaluation results.")
  min?: float32;

  @doc("Highest score value observed across all evaluation results.")
  max?: float32;

  @doc("Arithmetic mean of all score values in the evaluation results.")
  average?: float32;

  @doc("Standard deviation of score values, indicating the variability or spread of scores around the average.")
  standardDeviation?: float32;

  @doc("Confidence interval for the average score, providing a range within which the pass average is likely to fall (95th percentile).")
  confidenceInterval95th?: ConfidenceInterval;

  @doc("Label frequency distribution, showing how often each label appears in the evaluation results. This can help identify common themes or issues across evaluated data rows.")
  labelFrequency?: Record<safeint>;
}

@doc("Status of evaluator across all data rows.")
@added(Versions.v2025_07_31_preview)
model EvaluatorStatusResult {
  @doc("Name of the evaluator from the input request.")
  name: string;

  @doc("Identifier of the evaluator from the input request.")
  id: string;

  @doc("State of the evaluation for this evaluator.")
  state: OperationState;

  @doc("Error message if the evaluator failed to process.")
  error?: string;

  @doc("Usage statistics for the evaluation")
  usage: Usage;
}

@doc("Specifies the type of external storage destination where evaluation results can be exported for further analysis or long-term retention.")
@added(Versions.v2025_07_31_preview)
union EvaluationDestinationConfigurationType {
  @doc("Export evaluation results to Azure Application Insights for monitoring and analytics")
  AppInsightsStorage: "appInsightsStorage",

  @doc("Export evaluation results to Azure Blob Storage for data archival and custom processing")
  StorageAccountStorage: "storageAccountStorage",

  string,
}

@doc("Base configuration for exporting evaluation results to external storage destinations. This enables integration with external analytics and monitoring systems.")
@added(Versions.v2025_07_31_preview)
@discriminator("type")
model EvaluationDestinationConfiguration {
  @doc("Specifies the type of external storage destination")
  type: EvaluationDestinationConfigurationType;

  @doc("Name of the connection resource configured in the AI Foundry workspace. This connection must be properly configured with appropriate credentials before use.")
  connectionName: string;

  @doc("Current state of the export operation to this destination. This field is managed by the service and cannot be modified by clients.")
  @visibility(Lifecycle.Read)
  state?: OperationState;
}

@doc("Configuration for exporting evaluation results to Azure Application Insights. This enables monitoring, alerting, and analytics on evaluation performance and trends.")
@added(Versions.v2025_07_31_preview)
model AppInsightsDestinationConfiguration
  extends EvaluationDestinationConfiguration {
  @doc("Specifies Azure Application Insights as the export destination")
  type: EvaluationDestinationConfigurationType.AppInsightsStorage;
}

@doc("Configuration for exporting evaluation results to Azure Blob Storage. This provides long-term storage and enables custom data processing workflows.")
@added(Versions.v2025_07_31_preview)
model StorageAccountDestinationConfiguration
  extends EvaluationDestinationConfiguration {
  @doc("Specifies Azure Blob Storage as the export destination")
  type: EvaluationDestinationConfigurationType.StorageAccountStorage;

  @doc("Name of the container within the Azure Blob Storage account where evaluation results will be stored")
  containerName: string;
}

@doc("Configuration settings that control how evaluation results are processed, stored, and exported. These settings affect data privacy, retention, and integration with external systems.")
@added(Versions.v2025_07_31_preview)
model EvaluationResultSettings {
  @doc("List of external storage destinations where evaluation results should be exported in addition to the default AI Foundry storage")
  @added(Versions.v2025_07_31_preview)
  additionalDestinations?: Array<EvaluationDestinationConfiguration>;
}

@doc("cccc Evaluation Definition")
@added(Versions.v2025_07_31_preview)
model SingleEvaluation {
  @doc("Input data to evaluate. Single row support only.")
  dataSource: InlineDataSource;

  @doc("Evaluators to be used for the evaluation.")
  evaluators: EvaluatorConfiguration[];

  @doc("Evaluation's properties. Unlike tags, properties are add-only. Once added, a property cannot be removed.")
  properties?: Record<string>;
}

@doc("Single Evaluation Result")
@added(Versions.v2025_07_31_preview)
model SingleEvaluationResult {
  @doc("Evaluation results from all the configured evaluators.")
  metrics?: EvaluationMetric[];
}

@doc("Aggregated metric that summarizes evaluation results across multiple data rows.")
@added(Versions.v2025_07_31_preview)
model AggregatedMetric {
  @doc("Name of the metric")
  name: string;

  @doc("Name provided for the evaluator in the input request.")
  evaluatorName: string;

  @doc("Statistical summary of evaluation results including aggregated scores and pass/fail rates.")
  statistics: EvaluationSummaryStatistics;

  @doc("Metadata about the metric")
  metadata: EvaluationResultMetadata;

  @doc("Additional metadata about the aggregated metric, such as processing details or custom fields")
  additionalDetails?: Record<unknown>;
}

@added(Versions.v2025_07_31_preview)
@doc("Summary of evaluation results, including aggregated metrics and evaluator statuses.")
model EvaluationSummaryResult {
  @doc("Status of each evaluator from the input request.")
  evaluatorStatus: EvaluatorStatusResult[];

  @doc("Aggregated metrics that summarize evaluation results across multiple data rows. This provides a high-level overview of performance across all data rows.")
  metrics?: AggregatedMetric[];
}

@doc("Definition for evaluation data source configuration.")
@added(Versions.v2025_05_15_preview)
model DataSourceConfiguration {
  @doc("Type of the data source.")
  type: string;

  @doc("Schema of the data source.")
  schema?: Record<unknown>;
}

@doc("Evaluation Definition")
@resource("runs")
@added(Versions.v2025_05_15_preview)
model Evaluation {
  @doc("Identifier of the evaluation.")
  @key("id")
  @renamedFrom(Versions.v2025_07_31_preview, "name")
  @encodedName("application/json", "id")
  @visibility(Lifecycle.Read)
  id: string;

  @doc("Data for evaluation.")
  @removed(Versions.v2025_07_31_preview) // Replaced by `dataSource`
  data: InputData;

  @doc("Data source configuration that specifies where the evaluation data comes from. This replaces the legacy 'data' field and provides more flexible data source options.")
  @added(Versions.v2025_07_31_preview)
  dataSource: EvaluationDataSource;

  @doc("Configuration settings that control how evaluation results are processed, stored, and exported")
  @added(Versions.v2025_07_31_preview)
  resultSettings?: EvaluationResultSettings;

  @doc("Display Name for evaluation. It helps to find the evaluation easily in AI Foundry. It does not need to be unique.")
  displayName?: string;

  @doc("Description of the evaluation. It can be used to store additional information about the evaluation and is mutable.")
  description?: string;

  @doc("Status of the evaluation. It is set by service and is read-only.")
  @visibility(Lifecycle.Read)
  @removed(Versions.v2025_07_31_preview) // Replaced by `state` in 2025-07-31-preview
  status?: string;

  @doc("Current operational state of the evaluation. This field is managed by the service and reflects the evaluation's progress from initiation to completion.")
  @visibility(Lifecycle.Read)
  @added(Versions.v2025_07_31_preview)
  state: OperationState;

  @doc("Evaluation's tags. Unlike properties, tags are fully mutable.")
  tags?: Record<string>;

  @doc("Evaluation's properties. Unlike tags, properties are add-only. Once added, a property cannot be removed.")
  properties?: Record<string>;

  @doc("Evaluators to be used for the evaluation.")
  @typeChangedFrom(Versions.v2025_07_31_preview, Record<EvaluatorConfiguration>)
  evaluators: EvaluatorConfiguration[];

  @doc("Overrides the judge model used to grade this evaluation.")
  @added(Versions.v2025_07_31_preview)
  gradeWith?: string;

  @doc("Specifies the type and configuration of the entity used for this evaluation.")
  @removed(Versions.v2025_07_31_preview)
  target?: EvaluationTarget;

  @doc("Aggregated summary of evaluation results for each configured evaluator. This provides a high-level overview of performance across all data rows.")
  @visibility(Lifecycle.Read)
  @added(Versions.v2025_07_31_preview)
  summary?: EvaluationSummaryResult;

  @doc("""
     Unique identifier of the dataset containing detailed evaluation results. This dataset is created automatically upon evaluation completion and contains row-by-row results for analysis.
     The identifier follows the format: azureai://accounts/{resourceName}/projects/{projectName}/datasets/{datasetName}/versions/{versionNumber}.
     Note: This dataset is only available when the result retention policy permits detailed result storage.
    """)
  @added(Versions.v2025_07_31_preview)
  @visibility(Lifecycle.Read)
  resultDatasetId?: DatasetId;

  @doc("System-generated metadata containing internal service information for debugging and operational purposes. This field is managed by the service and cannot be modified by clients.")
  @visibility(Lifecycle.Read)
  @added(Versions.v2025_07_31_preview)
  systemData?: Record<string>;
}

@doc("Evaluation Group Definition")
@resource("groups")
@added(Versions.v2025_07_31_preview)
model EvaluationGroup {
  @doc("Identifier of the evaluation group.")
  @key("name")
  @encodedName("application/json", "id")
  @visibility(Lifecycle.Read)
  name: string;

  @doc("Display Name for evaluation group. It helps to find the evaluation group easily in AI Foundry. It does not need to be unique.")
  displayName?: string;

  @doc("Description of the evaluation group. It can be used to store additional information about the evaluation group and is mutable.")
  description?: string;

  @doc("Evaluation Group's tags. Unlike properties, tags are fully mutable.")
  tags?: Record<string>;

  @doc("Evaluation Group's properties. Unlike tags, properties are add-only. Once added, a property cannot be removed.")
  properties?: Record<string>;

  @doc("Data source configuration for the evaluation group.")
  dataSourceConfig: DataSourceConfiguration;

  @doc("Evaluation Runs under the group. It is mutable.")
  evaluationRuns?: Record<string>;

  @doc("Evaluators to be used for the evaluation.")
  evaluators: Record<EvaluatorConfiguration>;
}

@doc("Definition for sampling strategy.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluationSamplingConfiguration {
  @doc("Name of the sampling strategy.")
  name: string;

  @doc("Percentage of sampling per hour (0-100).")
  samplingPercent: float32;

  @doc("Maximum request rate per hour (0 to 1000).")
  maxRequestRate: float32;
}

@doc("The redaction configuration will allow the user to control what is redacted.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluationRedactionConfiguration {
  @doc("Redact score properties. If not specified, the default is to redact in production.")
  redactScoreProperties?: boolean;
}

@doc("Evaluation request for agent run.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluationRequest {
  @doc("Identifier of the agent run.")
  runId: string;

  @doc("Identifier of the agent thread. This field is mandatory currently, but it will be optional in the future.")
  threadId?: string;

  @doc("Evaluators to be used for the evaluation.")
  evaluators: Record<EvaluatorConfiguration>;

  @doc("Sampling configuration for the evaluation.")
  samplingConfiguration?: AgentEvaluationSamplingConfiguration;

  @doc("Redaction configuration for the evaluation.")
  redactionConfiguration?: AgentEvaluationRedactionConfiguration;

  @doc("Pass the AppInsights connection string to the agent evaluation for the evaluation results and the errors logs.")
  appInsightsConnectionString: string;
}

@doc("Result for the agent evaluation evaluator run.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluationResult {
  @doc("Evaluator's name. This is the name of the evaluator that was used to evaluate the agent's completion.")
  evaluator: string;

  @doc("Identifier of the evaluator.")
  evaluatorId: string;

  @doc("Score of the given evaluator. No restriction on range.")
  score: float32;

  @doc("Status of the evaluator result. Options: Running, Completed, Failed, NotApplicable.")
  status: string;

  @doc("Reasoning for the evaluation result.")
  reason?: string;

  @doc("Version of the evaluator that was used to evaluate the agent's completion.")
  version?: string;

  @doc("The unique identifier of the thread.")
  threadId?: string;

  @doc("The unique identifier of the run.")
  runId: string;

  @doc("A string explaining why there was an error, if applicable.")
  error?: string;

  @doc("Additional properties relevant to the evaluator. These will differ between evaluators.")
  additionalDetails?: Record<string>;
}

@doc("Evaluation response for agent evaluation run.")
@added(Versions.v2025_05_15_preview)
model AgentEvaluation {
  @doc("Identifier of the agent evaluation run.")
  id: string;

  @doc("Status of the agent evaluation. Options: Running, Completed, Failed.")
  status: string;

  @doc("The reason of the request failure for the long running process, if applicable.")
  error?: string;

  @doc("The agent evaluation result.")
  result?: Array<AgentEvaluationResult>;
}

@doc("Allowed types of evaluation targets.")
@added(Versions.v2025_05_15_preview)
@removed(Versions.v2025_07_31_preview)
union EvaluationTargetType {
  @doc("Evaluation target that uses a model for response generation.")
  modelResponseGeneration: "modelResponseGeneration",

  string,
}

@doc("Abstract base model for defining evaluation targets.")
@discriminator("type")
@added(Versions.v2025_05_15_preview)
@removed(Versions.v2025_07_31_preview)
model EvaluationTarget {
  @doc("Discriminator that defines the type of the evaluation target.")
  type: EvaluationTargetType;
}

@doc("Evaluation target for generating responses using a given model and dataset.")
@added(Versions.v2025_05_15_preview)
@removed(Versions.v2025_07_31_preview)
model modelResponseGenerationTarget extends EvaluationTarget {
  @doc("The type of evaluation target. Always 'modelResponseGeneration'.")
  type: EvaluationTargetType.modelResponseGeneration;

  @doc("A list of messages comprising the conversation so far.")
  baseMessages: Message[];

  @doc("The model deployment to be evaluated. Accepts either the deployment name alone or with the connection name as '{connectionName}/modelDeploymentName'.")
  modelDeploymentName: string;

  @doc("Optional parameters passed to the model for evaluation.")
  modelParams: Record<unknown>;
}

@doc("Defines the possible outcomes of an evaluation result, indicating whether the evaluated data meets the specified criteria or standards.")
@added(Versions.v2025_07_31_preview)
union EvaluationResultOutcome {
  @doc("The evaluation completed successfully and met the specified criteria or threshold")
  pass: "pass",

  @doc("The evaluation completed but did not meet the specified criteria or threshold")
  fail: "fail",

  @doc("The evaluation resulted in an error for this particular data row")
  error: "error",

  string,
}

@doc("Describes the desirable direction for the evaluation score. This indicates whether a higher or lower score is preferred for this evaluator.")
@added(Versions.v2025_07_31_preview)
union EvaluatorDesirableDirection {
  @doc("Indicates that a higher score is desirable for this evaluator")
  increase: "increase",

  @doc("Indicates that a lower score is desirable for this evaluator")
  decrease: "decrease",

  @doc("Indicates that the score should be neutral, meaning it does not have a preferred direction")
  neutral: "neutral",

  string,
}

@added(Versions.v2025_07_31_preview)
@doc("Usage statistics for the evaluation")
model Usage {
  @doc("Total number of input tokens consumed during the evaluation process")
  inputTokens: safeint;

  @doc("Total number of output tokens generated during the evaluation process")
  outputTokens: safeint;
}

@added(Versions.v2025_07_31_preview)
@doc("Metadata about the evaluation result")
model EvaluationResultMetadata {
  @doc("Unique identifier of the evaluator that produced this result")
  evaluatorId: string;

  @doc("Desired direction for the evaluation score.")
  desiredDirection?: EvaluatorDesirableDirection;

  @doc("Threshold value that this score is compared against to determine the evaluation outcome")
  threshold?: float32;

  @doc("Type of metric generated")
  type: MetricType;
}

@added(Versions.v2025_07_31_preview)
@doc("Enumeration of the different types of metrics that can be used in evaluations")
union MetricType {
  @doc("Continuous metric type, representing a numerical score")
  continuous: "continuous",

  @doc("Categorical metric type, representing discrete categories or labels")
  categorical: "categorical",

  @doc("Ordinal metric type, representing ordered categories")
  ordinal: "ordinal",

  string,
}

@added(Versions.v2025_07_31_preview)
@doc("Metric representing the evaluation result for a specific evaluator.")
model EvaluationMetric {
  @doc("Name of the evaluation metric")
  name: string;

  @doc("Name provided for the evaluator in the input request.")
  evaluatorName: string;

  @doc("Score assigned by the evaluator to the specific data row")
  score?: float32;

  @doc("Optional labels associated with the evaluation result.")
  labels?: string[];

  @doc("Outcome of the evaluation")
  outcome?: EvaluationResultOutcome;

  @doc("Reasoning or explanation provided by the evaluator for the score assigned to this data row")
  reasoning: string;

  @doc("Error message describing why the evaluation failed for this data row, if applicable")
  error?: string;

  @doc("Metadata about the evaluation result")
  metadata: EvaluationResultMetadata;

  @doc("Token consumption for this metric evaluation")
  usage?: Usage;

  @doc("Additional metadata about the evaluation result, such as processing details or custom fields")
  additionalDetails?: Record<unknown>;
}

@doc("Comprehensive evaluation results for a single data row, including the input data, processing state, and results from all configured evaluators.")
@added(Versions.v2025_07_31_preview)
@resource("runs/{evaluationId}/results")
model EvaluationResult {
  @doc("Unique identifier for this evaluation result row")
  @key("id")
  @visibility(Lifecycle.Read)
  id: string;

  @doc("Error message describing why the evaluation failed for this data row, if applicable")
  error?: string;

  @doc("Original input data for this row in JSON string format. This preserves the exact data that was evaluated for reference and debugging.")
  @added(Versions.v2025_07_31_preview)
  inputDataJson?: string;

  @doc("Evaluation results from all the configured evaluators for this data row.")
  metrics?: EvaluationMetric[];
}
