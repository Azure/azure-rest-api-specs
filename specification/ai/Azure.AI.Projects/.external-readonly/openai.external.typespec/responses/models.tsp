import "@typespec/openapi";
import "../specialized-types/models.tsp";
import "../images/models.tsp";
import "../common/models.tsp";
import "../conversations/models.tsp";
using TypeSpec.OpenAPI;
namespace OpenAI;
/** A click action. */
@summary("Click")
model Click {
  /**Specifies the event type. For a click action, this property is
  always set to `click`.*/
  @extension("x-stainless-const", true)
  type: "click" = "click";

  /** Indicates which mouse button was pressed during the click. One of `left`, `right`, `wheel`, `back`, or `forward`. */
  button: "left" | "right" | "wheel" | "back" | "forward";

  /** The x-coordinate where the click occurred. */
  x: integer;

  /** The y-coordinate where the click occurred. */
  y: integer;
}

/** The output of a code interpreter tool call that is a file. */
@summary("Code interpreter file output")
model CodeInterpreterFileOutput {
  /** The type of the code interpreter file output. Always `files`. */
  @extension("x-stainless-const", true)
  type: "files";

  files: {
    mime_type: string;
    file_id: string;
  }[];
}

/** The image output from the code interpreter. */
@summary("Code interpreter output image")
model CodeInterpreterOutputImage {
  /** The type of the output. Always 'image'. */
  @extension("x-stainless-const", true)
  type: "image" = "image";

  /** The URL of the image output from the code interpreter. */
  url: string;
}

/** The logs output from the code interpreter. */
@summary("Code interpreter output logs")
model CodeInterpreterOutputLogs {
  /** The type of the output. Always 'logs'. */
  @extension("x-stainless-const", true)
  type: "logs" = "logs";

  /** The logs output from the code interpreter. */
  logs: string;
}

/** The output of a code interpreter tool call that is text. */
@summary("Code interpreter text output")
model CodeInterpreterTextOutput {
  /** The type of the code interpreter text output. Always `logs`. */
  @extension("x-stainless-const", true)
  type: "logs";

  /** The logs of the code interpreter tool call. */
  logs: string;
}

/** A tool that runs Python code to help generate a response to a prompt. */
@summary("Code interpreter")
model CodeInterpreterTool {
  /** The type of the code interpreter tool. Always `code_interpreter`. */
  @extension("x-stainless-const", true)
  type: "code_interpreter";

  /**The code interpreter container. Can be a container ID or an object that
  specifies uploaded file IDs to make available to your code.*/
  container: string | CodeInterpreterToolAuto;
}

/**Configuration for a code interpreter container. Optionally specify the IDs
of the files to run the code on.*/
@summary("CodeInterpreterContainerAuto")
model CodeInterpreterToolAuto {
  /** Always `auto`. */
  @extension("x-stainless-const", true)
  type: "auto";

  /** An optional list of uploaded files to make available to your code. */
  file_ids?: string[];
}

/** A tool call to run code. */
@summary("Code interpreter tool call")
model CodeInterpreterToolCall {
  /** The type of the code interpreter tool call. Always `code_interpreter_call`. */
  @extension("x-stainless-const", true)
  type: "code_interpreter_call" = "code_interpreter_call";

  /** The unique ID of the code interpreter tool call. */
  id: string;

  /** The status of the code interpreter tool call. Valid values are `in_progress`, `completed`, `incomplete`, `interpreting`, and `failed`. */
  status:
    | "in_progress"
    | "completed"
    | "incomplete"
    | "interpreting"
    | "failed";

  /** The ID of the container used to run the code. */
  container_id: string;

  /** The code to run, or null if not available. */
  code: string | null;

  /**The outputs generated by the code interpreter, such as logs or images.
  Can be null if no outputs are available.*/
  outputs: (CodeInterpreterOutputLogs | CodeInterpreterOutputImage)[] | null;
}

/** A double click action. */
@summary("DoubleClick")
model DoubleClick {
  /**Specifies the event type. For a double click action, this property is
  always set to `double_click`.*/
  @extension("x-stainless-const", true)
  type: "double_click" = "double_click";

  /** The x-coordinate where the double click occurred. */
  x: integer;

  /** The y-coordinate where the double click occurred. */
  y: integer;
}

/** A drag action. */
@summary("Drag")
model Drag {
  /**Specifies the event type. For a drag action, this property is
  always set to `drag`.*/
  @extension("x-stainless-const", true)
  type: "drag" = "drag";

  /**An array of coordinates representing the path of the drag action. Coordinates will appear as an array
  of objects, eg
  ```
  [
    { x: 100, y: 200 },
    { x: 200, y: 300 }
  ]
  ```*/
  path: Coordinate[];
}

/** An x/y coordinate pair, e.g. `{ x: 100, y: 200 }`. */
@summary("Coordinate")
model Coordinate {
  /** The x-coordinate. */
  x: integer;

  /** The y-coordinate. */
  y: integer;
}

/** A collection of keypresses the model would like to perform. */
@summary("KeyPress")
model KeyPress {
  /**Specifies the event type. For a keypress action, this property is
  always set to `keypress`.*/
  @extension("x-stainless-const", true)
  type: "keypress" = "keypress";

  /**The combination of keys the model is requesting to be pressed. This is an
  array of strings, each representing a key.*/
  keys: string[];
}

/** A mouse move action. */
@summary("Move")
model Move {
  /**Specifies the event type. For a move action, this property is
  always set to `move`.*/
  @extension("x-stainless-const", true)
  type: "move" = "move";

  /** The x-coordinate to move to. */
  x: integer;

  /** The y-coordinate to move to. */
  y: integer;
}

/** A screenshot action. */
@summary("Screenshot")
model Screenshot {
  /**Specifies the event type. For a screenshot action, this property is
  always set to `screenshot`.*/
  @extension("x-stainless-const", true)
  type: "screenshot" = "screenshot";
}

/** A scroll action. */
@summary("Scroll")
model Scroll {
  /**Specifies the event type. For a scroll action, this property is
  always set to `scroll`.*/
  @extension("x-stainless-const", true)
  type: "scroll" = "scroll";

  /** The x-coordinate where the scroll occurred. */
  x: integer;

  /** The y-coordinate where the scroll occurred. */
  y: integer;

  /** The horizontal scroll distance. */
  scroll_x: integer;

  /** The vertical scroll distance. */
  scroll_y: integer;
}

/** An action to type in text. */
@summary("Type")
model Type {
  /**Specifies the event type. For a type action, this property is
  always set to `type`.*/
  @extension("x-stainless-const", true)
  type: "type" = "type";

  /** The text to type. */
  text: string;
}

/** A wait action. */
@summary("Wait")
model Wait {
  /**Specifies the event type. For a wait action, this property is
  always set to `wait`.*/
  @extension("x-stainless-const", true)
  type: "wait" = "wait";
}

/** A computer screenshot image used with the computer use tool. */
model ComputerScreenshotImage {
  /**Specifies the event type. For a computer screenshot, this property is
  always set to `computer_screenshot`.*/
  @extension("x-stainless-const", true)
  type: "computer_screenshot" = "computer_screenshot";

  /** The URL of the screenshot image. */
  image_url?: string;

  /** The identifier of an uploaded file that contains the screenshot. */
  file_id?: string;
}

/**A tool call to a computer use tool. See the
[computer use guide](https://platform.openai.com/docs/guides/tools-computer-use) for more information.*/
@summary("Computer tool call")
model ComputerToolCall {
  /** The type of the computer call. Always `computer_call`. */
  type: "computer_call" = "computer_call";

  /** The unique ID of the computer call. */
  id: string;

  /** An identifier used when responding to the tool call with output. */
  call_id: string;

  action: ComputerAction;

  /** The pending safety checks for the computer call. */
  pending_safety_checks: ComputerToolCallSafetyCheck[];

  /**The status of the item. One of `in_progress`, `completed`, or
  `incomplete`. Populated when items are returned via API.*/
  status: "in_progress" | "completed" | "incomplete";
}

@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union ComputerAction {
  click: Click,
  double_click: DoubleClick,
  drag: Drag,
  keypress: KeyPress,
  move: Move,
  screenshot: Screenshot,
  scroll: Scroll,
  type: Type,
  wait: Wait,
}

/** A pending safety check for the computer call. */
model ComputerToolCallSafetyCheck {
  /** The ID of the pending safety check. */
  id: string;

  /** The type of the pending safety check. */
  code: string;

  /** Details about the pending safety check. */
  message: string;
}

/** The output of a computer tool call. */
@summary("Computer tool call output")
model ComputerToolCallOutput {
  /** The type of the computer tool call output. Always `computer_call_output`. */
  @extension("x-stainless-const", true)
  type: "computer_call_output" = "computer_call_output";

  /** The ID of the computer tool call output. */
  id?: string;

  /** The ID of the computer tool call that produced the output. */
  call_id: string;

  /**The safety checks reported by the API that have been acknowledged by the
  developer.*/
  acknowledged_safety_checks?: ComputerToolCallSafetyCheck[];

  output: ComputerScreenshotImage;

  /**The status of the message input. One of `in_progress`, `completed`, or
  `incomplete`. Populated when input items are returned via API.*/
  status?: "in_progress" | "completed" | "incomplete";
}

model ComputerToolCallOutputResource {
  /** The type of the computer tool call output. Always `computer_call_output`. */
  @extension("x-stainless-const", true)
  type: "computer_call_output" = "computer_call_output";

  /** The ID of the computer tool call output. */
  id?: string;

  /** The ID of the computer tool call that produced the output. */
  call_id: string;

  /**The safety checks reported by the API that have been acknowledged by the
  developer.*/
  acknowledged_safety_checks?: ComputerToolCallSafetyCheck[];

  output: ComputerScreenshotImage;

  /**The status of the message input. One of `in_progress`, `completed`, or
  `incomplete`. Populated when input items are returned via API.*/
  status?: "in_progress" | "completed" | "incomplete";
}

/** A text input to the model. */
@summary("Input text")
model InputTextContent {
  /** The type of the input item. Always `input_text`. */
  @extension("x-stainless-const", true)
  type: "input_text" = "input_text";

  /** The text input to the model. */
  text: string;
}

/** An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision). */
@summary("Input image")
model InputImageContent {
  /** The type of the input item. Always `input_image`. */
  @extension("x-stainless-const", true)
  type: "input_image" = "input_image";

  image_url?: string | null;
  file_id?: string | null;

  /** The detail level of the image to be sent to the model. One of `high`, `low`, or `auto`. Defaults to `auto`. */
  detail: "low" | "high" | "auto";
}

/** A file input to the model. */
@summary("Input file")
model InputFileContent {
  /** The type of the input item. Always `input_file`. */
  @extension("x-stainless-const", true)
  type: "input_file" = "input_file";

  file_id?: string | null;

  /** The name of the file to be sent to the model. */
  filename?: string;

  /** The URL of the file to be sent to the model. */
  file_url?: string;

  /** The content of the file to be sent to the model. */
  file_data?: string;
}

/** An audio input to the model. */
@summary("Input audio")
model InputAudio {
  /** The type of the input item. Always `input_audio`. */
  @extension("x-stainless-const", true)
  type: "input_audio";

  input_audio: {
    data: string;
    format: "mp3" | "wav";
  };
}

/** A text output from the model. */
@summary("Output text")
model OutputTextContent {
  /** The type of the output text. Always `output_text`. */
  @extension("x-stainless-const", true)
  type: "output_text" = "output_text";

  /** The text output from the model. */
  text: string;

  /** The annotations of the text output. */
  annotations: Annotation[];

  logprobs?: LogProb[];
}

@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union Annotation {
  file_citation: FileCitationBody,
  url_citation: UrlCitationBody,
  container_file_citation: ContainerFileCitationBody,
  file_path: FilePath,
}

/** A citation to a file. */
@summary("File citation")
model FileCitationBody {
  /** The type of the file citation. Always `file_citation`. */
  @extension("x-stainless-const", true)
  type: "file_citation" = "file_citation";

  /** The ID of the file. */
  file_id: string;

  /** The index of the file in the list of files. */
  index: integer;

  /** The filename of the file cited. */
  filename: string;
}

/** A citation for a web resource used to generate a model response. */
@summary("URL citation")
model UrlCitationBody {
  /** The type of the URL citation. Always `url_citation`. */
  @extension("x-stainless-const", true)
  type: "url_citation" = "url_citation";

  /** The URL of the web resource. */
  url: string;

  /** The index of the first character of the URL citation in the message. */
  start_index: integer;

  /** The index of the last character of the URL citation in the message. */
  end_index: integer;

  /** The title of the web resource. */
  title: string;
}

/** A citation for a container file used to generate a model response. */
@summary("Container file citation")
model ContainerFileCitationBody {
  /** The type of the container file citation. Always `container_file_citation`. */
  @extension("x-stainless-const", true)
  type: "container_file_citation" = "container_file_citation";

  /** The ID of the container file. */
  container_id: string;

  /** The ID of the file. */
  file_id: string;

  /** The index of the first character of the container file citation in the message. */
  start_index: integer;

  /** The index of the last character of the container file citation in the message. */
  end_index: integer;

  /** The filename of the container file cited. */
  filename: string;
}

/** A path to a file. */
@summary("File path")
model FilePath {
  /** The type of the file path. Always `file_path`. */
  @extension("x-stainless-const", true)
  type: "file_path";

  /** The ID of the file. */
  file_id: string;

  /** The index of the file in the list of files. */
  index: integer;
}

/** The log probability of a token. */
@summary("Log probability")
model LogProb {
  token: string;
  logprob: numeric;
  bytes: integer[];
  top_logprobs: TopLogProb[];
}

/** The top log probability of a token. */
@summary("Top log probability")
model TopLogProb {
  token: string;
  logprob: numeric;
  bytes: integer[];
}

/** A refusal from the model. */
@summary("Refusal")
model RefusalContent {
  /** The type of the refusal. Always `refusal`. */
  @extension("x-stainless-const", true)
  type: "refusal" = "refusal";

  /** The refusal explanation from the model. */
  refusal: string;
}

/**A tool call to run a function. See the
[function calling guide](https://platform.openai.com/docs/guides/function-calling) for more information.*/
@summary("Function tool call")
model FunctionToolCall {
  /** The unique ID of the function tool call. */
  id?: string;

  /** The type of the function tool call. Always `function_call`. */
  @extension("x-stainless-const", true)
  type: "function_call";

  /** The unique ID of the function tool call generated by the model. */
  call_id: string;

  /** The name of the function to run. */
  name: string;

  /** A JSON string of the arguments to pass to the function. */
  arguments: string;

  /**The status of the item. One of `in_progress`, `completed`, or
  `incomplete`. Populated when items are returned via API.*/
  status?: "in_progress" | "completed" | "incomplete";
}

model FunctionToolCallResource {
  /** The unique ID of the function tool call. */
  id?: string;

  /** The type of the function tool call. Always `function_call`. */
  @extension("x-stainless-const", true)
  type: "function_call";

  /** The unique ID of the function tool call generated by the model. */
  call_id: string;

  /** The name of the function to run. */
  name: string;

  /** A JSON string of the arguments to pass to the function. */
  arguments: string;

  /**The status of the item. One of `in_progress`, `completed`, or
  `incomplete`. Populated when items are returned via API.*/
  status?: "in_progress" | "completed" | "incomplete";
}

/** The output of a function tool call. */
@summary("Function tool call output")
model FunctionToolCallOutput {
  /**The unique ID of the function tool call output. Populated when this item
  is returned via API.*/
  id?: string;

  /** The type of the function tool call output. Always `function_call_output`. */
  @extension("x-stainless-const", true)
  type: "function_call_output";

  /** The unique ID of the function tool call generated by the model. */
  call_id: string;

  /** A JSON string of the output of the function tool call. */
  output: string;

  /**The status of the item. One of `in_progress`, `completed`, or
  `incomplete`. Populated when items are returned via API.*/
  status?: "in_progress" | "completed" | "incomplete";
}

model FunctionToolCallOutputResource {
  /**The unique ID of the function tool call output. Populated when this item
  is returned via API.*/
  id?: string;

  /** The type of the function tool call output. Always `function_call_output`. */
  @extension("x-stainless-const", true)
  type: "function_call_output";

  /** The unique ID of the function tool call generated by the model. */
  call_id: string;

  /** A JSON string of the output of the function tool call. */
  output: string;

  /**The status of the item. One of `in_progress`, `completed`, or
  `incomplete`. Populated when items are returned via API.*/
  status?: "in_progress" | "completed" | "incomplete";
}

/**The results of a file search tool call. See the
[file search guide](https://platform.openai.com/docs/guides/tools-file-search) for more information.*/
@summary("File search tool call")
model FileSearchToolCall {
  /** The unique ID of the file search tool call. */
  id: string;

  /** The type of the file search tool call. Always `file_search_call`. */
  @extension("x-stainless-const", true)
  type: "file_search_call";

  /**The status of the file search tool call. One of `in_progress`,
  `searching`, `incomplete` or `failed`,*/
  status: "in_progress" | "searching" | "completed" | "incomplete" | "failed";

  /** The queries used to search for files. */
  queries: string[];

  /** The results of the file search tool call. */
  results?: {
    file_id?: string;
    text?: string;
    filename?: string;
    attributes?: VectorStoreFileAttributes;
    score?: float32;
  }[] | null;
}

/**The results of a web search tool call. See the
[web search guide](https://platform.openai.com/docs/guides/tools-web-search) for more information.*/
@summary("Web search tool call")
model WebSearchToolCall {
  /** The unique ID of the web search tool call. */
  id: string;

  /** The type of the web search tool call. Always `web_search_call`. */
  @extension("x-stainless-const", true)
  type: "web_search_call";

  /** The status of the web search tool call. */
  status: "in_progress" | "searching" | "completed" | "failed";

  /**An object describing the specific action taken in this web search call.
  Includes details on how the model used the web (search, open_page, find).*/
  action: WebSearchActionSearch | WebSearchActionOpenPage | WebSearchActionFind;
}

/** Action type "search" - Performs a web search query. */
@summary("Search action")
model WebSearchActionSearch {
  /** The action type. */
  @extension("x-stainless-const", true)
  type: "search";

  /** The search query. */
  query: string;

  /** The sources used in the search. */
  @summary("Web search sources")
  sources?: {
    @extension("x-stainless-const", true)
    type: "url";

    url: string;
  }[];
}

/** Action type "open_page" - Opens a specific URL from search results. */
@summary("Open page action")
model WebSearchActionOpenPage {
  /** The action type. */
  @extension("x-stainless-const", true)
  type: "open_page";

  /** The URL opened by the model. */
  url: url;
}

/** Action type "find": Searches for a pattern within a loaded page. */
@summary("Find action")
model WebSearchActionFind {
  /** The action type. */
  @extension("x-stainless-const", true)
  type: "find";

  /** The URL of the page searched for the pattern. */
  url: url;

  /** The pattern or text to search for within the page. */
  pattern: string;
}

/** An image generation request made by the model. */
@summary("Image generation call")
model ImageGenToolCall {
  /** The type of the image generation call. Always `image_generation_call`. */
  @extension("x-stainless-const", true)
  type: "image_generation_call";

  /** The unique ID of the image generation call. */
  id: string;

  /** The status of the image generation call. */
  status: "in_progress" | "completed" | "generating" | "failed";

  /** The generated image encoded in base64. */
  result: string | null;
}

/**A description of the chain of thought used by a reasoning model while generating
a response. Be sure to include these items in your `input` to the Responses API
for subsequent turns of a conversation if you are manually
[managing context](https://platform.openai.com/docs/guides/conversation-state).*/
@summary("Reasoning")
model ReasoningItem {
  /** The type of the object. Always `reasoning`. */
  @extension("x-stainless-const", true)
  type: "reasoning";

  /** The unique identifier of the reasoning content. */
  id: string;

  /**The encrypted content of the reasoning item - populated when a response is
  generated with `reasoning.encrypted_content` in the `include` parameter.*/
  encrypted_content?: string | null;

  /** Reasoning summary content. */
  summary: {
    @extension("x-stainless-const", true)
    type: "summary_text";

    text: string;
  }[];

  /** Reasoning text content. */
  content?: {
    @extension("x-stainless-const", true)
    type: "reasoning_text";

    text: string;
  }[];

  /**The status of the item. One of `in_progress`, `completed`, or
  `incomplete`. Populated when items are returned via API.*/
  status?: "in_progress" | "completed" | "incomplete";
}

/** A tool call to run a command on the local shell. */
@summary("Local shell call")
model LocalShellToolCall {
  /** The type of the local shell call. Always `local_shell_call`. */
  @extension("x-stainless-const", true)
  type: "local_shell_call";

  /** The unique ID of the local shell call. */
  id: string;

  /** The unique ID of the local shell tool call generated by the model. */
  call_id: string;

  action: LocalShellExecAction;

  /** The status of the local shell call. */
  status: "in_progress" | "completed" | "incomplete";
}

/** Execute a shell command on the server. */
@summary("Local shell exec action")
model LocalShellExecAction {
  /** The type of the local shell action. Always `exec`. */
  @extension("x-stainless-const", true)
  type: "exec";

  /** The command to run. */
  command: string[];

  /** Optional timeout in milliseconds for the command. */
  timeout_ms?: integer | null;

  /** Optional working directory to run the command in. */
  working_directory?: string | null;

  /** Environment variables to set for the command. */
  env: Record<string>;

  /** Optional user to run the command as. */
  user?: string | null;
}

/** The output of a local shell tool call. */
@summary("Local shell call output")
model LocalShellToolCallOutput {
  /** The type of the local shell tool call output. Always `local_shell_call_output`. */
  @extension("x-stainless-const", true)
  type: "local_shell_call_output";

  /** The unique ID of the local shell tool call generated by the model. */
  id: string;

  /** A JSON string of the output of the local shell tool call. */
  output: string;

  /** The status of the item. One of `in_progress`, `completed`, or `incomplete`. */
  status?: "in_progress" | "completed" | "incomplete" | null;
}

/** A list of tools available on an MCP server. */
@summary("MCP list tools")
model MCPListTools {
  /** The type of the item. Always `mcp_list_tools`. */
  @extension("x-stainless-const", true)
  type: "mcp_list_tools";

  /** The unique ID of the list. */
  id: string;

  /** The label of the MCP server. */
  server_label: string;

  /** The tools available on the server. */
  tools: MCPListToolsTool[];

  /** Error message if the server could not list tools. */
  error?: string | null;
}

/** A tool available on an MCP server. */
@summary("MCP list tools tool")
model MCPListToolsTool {
  /** The name of the tool. */
  name: string;

  /** The description of the tool. */
  description?: string | null;

  /** The JSON schema describing the tool's input. */
  input_schema: {};

  /** Additional annotations about the tool. */
  annotations?: {} | null;
}

/** A request for human approval of a tool invocation. */
@summary("MCP approval request")
model MCPApprovalRequest {
  /** The type of the item. Always `mcp_approval_request`. */
  @extension("x-stainless-const", true)
  type: "mcp_approval_request";

  /** The unique ID of the approval request. */
  id: string;

  /** The label of the MCP server making the request. */
  server_label: string;

  /** The name of the tool to run. */
  name: string;

  /** A JSON string of arguments for the tool. */
  arguments: string;
}

/** A response to an MCP approval request. */
@summary("MCP approval response")
model MCPApprovalResponseResource {
  /** The type of the item. Always `mcp_approval_response`. */
  @extension("x-stainless-const", true)
  type: "mcp_approval_response";

  /** The unique ID of the approval response */
  id: string;

  /** The ID of the approval request being answered. */
  approval_request_id: string;

  /** Whether the request was approved. */
  approve: boolean;

  /** Optional reason for the decision. */
  reason?: string | null;
}

/** An invocation of a tool on an MCP server. */
@summary("MCP tool call")
model MCPToolCall {
  /** The type of the item. Always `mcp_call`. */
  @extension("x-stainless-const", true)
  type: "mcp_call";

  /** The unique ID of the tool call. */
  id: string;

  /** The label of the MCP server running the tool. */
  server_label: string;

  /** The name of the tool that was run. */
  name: string;

  /** A JSON string of the arguments passed to the tool. */
  arguments: string;

  /** The output from the tool call. */
  output?: string | null;

  /** The error from the tool call, if any. */
  error?: string | null;
}

/** A call to a custom tool created by the model. */
@summary("Custom tool call")
model CustomToolCall {
  /** The type of the custom tool call. Always `custom_tool_call`. */
  @extension("x-stainless-const", true)
  type: "custom_tool_call";

  /** The unique ID of the custom tool call in the OpenAI platform. */
  id?: string;

  /** An identifier used to map this custom tool call to a tool call output. */
  call_id: string;

  /** The name of the custom tool being called. */
  name: string;

  /** The input for the custom tool call generated by the model. */
  input: string;
}

/** The output of a custom tool call from your code, being sent back to the model. */
@summary("Custom tool call output")
model CustomToolCallOutput {
  /** The type of the custom tool call output. Always `custom_tool_call_output`. */
  @extension("x-stainless-const", true)
  type: "custom_tool_call_output";

  /** The unique ID of the custom tool call output in the OpenAI platform. */
  id?: string;

  /** The call ID, used to map this custom tool call output to a custom tool call. */
  call_id: string;

  /** The output from the custom tool call generated by your code. */
  output: string;
}

/**Constrains the verbosity of the model's response. Lower values will result in
more concise responses, while higher values will result in more verbose responses.
Currently supported values are `low`, `medium`, and `high`.*/
union Verbosity {
  "low",
  "medium",
  "high",
  null,
}

@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union InputItem {
  message: EasyInputMessage,
  Item,
  item_reference: ItemReferenceParam,
}

/**A message input to the model with a role indicating instruction following
hierarchy. Instructions given with the `developer` or `system` role take
precedence over instructions given with the `user` role. Messages with the
`assistant` role are presumed to have been generated by the model in previous
interactions.*/
@summary("Input message")
model EasyInputMessage {
  /**The role of the message input. One of `user`, `assistant`, `system`, or
  `developer`.*/
  role: "user" | "assistant" | "system" | "developer";

  /**Text, image, or audio input to the model, used to generate a response.
  Can also contain previous assistant responses.*/
  content: string | InputContent[];

  /** The type of the message input. Always `message`. */
  @extension("x-stainless-const", true)
  type?: "message";
}

/**A list of one or many input items to the model, containing different content
types.*/
@summary("Input item content list")
model InputMessageContentList {}

@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union InputContent {
  input_text: InputTextContent,
  input_image: InputImageContent,
  input_file: InputFileContent,
  input_audio: InputAudio,
}

/** Content item used to generate a response. */
@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union Item {
  message: InputMessage,
  output_message: OutputMessage,
  file_search_call: FileSearchToolCall,
  computer_call: ComputerToolCall,
  computer_call_output: ComputerCallOutputItemParam,
  web_search_call: WebSearchToolCall,
  function_call: FunctionToolCall,
  function_call_output: FunctionCallOutputItemParam,
  reasoning: ReasoningItem,
  image_generation_call: ImageGenToolCall,
  code_interpreter_call: CodeInterpreterToolCall,
  local_shell_call: LocalShellToolCall,
  local_shell_call_output: LocalShellToolCallOutput,
  mcp_list_tools: MCPListTools,
  mcp_approval_request: MCPApprovalRequest,
  mcp_approval_response: MCPApprovalResponse,
  mcp_call: MCPToolCall,
  custom_tool_call_output: CustomToolCallOutput,
  custom_tool_call: CustomToolCall,
}

/**A message input to the model with a role indicating instruction following
hierarchy. Instructions given with the `developer` or `system` role take
precedence over instructions given with the `user` role.*/
@summary("Input message")
model InputMessage {
  /** The type of the message input. Always set to `message`. */
  @extension("x-stainless-const", true)
  type?: "message";

  /** The role of the message input. One of `user`, `system`, or `developer`. */
  role: "user" | "system" | "developer";

  /**The status of item. One of `in_progress`, `completed`, or
  `incomplete`. Populated when items are returned via API.*/
  status?: "in_progress" | "completed" | "incomplete";

  content: InputContent[];
}

/** An output message from the model. */
@summary("Output message")
model OutputMessage {
  /** The unique ID of the output message. */
  @extension("x-stainless-go-json", "omitzero")
  id: string;

  /** The type of the output message. Always `message`. */
  @extension("x-stainless-const", true)
  type: "output_message";

  /** The role of the output message. Always `assistant`. */
  @extension("x-stainless-const", true)
  role: "assistant";

  /** The content of the output message. */
  content: OutputContent[];

  /**The status of the message input. One of `in_progress`, `completed`, or
  `incomplete`. Populated when input items are returned via API.*/
  status: "in_progress" | "completed" | "incomplete";
}

@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union OutputContent {
  output_text: OutputTextContent,
  refusal: RefusalContent,
}

/** The output of a computer tool call. */
@summary("Computer tool call output")
model ComputerCallOutputItemParam {
  id?: string | null;

  /** The ID of the computer tool call that produced the output. */
  @maxLength(64)
  @minLength(1)
  call_id: string;

  /** The type of the computer tool call output. Always `computer_call_output`. */
  @extension("x-stainless-const", true)
  type: "computer_call_output" = "computer_call_output";

  output: ComputerScreenshotImage;
  acknowledged_safety_checks?: ComputerCallSafetyCheckParam[] | null;
  status?: "in_progress" | "completed" | "incomplete" | null;
}

/** A pending safety check for the computer call. */
model ComputerCallSafetyCheckParam {
  /** The ID of the pending safety check. */
  id: string;

  code?: string | null;
  message?: string | null;
}

/** The output of a function tool call. */
@summary("Function tool call output")
model FunctionCallOutputItemParam {
  id?: string | null;

  /** The unique ID of the function tool call generated by the model. */
  @maxLength(64)
  @minLength(1)
  call_id: string;

  /** The type of the function tool call output. Always `function_call_output`. */
  @extension("x-stainless-const", true)
  type: "function_call_output" = "function_call_output";

  /** A JSON string of the output of the function tool call. */
  @maxLength(10485760)
  output: string;

  status?: "in_progress" | "completed" | "incomplete" | null;
}

/** A response to an MCP approval request. */
@summary("MCP approval response")
model MCPApprovalResponse {
  /** The type of the item. Always `mcp_approval_response`. */
  @extension("x-stainless-const", true)
  type: "mcp_approval_response";

  /** The unique ID of the approval response */
  id?: string | null;

  /** The ID of the approval request being answered. */
  approval_request_id: string;

  /** Whether the request was approved. */
  approve: boolean;

  /** Optional reason for the decision. */
  reason?: string | null;
}

/** An internal identifier for an item to reference. */
@summary("Item reference")
model ItemReferenceParam {
  /** The type of item to reference. Always `item_reference`. */
  @extension("x-stainless-const", true)
  type?: "item_reference" = "item_reference";

  /** The ID of the item to reference. */
  id: string;
}

/** A tool that can be used to generate a response. */
@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union Tool {
  function: FunctionTool,
  file_search: FileSearchTool,
  computer_use_preview: ComputerUsePreviewTool,
  web_search: WebSearchTool,
  mcp: MCPTool,
  code_interpreter: CodeInterpreterTool,
  image_generation: ImageGenTool,
  local_shell: LocalShellTool,
  custom: CustomTool,
  web_search_preview: WebSearchPreviewTool,
}

/** Defines a function in your own code the model can choose to call. Learn more about [function calling](https://platform.openai.com/docs/guides/function-calling). */
@summary("Function")
model FunctionTool {
  /** The type of the function tool. Always `function`. */
  @extension("x-stainless-const", true)
  type: "function" = "function";

  /** The name of the function to call. */
  name: string;

  description?: string | null;
  parameters: Record<unknown> | null;
  strict: boolean | null;
}

/** A tool that searches for relevant content from uploaded files. Learn more about the [file search tool](https://platform.openai.com/docs/guides/tools-file-search). */
@summary("File search")
model FileSearchTool {
  /** The type of the file search tool. Always `file_search`. */
  @extension("x-stainless-const", true)
  type: "file_search" = "file_search";

  /** The IDs of the vector stores to search. */
  vector_store_ids: string[];

  /** The maximum number of results to return. This number should be between 1 and 50 inclusive. */
  max_num_results?: integer;

  /** Ranking options for search. */
  ranking_options?: RankingOptions;

  filters?: Filters | null;
}

model RankingOptions {
  /** The ranker to use for the file search. */
  ranker?: "auto" | "default-2024-11-15";

  /** The score threshold for the file search, a number between 0 and 1. Numbers closer to 1 will attempt to return only the most relevant results, but may return fewer results. */
  score_threshold?: numeric;
}

union Filters {
  ComparisonFilter,
  CompoundFilter,
}

/** A tool that controls a virtual computer. Learn more about the [computer tool](https://platform.openai.com/docs/guides/tools-computer-use). */
@summary("Computer use preview")
model ComputerUsePreviewTool {
  /** The type of the computer use tool. Always `computer_use_preview`. */
  @extension("x-stainless-const", true)
  type: "computer_use_preview" = "computer_use_preview";

  /** The type of computer environment to control. */
  environment: "windows" | "mac" | "linux" | "ubuntu" | "browser";

  /** The width of the computer display. */
  display_width: integer;

  /** The height of the computer display. */
  display_height: integer;
}

/**Search the Internet for sources related to the prompt. Learn more about the
[web search tool](https://platform.openai.com/docs/guides/tools-web-search).*/
@summary("Web search")
model WebSearchTool {
  /** The type of the web search tool. One of `web_search` or `web_search_2025_08_26`. */
  type: "web_search" = "web_search";

  /** Filters for the search. */
  filters?: {
    @summary("Allowed domains for the search.")
    allowed_domains?: string[] | null = #[];
  } | null;

  user_location?: WebSearchApproximateLocation;

  /** High level guidance for the amount of context window space to use for the search. One of `low`, `medium`, or `high`. `medium` is the default. */
  search_context_size?: "low" | "medium" | "high" = "medium";
}

/** The approximate location of the user. */
@summary("Web search approximate location")
model WebSearchApproximateLocation {
  /** The type of location approximation. Always `approximate`. */
  @extension("x-stainless-const", true)
  type?: "approximate" = "approximate";

  /** The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of the user, e.g. `US`. */
  country?: string | null;

  /** Free text input for the region of the user, e.g. `California`. */
  region?: string | null;

  /** Free text input for the city of the user, e.g. `San Francisco`. */
  city?: string | null;

  /** The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the user, e.g. `America/Los_Angeles`. */
  timezone?: string | null;
}

/**Give the model access to additional tools via remote Model Context Protocol
(MCP) servers. [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).*/
@summary("MCP tool")
model MCPTool {
  /** The type of the MCP tool. Always `mcp`. */
  @extension("x-stainless-const", true)
  type: "mcp";

  /** A label for this MCP server, used to identify it in tool calls. */
  server_label: string;

  /**The URL for the MCP server. One of `server_url` or `connector_id` must be
  provided.*/
  server_url?: string;

  /**Identifier for service connectors, like those available in ChatGPT. One of
  `server_url` or `connector_id` must be provided. Learn more about service
  connectors [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).

  Currently supported `connector_id` values are:

  - Dropbox: `connector_dropbox`
  - Gmail: `connector_gmail`
  - Google Calendar: `connector_googlecalendar`
  - Google Drive: `connector_googledrive`
  - Microsoft Teams: `connector_microsoftteams`
  - Outlook Calendar: `connector_outlookcalendar`
  - Outlook Email: `connector_outlookemail`
  - SharePoint: `connector_sharepoint`*/
  connector_id?:
    | "connector_dropbox"
    | "connector_gmail"
    | "connector_googlecalendar"
    | "connector_googledrive"
    | "connector_microsoftteams"
    | "connector_outlookcalendar"
    | "connector_outlookemail"
    | "connector_sharepoint";

  /**An OAuth access token that can be used with a remote MCP server, either
  with a custom MCP server URL or a service connector. Your application
  must handle the OAuth authorization flow and provide the token here.*/
  authorization?: string;

  /** Optional description of the MCP server, used to provide more context. */
  server_description?: string;

  /**Optional HTTP headers to send to the MCP server. Use for authentication
  or other purposes.*/
  headers?: Record<string> | null;

  /** List of allowed tool names or a filter object. */
  allowed_tools?: string[] | MCPToolFilter | null;

  /** Specify which of the MCP server's tools require approval. */
  require_approval?:
    | {
        always?: MCPToolFilter;
        `never`?: MCPToolFilter;
      }
    | "always"
    | "never"
    | null;
}

/** A filter object to specify which tools are allowed. */
@summary("MCP tool filter")
model MCPToolFilter {
  /** List of allowed tool names. */
  @summary("MCP allowed tools")
  tool_names?: string[];

  /**Indicates whether or not a tool modifies data or is read-only. If an
  MCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),
  it will match this filter.*/
  read_only?: boolean;
}

/** A tool that generates images using a model like `gpt-image-1`. */
@summary("Image generation tool")
model ImageGenTool {
  /** The type of the image generation tool. Always `image_generation`. */
  @extension("x-stainless-const", true)
  type: "image_generation" = "image_generation";

  /** The image generation model to use. Default: `gpt-image-1`. */
  `model`?: "gpt-image-1" = "gpt-image-1";

  /**The quality of the generated image. One of `low`, `medium`, `high`,
  or `auto`. Default: `auto`.*/
  quality?: "low" | "medium" | "high" | "auto" = "auto";

  /**The size of the generated image. One of `1024x1024`, `1024x1536`,
  `1536x1024`, or `auto`. Default: `auto`.*/
  size?: "1024x1024" | "1024x1536" | "1536x1024" | "auto" = "auto";

  /**The output format of the generated image. One of `png`, `webp`, or
  `jpeg`. Default: `png`.*/
  output_format?: "png" | "webp" | "jpeg" = "png";

  /** Compression level for the output image. Default: 100. */
  @maxValue(100)
  @minValue(0)
  output_compression?: integer = 100;

  /** Moderation level for the generated image. Default: `auto`. */
  moderation?: "auto" | "low" = "auto";

  /**Background type for the generated image. One of `transparent`,
  `opaque`, or `auto`. Default: `auto`.*/
  background?: "transparent" | "opaque" | "auto" = "auto";

  input_fidelity?: ImageInputFidelity;

  /**Optional mask for inpainting. Contains `image_url`
  (string, optional) and `file_id` (string, optional).*/
  input_image_mask?: {
    image_url?: string;
    file_id?: string;
  };

  /** Number of partial images to generate in streaming mode, from 0 (default value) to 3. */
  @maxValue(3)
  @minValue(0)
  partial_images?: integer;
}

/** A tool that allows the model to execute shell commands in a local environment. */
@summary("Local shell tool")
model LocalShellTool {
  /** The type of the local shell tool. Always `local_shell`. */
  @extension("x-stainless-const", true)
  type: "local_shell" = "local_shell";
}

/**A custom tool that processes input using a specified format. Learn more about
[custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools).*/
@summary("Custom tool")
model CustomTool {
  /** The type of the custom tool. Always `custom`. */
  @extension("x-stainless-const", true)
  type: "custom";

  /** The name of the custom tool, used to identify it in tool calls. */
  name: string;

  /** Optional description of the custom tool, used to provide more context. */
  description?: string;

  /** The input format for the custom tool. Default is unconstrained text. */
  format?: {
    @extension("x-stainless-const", true)
    type: "text";
  } | {
    @extension("x-stainless-const", true)
    type: "grammar";

    definition: string;
    syntax: "lark" | "regex";
  };
}

/** This tool searches the web for relevant results to use in a response. Learn more about the [web search tool](https://platform.openai.com/docs/guides/tools-web-search). */
@summary("Web search preview")
model WebSearchPreviewTool {
  /** The type of the web search tool. One of `web_search_preview` or `web_search_preview_2025_03_11`. */
  @extension("x-stainless-const", true)
  type: "web_search_preview" = "web_search_preview";

  user_location?: ApproximateLocation | null;

  /** High level guidance for the amount of context window space to use for the search. One of `low`, `medium`, or `high`. `medium` is the default. */
  search_context_size?: "low" | "medium" | "high";
}

model ApproximateLocation {
  /** The type of location approximation. Always `approximate`. */
  @extension("x-stainless-const", true)
  type: "approximate" = "approximate";

  country?: string | null;
  region?: string | null;
  city?: string | null;
  timezone?: string | null;
}

/**An object specifying the format that the model must output.

Configuring `{ "type": "json_schema" }` enables Structured Outputs,
which ensures the model will match your supplied JSON schema. Learn more in the
[Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).

The default format is `{ "type": "text" }` with no additional options.

**Not recommended for gpt-4o and newer models:**

Setting to `{ "type": "json_object" }` enables the older JSON mode, which
ensures the message the model generates is valid JSON. Using `json_schema`
is preferred for models that support it.*/
@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union TextResponseFormatConfiguration {
  text: ResponseFormatText,
  json_schema: TextResponseFormatJsonSchema,
  json_object: ResponseFormatJsonObject,
}

/**JSON Schema response format. Used to generate structured JSON responses.
Learn more about [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs).*/
@summary("JSON schema")
model TextResponseFormatJsonSchema {
  /** The type of response format being defined. Always `json_schema`. */
  @extension("x-stainless-const", true)
  type: "json_schema";

  /**A description of what the response format is for, used by the model to
  determine how to respond in the format.*/
  description?: string;

  /**The name of the response format. Must be a-z, A-Z, 0-9, or contain
  underscores and dashes, with a maximum length of 64.*/
  name: string;

  schema: ResponseFormatJsonSchemaSchema;

  /**Whether to enable strict schema adherence when generating the output.
  If set to true, the model will always follow the exact schema defined
  in the `schema` field. Only a subset of JSON Schema is supported when
  `strict` is `true`. To learn more, read the [Structured Outputs
  guide](https://platform.openai.com/docs/guides/structured-outputs).*/
  strict?: boolean | null;
}

model ResponseProperties {
  /**The unique ID of the previous response to the model. Use this to
  create multi-turn conversations. Learn more about
  [conversation state](https://platform.openai.com/docs/guides/conversation-state). Cannot be used in conjunction with `conversation`.*/
  previous_response_id?: string | null;

  /**Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI
  offers a wide range of models with different capabilities, performance
  characteristics, and price points. Refer to the [model guide](https://platform.openai.com/docs/models)
  to browse and compare available models.*/
  `model`?: ModelIdsResponses;

  reasoning?: Reasoning;

  /**Whether to run the model response in the background.
  [Learn more](https://platform.openai.com/docs/guides/background).*/
  background?: boolean | null;

  /** An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning). */
  max_output_tokens?: integer | null;

  /** The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored. */
  max_tool_calls?: integer | null;

  /**Configuration options for a text response from the model. Can be plain
  text or structured JSON data. Learn more:
  - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
  - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)*/
  text?: {
    format?: TextResponseFormatConfiguration;
    verbosity?: Verbosity;
  };

  /**An array of tools the model may call while generating a response. You
  can specify which tool to use by setting the `tool_choice` parameter.

  We support the following categories of tools:
  - **Built-in tools**: Tools that are provided by OpenAI that extend the
  model's capabilities, like [web search](https://platform.openai.com/docs/guides/tools-web-search)
  or [file search](https://platform.openai.com/docs/guides/tools-file-search). Learn more about
  [built-in tools](https://platform.openai.com/docs/guides/tools).
  - **MCP Tools**: Integrations with third-party systems via custom MCP servers
  or predefined connectors such as Google Drive and SharePoint. Learn more about
  [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).
  - **Function calls (custom tools)**: Functions that are defined by you,
  enabling the model to call your own code with strongly typed arguments
  and outputs. Learn more about
  [function calling](https://platform.openai.com/docs/guides/function-calling). You can also use
  custom tools to call your own code.*/
  tools?: Tool[];

  /**How the model should select which tool (or tools) to use when generating
  a response. See the `tools` parameter to see how to specify which tools
  the model can call.*/
  tool_choice?: ToolChoice;

  prompt?: Prompt;

  /**The truncation strategy to use for the model response.
  - `auto`: If the input to this Response exceeds
  the model's context window size, the model will truncate the
  response to fit the context window by dropping items from the beginning of the conversation.
  - `disabled` (default): If the input size will exceed the context window
  size for a model, the request will fail with a 400 error.*/
  truncation?: "auto" | "disabled" | null = "disabled";
}

union ModelIdsResponses {
  ModelIdsShared,

    | "o1-pro"
    | "o1-pro-2025-03-19"
    | "o3-pro"
    | "o3-pro-2025-06-10"
    | "o3-deep-research"
    | "o3-deep-research-2025-06-26"
    | "o4-mini-deep-research"
    | "o4-mini-deep-research-2025-06-26"
    | "computer-use-preview"
    | "computer-use-preview-2025-03-11",
}

/****gpt-5 and o-series models only**

Configuration options for
[reasoning models](https://platform.openai.com/docs/guides/reasoning).*/
@summary("Reasoning")
model Reasoning {
  effort?: ReasoningEffort;

  /**A summary of the reasoning performed by the model. This can be
  useful for debugging and understanding the model's reasoning process.
  One of `auto`, `concise`, or `detailed`.*/
  summary?: "auto" | "concise" | "detailed" | null;

  /****Deprecated:** use `summary` instead.

  A summary of the reasoning performed by the model. This can be
  useful for debugging and understanding the model's reasoning process.
  One of `auto`, `concise`, or `detailed`.*/
  generate_summary?: "auto" | "concise" | "detailed" | null;
}

@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union ToolChoice {
  allowed_tools: ToolChoiceAllowed,
  file_search: ToolChoiceFileSearch,
  web_search_preview: ToolChoiceWebSearchPreview,
  web_search_preview_2025_03_11: ToolChoiceWebSearchPreview20250311,
  computer_use_preview: ToolChoiceComputerUsePreview,
  image_generation: ToolChoiceImageGeneration,
  code_interpreter: ToolChoiceCodeInterpreter,
  function: ToolChoiceFunction,
  mcp: ToolChoiceMCP,
  custom: ToolChoiceCustom,
}

/** Constrains the tools available to the model to a pre-defined set. */
@summary("Allowed tools")
model ToolChoiceAllowed {
  /** Allowed tool configuration type. Always `allowed_tools`. */
  @extension("x-stainless-const", true)
  type: "allowed_tools";

  /**Constrains the tools available to the model to a pre-defined set.

  `auto` allows the model to pick from among the allowed tools and generate a
  message.

  `required` requires the model to call one or more of the allowed tools.*/
  mode: "auto" | "required";

  /**A list of tool definitions that the model should be allowed to call.

  For the Responses API, the list of tool definitions might look like:
  ```json
  [
    { "type": "function", "name": "get_weather" },
    { "type": "mcp", "server_label": "deepwiki" },
    { "type": "image_generation" }
  ]
  ```*/
  tools: {}[];
}

/**Indicates that the model should use a built-in tool to generate a response.
[Learn more about built-in tools](https: //platform.openai.com/docs/guides/tools).*/
model ToolChoiceFileSearch {
  type: "file_search";
}

/**Indicates that the model should use a built-in tool to generate a response.
[Learn more about built-in tools](https: //platform.openai.com/docs/guides/tools).*/
model ToolChoiceWebSearchPreview {
  type: "web_search_preview";
}

/**Indicates that the model should use a built-in tool to generate a response.
[Learn more about built-in tools](https: //platform.openai.com/docs/guides/tools).*/
model ToolChoiceWebSearchPreview20250311 {
  type: "web_search_preview_2025_03_11";
}

/**Indicates that the model should use a built-in tool to generate a response.
[Learn more about built-in tools](https: //platform.openai.com/docs/guides/tools).*/
model ToolChoiceComputerUsePreview {
  type: "computer_use_preview";
}

/**Indicates that the model should use a built-in tool to generate a response.
[Learn more about built-in tools](https: //platform.openai.com/docs/guides/tools).*/
model ToolChoiceImageGeneration {
  type: "image_generation";
}

/**Indicates that the model should use a built-in tool to generate a response.
[Learn more about built-in tools](https: //platform.openai.com/docs/guides/tools).*/
model ToolChoiceCodeInterpreter {
  type: "code_interpreter";
}

/** Use this option to force the model to call a specific function. */
@summary("Function tool")
model ToolChoiceFunction {
  /** For function calling, the type is always `function`. */
  @extension("x-stainless-const", true)
  type: "function";

  /** The name of the function to call. */
  name: string;
}

/** Use this option to force the model to call a specific tool on a remote MCP server. */
@summary("MCP tool")
model ToolChoiceMCP {
  /** For MCP tools, the type is always `mcp`. */
  @extension("x-stainless-const", true)
  type: "mcp";

  /** The label of the MCP server to use. */
  server_label: string;

  /** The name of the tool to call on the server. */
  name?: string | null;
}

/** Use this option to force the model to call a specific custom tool. */
@summary("Custom tool")
model ToolChoiceCustom {
  /** For custom tool calling, the type is always `custom`. */
  @extension("x-stainless-const", true)
  type: "custom";

  /** The name of the custom tool to call. */
  name: string;
}

/**Reference to a prompt template and its variables.
[Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).*/
model Prompt {
  /** The unique identifier of the prompt template to use. */
  id: string;

  /** Optional version of the prompt template. */
  version?: string | null;

  variables?: ResponsePromptVariables;
}

/**Optional map of values to substitute in for variables in your
prompt. The substitution values can either be strings, or other
Response input types like images or files.*/
@summary("Prompt Variables")
@extension("x-oaiTypeLabel", "map")
@extension("x-oaiExpandable", true)
model ResponsePromptVariables {}

model CreateResponse {
  metadata?: Metadata;

  /**An integer between 0 and 20 specifying the number of most likely tokens to
  return at each token position, each with an associated log probability.*/
  @maxValue(20)
  @minValue(0)
  top_logprobs?: integer | null;

  /**What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
  We generally recommend altering this or `top_p` but not both.*/
  @maxValue(2)
  @minValue(0)
  temperature?: numeric | null = 1;

  /**An alternative to sampling with temperature, called nucleus sampling,
  where the model considers the results of the tokens with top_p probability
  mass. So 0.1 means only the tokens comprising the top 10% probability mass
  are considered.

  We generally recommend altering this or `temperature` but not both.*/
  @maxValue(1)
  @minValue(0)
  top_p?: numeric | null = 1;

  /**This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use `prompt_cache_key` instead to maintain caching optimizations.
  A stable identifier for your end-users.
  Used to boost cache hit rates by better bucketing similar requests and  to help OpenAI detect and prevent abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).*/
  user?: string;

  /**A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies.
  The IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).*/
  safety_identifier?: string;

  /** Used by OpenAI to cache responses for similar requests to optimize your cache hit rates. Replaces the `user` field. [Learn more](https://platform.openai.com/docs/guides/prompt-caching). */
  prompt_cache_key?: string;

  service_tier?: ServiceTier;

  /**The unique ID of the previous response to the model. Use this to
  create multi-turn conversations. Learn more about
  [conversation state](https://platform.openai.com/docs/guides/conversation-state). Cannot be used in conjunction with `conversation`.*/
  previous_response_id?: string | null;

  /**Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI
  offers a wide range of models with different capabilities, performance
  characteristics, and price points. Refer to the [model guide](https://platform.openai.com/docs/models)
  to browse and compare available models.*/
  `model`?: ModelIdsResponses;

  reasoning?: Reasoning;

  /**Whether to run the model response in the background.
  [Learn more](https://platform.openai.com/docs/guides/background).*/
  background?: boolean | null;

  /** An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning). */
  max_output_tokens?: integer | null;

  /** The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored. */
  max_tool_calls?: integer | null;

  /**Configuration options for a text response from the model. Can be plain
  text or structured JSON data. Learn more:
  - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
  - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)*/
  text?: {
    format?: TextResponseFormatConfiguration;
    verbosity?: Verbosity;
  };

  /**An array of tools the model may call while generating a response. You
  can specify which tool to use by setting the `tool_choice` parameter.

  We support the following categories of tools:
  - **Built-in tools**: Tools that are provided by OpenAI that extend the
  model's capabilities, like [web search](https://platform.openai.com/docs/guides/tools-web-search)
  or [file search](https://platform.openai.com/docs/guides/tools-file-search). Learn more about
  [built-in tools](https://platform.openai.com/docs/guides/tools).
  - **MCP Tools**: Integrations with third-party systems via custom MCP servers
  or predefined connectors such as Google Drive and SharePoint. Learn more about
  [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).
  - **Function calls (custom tools)**: Functions that are defined by you,
  enabling the model to call your own code with strongly typed arguments
  and outputs. Learn more about
  [function calling](https://platform.openai.com/docs/guides/function-calling). You can also use
  custom tools to call your own code.*/
  tools?: Tool[];

  /**How the model should select which tool (or tools) to use when generating
  a response. See the `tools` parameter to see how to specify which tools
  the model can call.*/
  tool_choice?: ToolChoice;

  prompt?: Prompt;

  /**The truncation strategy to use for the model response.
  - `auto`: If the input to this Response exceeds
  the model's context window size, the model will truncate the
  response to fit the context window by dropping items from the beginning of the conversation.
  - `disabled` (default): If the input size will exceed the context window
  size for a model, the request will fail with a 400 error.*/
  truncation?: "auto" | "disabled" | null = "disabled";

  /**Text, image, or file inputs to the model, used to generate a response.

  Learn more:
  - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
  - [Image inputs](https://platform.openai.com/docs/guides/images)
  - [File inputs](https://platform.openai.com/docs/guides/pdf-files)
  - [Conversation state](https://platform.openai.com/docs/guides/conversation-state)
  - [Function calling](https://platform.openai.com/docs/guides/function-calling)*/
  input?: string | InputItem[];

  /**Specify additional output data to include in the model response. Currently
  supported values are:
  - `web_search_call.action.sources`: Include the sources of the web search tool call.
  - `code_interpreter_call.outputs`: Includes the outputs of python code execution
  in code interpreter tool call items.
  - `computer_call_output.output.image_url`: Include image urls from the computer call output.
  - `file_search_call.results`: Include the search results of
  the file search tool call.
  - `message.input_image.image_url`: Include image urls from the input message.
  - `message.output_text.logprobs`: Include logprobs with assistant messages.
  - `reasoning.encrypted_content`: Includes an encrypted version of reasoning
  tokens in reasoning item outputs. This enables reasoning items to be used in
  multi-turn conversations when using the Responses API statelessly (like
  when the `store` parameter is set to `false`, or when an organization is
  enrolled in the zero data retention program).*/
  include?: Includable[] | null;

  /** Whether to allow the model to run tool calls in parallel. */
  parallel_tool_calls?: boolean | null = true;

  /**Whether to store the generated model response for later retrieval via
  API.*/
  store?: boolean | null = true;

  /**A system (or developer) message inserted into the model's context.

  When using along with `previous_response_id`, the instructions from a previous
  response will not be carried over to the next response. This makes it simple
  to swap out system (or developer) messages in new responses.*/
  instructions?: string | null;

  /**If set to true, the model response data will be streamed to the client
  as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
  See the [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)
  for more information.*/
  stream?: boolean | null;

  stream_options?: ResponseStreamOptions;

  /**The conversation that this response belongs to. Items from this conversation are prepended to `input_items` for this response request.
  Input items and output items from this response are automatically added to this conversation after this response completes.*/
  conversation?: string | ConversationParam | null;
}

/**Specify additional output data to include in the model response. Currently
supported values are:
- `web_search_call.action.sources`: Include the sources of the web search tool call.
- `code_interpreter_call.outputs`: Includes the outputs of python code execution
in code interpreter tool call items.
- `computer_call_output.output.image_url`: Include image urls from the computer call output.
- `file_search_call.results`: Include the search results of
the file search tool call.
- `message.input_image.image_url`: Include image urls from the input message.
- `message.output_text.logprobs`: Include logprobs with assistant messages.
- `reasoning.encrypted_content`: Includes an encrypted version of reasoning
tokens in reasoning item outputs. This enables reasoning items to be used in
multi-turn conversations when using the Responses API statelessly (like
when the `store` parameter is set to `false`, or when an organization is
enrolled in the zero data retention program).*/
enum Includable {
  `code_interpreter_call.outputs`,
  `computer_call_output.output.image_url`,
  `file_search_call.results`,
  `message.input_image.image_url`,
  `message.output_text.logprobs`,
  `reasoning.encrypted_content`,
}

/** Options for streaming responses. Only set this when you set `stream: true`. */
model ResponseStreamOptions {
  /**When true, stream obfuscation will be enabled. Stream obfuscation adds
  random characters to an `obfuscation` field on streaming delta events to
  normalize payload sizes as a mitigation to certain side-channel attacks.
  These obfuscation fields are included by default, but add a small amount
  of overhead to the data stream. You can set `include_obfuscation` to
  false to optimize for bandwidth if you trust the network links between
  your application and the OpenAI API.*/
  include_obfuscation?: boolean;
}

model InputMessageResource {
  /** The type of the message input. Always set to `message`. */
  @extension("x-stainless-const", true)
  type?: "message";

  /** The role of the message input. One of `user`, `system`, or `developer`. */
  role: "user" | "system" | "developer";

  /**The status of item. One of `in_progress`, `completed`, or
  `incomplete`. Populated when items are returned via API.*/
  status?: "in_progress" | "completed" | "incomplete";

  content: InputContent[];

  /** The unique ID of the message input. */
  id: string;
}

/** An audio output from the model. */
@summary("Output audio")
model OutputAudio {
  /** The type of the output audio. Always `output_audio`. */
  @extension("x-stainless-const", true)
  type: "output_audio";

  /** Base64-encoded audio data from the model. */
  data: string;

  /** The transcript of the audio data from the model. */
  transcript: string;
}

/**Controls which (if any) tool is called by the model.

`none` means the model will not call any tool and instead generates a message.

`auto` means the model can pick between generating a message or calling one or
more tools.

`required` means the model must call one or more tools.*/
@summary("Tool choice mode")
enum ToolChoiceOptions {
  none,
  auto,
  required,
}

@summary("The response object")
model Response {
  metadata?: Metadata;

  /**An integer between 0 and 20 specifying the number of most likely tokens to
  return at each token position, each with an associated log probability.*/
  @maxValue(20)
  @minValue(0)
  top_logprobs?: integer | null;

  /**What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
  We generally recommend altering this or `top_p` but not both.*/
  @maxValue(2)
  @minValue(0)
  temperature?: numeric | null = 1;

  /**An alternative to sampling with temperature, called nucleus sampling,
  where the model considers the results of the tokens with top_p probability
  mass. So 0.1 means only the tokens comprising the top 10% probability mass
  are considered.

  We generally recommend altering this or `temperature` but not both.*/
  @maxValue(1)
  @minValue(0)
  top_p?: numeric | null = 1;

  /**This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use `prompt_cache_key` instead to maintain caching optimizations.
  A stable identifier for your end-users.
  Used to boost cache hit rates by better bucketing similar requests and  to help OpenAI detect and prevent abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).*/
  user?: string;

  /**A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies.
  The IDs should be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending us any identifying information. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).*/
  safety_identifier?: string;

  /** Used by OpenAI to cache responses for similar requests to optimize your cache hit rates. Replaces the `user` field. [Learn more](https://platform.openai.com/docs/guides/prompt-caching). */
  prompt_cache_key?: string;

  service_tier?: ServiceTier;

  /**The unique ID of the previous response to the model. Use this to
  create multi-turn conversations. Learn more about
  [conversation state](https://platform.openai.com/docs/guides/conversation-state). Cannot be used in conjunction with `conversation`.*/
  previous_response_id?: string | null;

  /**Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI
  offers a wide range of models with different capabilities, performance
  characteristics, and price points. Refer to the [model guide](https://platform.openai.com/docs/models)
  to browse and compare available models.*/
  `model`?: ModelIdsResponses;

  reasoning?: Reasoning;

  /**Whether to run the model response in the background.
  [Learn more](https://platform.openai.com/docs/guides/background).*/
  background?: boolean | null;

  /** An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning). */
  max_output_tokens?: integer | null;

  /** The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored. */
  max_tool_calls?: integer | null;

  /**Configuration options for a text response from the model. Can be plain
  text or structured JSON data. Learn more:
  - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
  - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)*/
  text?: {
    format?: TextResponseFormatConfiguration;
    verbosity?: Verbosity;
  };

  /**An array of tools the model may call while generating a response. You
  can specify which tool to use by setting the `tool_choice` parameter.

  We support the following categories of tools:
  - **Built-in tools**: Tools that are provided by OpenAI that extend the
  model's capabilities, like [web search](https://platform.openai.com/docs/guides/tools-web-search)
  or [file search](https://platform.openai.com/docs/guides/tools-file-search). Learn more about
  [built-in tools](https://platform.openai.com/docs/guides/tools).
  - **MCP Tools**: Integrations with third-party systems via custom MCP servers
  or predefined connectors such as Google Drive and SharePoint. Learn more about
  [MCP Tools](https://platform.openai.com/docs/guides/tools-connectors-mcp).
  - **Function calls (custom tools)**: Functions that are defined by you,
  enabling the model to call your own code with strongly typed arguments
  and outputs. Learn more about
  [function calling](https://platform.openai.com/docs/guides/function-calling). You can also use
  custom tools to call your own code.*/
  tools?: Tool[];

  /**How the model should select which tool (or tools) to use when generating
  a response. See the `tools` parameter to see how to specify which tools
  the model can call.*/
  tool_choice?: ToolChoice;

  prompt?: Prompt;

  /**The truncation strategy to use for the model response.
  - `auto`: If the input to this Response exceeds
  the model's context window size, the model will truncate the
  response to fit the context window by dropping items from the beginning of the conversation.
  - `disabled` (default): If the input size will exceed the context window
  size for a model, the request will fail with a 400 error.*/
  truncation?: "auto" | "disabled" | null = "disabled";

  /** Unique identifier for this Response. */
  id: string;

  /** The object type of this resource - always set to `response`. */
  @extension("x-stainless-const", true)
  object: "response";

  /**The status of the response generation. One of `completed`, `failed`,
  `in_progress`, `cancelled`, `queued`, or `incomplete`.*/
  status?:
    | "completed"
    | "failed"
    | "in_progress"
    | "cancelled"
    | "queued"
    | "incomplete";

  /** Unix timestamp (in seconds) of when this Response was created. */
  created_at: numeric;

  error: ResponseError;

  /** Details about why the response is incomplete. */
  incomplete_details: {
    reason?: "max_output_tokens" | "content_filter";
  } | null;

  /**An array of content items generated by the model.

  - The length and order of items in the `output` array is dependent
  on the model's response.
  - Rather than accessing the first item in the `output` array and
  assuming it's an `assistant` message with the content generated by
  the model, you might consider using the `output_text` property where
  supported in SDKs.*/
  output: OutputItem[];

  /**A system (or developer) message inserted into the model's context.

  When using along with `previous_response_id`, the instructions from a previous
  response will not be carried over to the next response. This makes it simple
  to swap out system (or developer) messages in new responses.*/
  instructions: string | InputItem[] | null;

  /**SDK-only convenience property that contains the aggregated text output
  from all `output_text` items in the `output` array, if any are present.
  Supported in the Python and JavaScript SDKs.*/
  @extension("x-stainless-skip", true)
  @extension("x-oaiSupportedSDKs", #["python", "javascript"])
  output_text?: string | null;

  usage?: ResponseUsage;

  /** Whether to allow the model to run tool calls in parallel. */
  parallel_tool_calls: boolean = true;

  conversation?: Conversation;
}

/** An error object returned when the model fails to generate a Response. */
model ResponseError {
  code: ResponseErrorCode;

  /** A human-readable description of the error. */
  message: string;
}

/** The error code for the response. */
enum ResponseErrorCode {
  server_error,
  rate_limit_exceeded,
  invalid_prompt,
  vector_store_timeout,
  invalid_image,
  invalid_image_format,
  invalid_base64_image,
  invalid_image_url,
  image_too_large,
  image_too_small,
  image_parse_error,
  image_content_policy_violation,
  invalid_image_mode,
  image_file_too_large,
  unsupported_image_media_type,
  empty_image_file,
  failed_to_download_image,
  image_file_not_found,
}

@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union OutputItem {
  output_message: OutputMessage,
  file_search_call: FileSearchToolCall,
  function_call: FunctionToolCall,
  web_search_call: WebSearchToolCall,
  computer_call: ComputerToolCall,
  reasoning: ReasoningItem,
  image_generation_call: ImageGenToolCall,
  code_interpreter_call: CodeInterpreterToolCall,
  local_shell_call: LocalShellToolCall,
  mcp_call: MCPToolCall,
  mcp_list_tools: MCPListTools,
  mcp_approval_request: MCPApprovalRequest,
  custom_tool_call: CustomToolCall,
}

/**Represents token usage details including input tokens, output tokens,
a breakdown of output tokens, and the total tokens used.*/
model ResponseUsage {
  /** The number of input tokens. */
  input_tokens: integer;

  /** A detailed breakdown of the input tokens. */
  input_tokens_details: {
    cached_tokens: integer;
  };

  /** The number of output tokens. */
  output_tokens: integer;

  /** A detailed breakdown of the output tokens. */
  output_tokens_details: {
    reasoning_tokens: integer;
  };

  /** The total number of tokens used. */
  total_tokens: integer;
}

/** Emitted when there is a partial audio response. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.audio.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.audio.delta\",\n  \"response_id\": \"resp_123\",\n  \"delta\": \"base64encoded...\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseAudioDeltaEvent {
  /** The type of the event. Always `response.audio.delta`. */
  @extension("x-stainless-const", true)
  type: "response.audio.delta";

  /** A sequence number for this chunk of the stream response. */
  sequence_number: integer;

  /** A chunk of Base64 encoded response audio bytes. */
  delta: string;
}

/** Emitted when the audio response is complete. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.audio.done",
    group: "responses",
    example: "{\n  \"type\": \"response.audio.done\",\n  \"response_id\": \"resp-123\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseAudioDoneEvent {
  /** The type of the event. Always `response.audio.done`. */
  @extension("x-stainless-const", true)
  type: "response.audio.done";

  /** The sequence number of the delta. */
  sequence_number: integer;
}

/** Emitted when there is a partial transcript of audio. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.audio.transcript.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.audio.transcript.delta\",\n  \"response_id\": \"resp_123\",\n  \"delta\": \" ... partial transcript ... \",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseAudioTranscriptDeltaEvent {
  /** The type of the event. Always `response.audio.transcript.delta`. */
  @extension("x-stainless-const", true)
  type: "response.audio.transcript.delta";

  /** The partial transcript of the audio response. */
  delta: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when the full audio transcript is completed. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.audio.transcript.done",
    group: "responses",
    example: "{\n  \"type\": \"response.audio.transcript.done\",\n  \"response_id\": \"resp_123\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseAudioTranscriptDoneEvent {
  /** The type of the event. Always `response.audio.transcript.done`. */
  @extension("x-stainless-const", true)
  type: "response.audio.transcript.done";

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when a partial code snippet is streamed by the code interpreter. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.code_interpreter_call_code.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.code_interpreter_call_code.delta\",\n  \"output_index\": 0,\n  \"item_id\": \"ci_12345\",\n  \"delta\": \"print('Hello, world')\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseCodeInterpreterCallCodeDeltaEvent {
  /** The type of the event. Always `response.code_interpreter_call_code.delta`. */
  @extension("x-stainless-const", true)
  type: "response.code_interpreter_call_code.delta";

  /** The index of the output item in the response for which the code is being streamed. */
  output_index: integer;

  /** The unique identifier of the code interpreter tool call item. */
  item_id: string;

  /** The partial code snippet being streamed by the code interpreter. */
  delta: string;

  /** The sequence number of this event, used to order streaming events. */
  sequence_number: integer;
}

/** Emitted when the code snippet is finalized by the code interpreter. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.code_interpreter_call_code.done",
    group: "responses",
    example: "{\n  \"type\": \"response.code_interpreter_call_code.done\",\n  \"output_index\": 3,\n  \"item_id\": \"ci_12345\",\n  \"code\": \"print('done')\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseCodeInterpreterCallCodeDoneEvent {
  /** The type of the event. Always `response.code_interpreter_call_code.done`. */
  @extension("x-stainless-const", true)
  type: "response.code_interpreter_call_code.done";

  /** The index of the output item in the response for which the code is finalized. */
  output_index: integer;

  /** The unique identifier of the code interpreter tool call item. */
  item_id: string;

  /** The final code snippet output by the code interpreter. */
  code: string;

  /** The sequence number of this event, used to order streaming events. */
  sequence_number: integer;
}

/** Emitted when the code interpreter call is completed. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.code_interpreter_call.completed",
    group: "responses",
    example: "{\n  \"type\": \"response.code_interpreter_call.completed\",\n  \"output_index\": 5,\n  \"item_id\": \"ci_12345\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseCodeInterpreterCallCompletedEvent {
  /** The type of the event. Always `response.code_interpreter_call.completed`. */
  @extension("x-stainless-const", true)
  type: "response.code_interpreter_call.completed";

  /** The index of the output item in the response for which the code interpreter call is completed. */
  output_index: integer;

  /** The unique identifier of the code interpreter tool call item. */
  item_id: string;

  /** The sequence number of this event, used to order streaming events. */
  sequence_number: integer;
}

/** Emitted when a code interpreter call is in progress. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.code_interpreter_call.in_progress",
    group: "responses",
    example: "{\n  \"type\": \"response.code_interpreter_call.in_progress\",\n  \"output_index\": 0,\n  \"item_id\": \"ci_12345\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseCodeInterpreterCallInProgressEvent {
  /** The type of the event. Always `response.code_interpreter_call.in_progress`. */
  @extension("x-stainless-const", true)
  type: "response.code_interpreter_call.in_progress";

  /** The index of the output item in the response for which the code interpreter call is in progress. */
  output_index: integer;

  /** The unique identifier of the code interpreter tool call item. */
  item_id: string;

  /** The sequence number of this event, used to order streaming events. */
  sequence_number: integer;
}

/** Emitted when the code interpreter is actively interpreting the code snippet. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.code_interpreter_call.interpreting",
    group: "responses",
    example: "{\n  \"type\": \"response.code_interpreter_call.interpreting\",\n  \"output_index\": 4,\n  \"item_id\": \"ci_12345\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseCodeInterpreterCallInterpretingEvent {
  /** The type of the event. Always `response.code_interpreter_call.interpreting`. */
  @extension("x-stainless-const", true)
  type: "response.code_interpreter_call.interpreting";

  /** The index of the output item in the response for which the code interpreter is interpreting code. */
  output_index: integer;

  /** The unique identifier of the code interpreter tool call item. */
  item_id: string;

  /** The sequence number of this event, used to order streaming events. */
  sequence_number: integer;
}

/** Emitted when the model response is complete. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.completed",
    group: "responses",
    example: "{\n  \"type\": \"response.completed\",\n  \"response\": {\n    \"id\": \"resp_123\",\n    \"object\": \"response\",\n    \"created_at\": 1740855869,\n    \"status\": \"completed\",\n    \"error\": null,\n    \"incomplete_details\": null,\n    \"input\": [],\n    \"instructions\": null,\n    \"max_output_tokens\": null,\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"output\": [\n      {\n        \"id\": \"msg_123\",\n        \"type\": \"message\",\n        \"role\": \"assistant\",\n        \"content\": [\n          {\n            \"type\": \"output_text\",\n            \"text\": \"In a shimmering forest under a sky full of stars, a lonely unicorn named Lila discovered a hidden pond that glowed with moonlight. Every night, she would leave sparkling, magical flowers by the water's edge, hoping to share her beauty with others. One enchanting evening, she woke to find a group of friendly animals gathered around, eager to be friends and share in her magic.\",\n            \"annotations\": []\n          }\n        ]\n      }\n    ],\n    \"previous_response_id\": null,\n    \"reasoning_effort\": null,\n    \"store\": false,\n    \"temperature\": 1,\n    \"text\": {\n      \"format\": {\n        \"type\": \"text\"\n      }\n    },\n    \"tool_choice\": \"auto\",\n    \"tools\": [],\n    \"top_p\": 1,\n    \"truncation\": \"disabled\",\n    \"usage\": {\n      \"input_tokens\": 0,\n      \"output_tokens\": 0,\n      \"output_tokens_details\": {\n        \"reasoning_tokens\": 0\n      },\n      \"total_tokens\": 0\n    },\n    \"user\": null,\n    \"metadata\": {}\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseCompletedEvent {
  /** The type of the event. Always `response.completed`. */
  @extension("x-stainless-const", true)
  type: "response.completed";

  /** Properties of the completed response. */
  response: Response;

  /** The sequence number for this event. */
  sequence_number: integer;
}

/** Emitted when a new content part is added. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.content_part.added",
    group: "responses",
    example: "{\n  \"type\": \"response.content_part.added\",\n  \"item_id\": \"msg_123\",\n  \"output_index\": 0,\n  \"content_index\": 0,\n  \"part\": {\n    \"type\": \"output_text\",\n    \"text\": \"\",\n    \"annotations\": []\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseContentPartAddedEvent {
  /** The type of the event. Always `response.content_part.added`. */
  @extension("x-stainless-const", true)
  type: "response.content_part.added";

  /** The ID of the output item that the content part was added to. */
  item_id: string;

  /** The index of the output item that the content part was added to. */
  output_index: integer;

  /** The index of the content part that was added. */
  content_index: integer;

  /** The content part that was added. */
  part: OutputContent;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when a content part is done. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.content_part.done",
    group: "responses",
    example: "{\n  \"type\": \"response.content_part.done\",\n  \"item_id\": \"msg_123\",\n  \"output_index\": 0,\n  \"content_index\": 0,\n  \"sequence_number\": 1,\n  \"part\": {\n    \"type\": \"output_text\",\n    \"text\": \"In a shimmering forest under a sky full of stars, a lonely unicorn named Lila discovered a hidden pond that glowed with moonlight. Every night, she would leave sparkling, magical flowers by the water's edge, hoping to share her beauty with others. One enchanting evening, she woke to find a group of friendly animals gathered around, eager to be friends and share in her magic.\",\n    \"annotations\": []\n  }\n}\n",
  }
)
model ResponseContentPartDoneEvent {
  /** The type of the event. Always `response.content_part.done`. */
  @extension("x-stainless-const", true)
  type: "response.content_part.done";

  /** The ID of the output item that the content part was added to. */
  item_id: string;

  /** The index of the output item that the content part was added to. */
  output_index: integer;

  /** The index of the content part that is done. */
  content_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The content part that is done. */
  part: OutputContent;
}

/** An event that is emitted when a response is created. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.created",
    group: "responses",
    example: "{\n  \"type\": \"response.created\",\n  \"response\": {\n    \"id\": \"resp_67ccfcdd16748190a91872c75d38539e09e4d4aac714747c\",\n    \"object\": \"response\",\n    \"created_at\": 1741487325,\n    \"status\": \"in_progress\",\n    \"error\": null,\n    \"incomplete_details\": null,\n    \"instructions\": null,\n    \"max_output_tokens\": null,\n    \"model\": \"gpt-4o-2024-08-06\",\n    \"output\": [],\n    \"parallel_tool_calls\": true,\n    \"previous_response_id\": null,\n    \"reasoning\": {\n      \"effort\": null,\n      \"summary\": null\n    },\n    \"store\": true,\n    \"temperature\": 1,\n    \"text\": {\n      \"format\": {\n        \"type\": \"text\"\n      }\n    },\n    \"tool_choice\": \"auto\",\n    \"tools\": [],\n    \"top_p\": 1,\n    \"truncation\": \"disabled\",\n    \"usage\": null,\n    \"user\": null,\n    \"metadata\": {}\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseCreatedEvent {
  /** The type of the event. Always `response.created`. */
  @extension("x-stainless-const", true)
  type: "response.created";

  /** The response that was created. */
  response: Response;

  /** The sequence number for this event. */
  sequence_number: integer;
}

/** Event representing a delta (partial update) to the input of a custom tool call. */
@summary("ResponseCustomToolCallInputDelta")
@extension(
  "x-oaiMeta",
  #{
    name: "response.custom_tool_call_input.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.custom_tool_call_input.delta\",\n  \"output_index\": 0,\n  \"item_id\": \"ctc_1234567890abcdef\",\n  \"delta\": \"partial input text\"\n}\n",
  }
)
model ResponseCustomToolCallInputDeltaEvent {
  /** The event type identifier. */
  @extension("x-stainless-const", true)
  type: "response.custom_tool_call_input.delta";

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The index of the output this delta applies to. */
  output_index: integer;

  /** Unique identifier for the API item associated with this event. */
  item_id: string;

  /** The incremental input data (delta) for the custom tool call. */
  delta: string;
}

/** Event indicating that input for a custom tool call is complete. */
@summary("ResponseCustomToolCallInputDone")
@extension(
  "x-oaiMeta",
  #{
    name: "response.custom_tool_call_input.done",
    group: "responses",
    example: "{\n  \"type\": \"response.custom_tool_call_input.done\",\n  \"output_index\": 0,\n  \"item_id\": \"ctc_1234567890abcdef\",\n  \"input\": \"final complete input text\"\n}\n",
  }
)
model ResponseCustomToolCallInputDoneEvent {
  /** The event type identifier. */
  @extension("x-stainless-const", true)
  type: "response.custom_tool_call_input.done";

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The index of the output this event applies to. */
  output_index: integer;

  /** Unique identifier for the API item associated with this event. */
  item_id: string;

  /** The complete input data for the custom tool call. */
  input: string;
}

/** Emitted when an error occurs. */
@extension(
  "x-oaiMeta",
  #{
    name: "error",
    group: "responses",
    example: "{\n  \"type\": \"error\",\n  \"code\": \"ERR_SOMETHING\",\n  \"message\": \"Something went wrong\",\n  \"param\": null,\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseErrorEvent {
  /** The type of the event. Always `error`. */
  @extension("x-stainless-const", true)
  type: "error";

  /** The error code. */
  code: string | null;

  /** The error message. */
  message: string;

  /** The error parameter. */
  param: string | null;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** An event that is emitted when a response fails. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.failed",
    group: "responses",
    example: "{\n  \"type\": \"response.failed\",\n  \"response\": {\n    \"id\": \"resp_123\",\n    \"object\": \"response\",\n    \"created_at\": 1740855869,\n    \"status\": \"failed\",\n    \"error\": {\n      \"code\": \"server_error\",\n      \"message\": \"The model failed to generate a response.\"\n    },\n    \"incomplete_details\": null,\n    \"instructions\": null,\n    \"max_output_tokens\": null,\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"output\": [],\n    \"previous_response_id\": null,\n    \"reasoning_effort\": null,\n    \"store\": false,\n    \"temperature\": 1,\n    \"text\": {\n      \"format\": {\n        \"type\": \"text\"\n      }\n    },\n    \"tool_choice\": \"auto\",\n    \"tools\": [],\n    \"top_p\": 1,\n    \"truncation\": \"disabled\",\n    \"usage\": null,\n    \"user\": null,\n    \"metadata\": {}\n  }\n}\n",
  }
)
model ResponseFailedEvent {
  /** The type of the event. Always `response.failed`. */
  @extension("x-stainless-const", true)
  type: "response.failed";

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The response that failed. */
  response: Response;
}

/** Emitted when a file search call is completed (results found). */
@extension(
  "x-oaiMeta",
  #{
    name: "response.file_search_call.completed",
    group: "responses",
    example: "{\n  \"type\": \"response.file_search_call.completed\",\n  \"output_index\": 0,\n  \"item_id\": \"fs_123\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseFileSearchCallCompletedEvent {
  /** The type of the event. Always `response.file_search_call.completed`. */
  @extension("x-stainless-const", true)
  type: "response.file_search_call.completed";

  /** The index of the output item that the file search call is initiated. */
  output_index: integer;

  /** The ID of the output item that the file search call is initiated. */
  item_id: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when a file search call is initiated. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.file_search_call.in_progress",
    group: "responses",
    example: "{\n  \"type\": \"response.file_search_call.in_progress\",\n  \"output_index\": 0,\n  \"item_id\": \"fs_123\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseFileSearchCallInProgressEvent {
  /** The type of the event. Always `response.file_search_call.in_progress`. */
  @extension("x-stainless-const", true)
  type: "response.file_search_call.in_progress";

  /** The index of the output item that the file search call is initiated. */
  output_index: integer;

  /** The ID of the output item that the file search call is initiated. */
  item_id: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when a file search is currently searching. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.file_search_call.searching",
    group: "responses",
    example: "{\n  \"type\": \"response.file_search_call.searching\",\n  \"output_index\": 0,\n  \"item_id\": \"fs_123\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseFileSearchCallSearchingEvent {
  /** The type of the event. Always `response.file_search_call.searching`. */
  @extension("x-stainless-const", true)
  type: "response.file_search_call.searching";

  /** The index of the output item that the file search call is searching. */
  output_index: integer;

  /** The ID of the output item that the file search call is initiated. */
  item_id: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/**A custom grammar for the model to follow when generating text.
Learn more in the [custom grammars guide](https://platform.openai.com/docs/guides/custom-grammars).*/
@summary("Text grammar")
model ResponseFormatTextGrammar {
  /** The type of response format being defined. Always `grammar`. */
  @extension("x-stainless-const", true)
  type: "grammar";

  /** The custom grammar for the model to follow. */
  grammar: string;
}

/**Configure the model to generate valid Python code. See the
[custom grammars guide](https://platform.openai.com/docs/guides/custom-grammars) for more details.*/
@summary("Python grammar")
model ResponseFormatTextPython {
  /** The type of response format being defined. Always `python`. */
  @extension("x-stainless-const", true)
  type: "python";
}

/** Emitted when there is a partial function-call arguments delta. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.function_call_arguments.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.function_call_arguments.delta\",\n  \"item_id\": \"item-abc\",\n  \"output_index\": 0,\n  \"delta\": \"{ \\\"arg\\\":\"\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseFunctionCallArgumentsDeltaEvent {
  /** The type of the event. Always `response.function_call_arguments.delta`. */
  @extension("x-stainless-const", true)
  type: "response.function_call_arguments.delta";

  /** The ID of the output item that the function-call arguments delta is added to. */
  item_id: string;

  /** The index of the output item that the function-call arguments delta is added to. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The function-call arguments delta that is added. */
  delta: string;
}

/** Emitted when function-call arguments are finalized. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.function_call_arguments.done",
    group: "responses",
    example: "{\n  \"type\": \"response.function_call_arguments.done\",\n  \"item_id\": \"item-abc\",\n  \"output_index\": 1,\n  \"arguments\": \"{ \\\"arg\\\": 123 }\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseFunctionCallArgumentsDoneEvent {
  @extension("x-stainless-const", true)
  type: "response.function_call_arguments.done";

  /** The ID of the item. */
  item_id: string;

  /** The index of the output item. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The function-call arguments. */
  arguments: string;
}

/** Emitted when an image generation tool call has completed and the final image is available. */
@summary("ResponseImageGenCallCompletedEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.image_generation_call.completed",
    group: "responses",
    example: "{\n  \"type\": \"response.image_generation_call.completed\",\n  \"output_index\": 0,\n  \"item_id\": \"item-123\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseImageGenCallCompletedEvent {
  /** The type of the event. Always 'response.image_generation_call.completed'. */
  @extension("x-stainless-const", true)
  type: "response.image_generation_call.completed";

  /** The index of the output item in the response's output array. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The unique identifier of the image generation item being processed. */
  item_id: string;
}

/** Emitted when an image generation tool call is actively generating an image (intermediate state). */
@summary("ResponseImageGenCallGeneratingEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.image_generation_call.generating",
    group: "responses",
    example: "{\n  \"type\": \"response.image_generation_call.generating\",\n  \"output_index\": 0,\n  \"item_id\": \"item-123\",\n  \"sequence_number\": 0\n}\n",
  }
)
model ResponseImageGenCallGeneratingEvent {
  /** The type of the event. Always 'response.image_generation_call.generating'. */
  @extension("x-stainless-const", true)
  type: "response.image_generation_call.generating";

  /** The index of the output item in the response's output array. */
  output_index: integer;

  /** The unique identifier of the image generation item being processed. */
  item_id: string;

  /** The sequence number of the image generation item being processed. */
  sequence_number: integer;
}

/** Emitted when an image generation tool call is in progress. */
@summary("ResponseImageGenCallInProgressEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.image_generation_call.in_progress",
    group: "responses",
    example: "{\n  \"type\": \"response.image_generation_call.in_progress\",\n  \"output_index\": 0,\n  \"item_id\": \"item-123\",\n  \"sequence_number\": 0\n}\n",
  }
)
model ResponseImageGenCallInProgressEvent {
  /** The type of the event. Always 'response.image_generation_call.in_progress'. */
  @extension("x-stainless-const", true)
  type: "response.image_generation_call.in_progress";

  /** The index of the output item in the response's output array. */
  output_index: integer;

  /** The unique identifier of the image generation item being processed. */
  item_id: string;

  /** The sequence number of the image generation item being processed. */
  sequence_number: integer;
}

/** Emitted when a partial image is available during image generation streaming. */
@summary("ResponseImageGenCallPartialImageEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.image_generation_call.partial_image",
    group: "responses",
    example: "{\n  \"type\": \"response.image_generation_call.partial_image\",\n  \"output_index\": 0,\n  \"item_id\": \"item-123\",\n  \"sequence_number\": 0,\n  \"partial_image_index\": 0,\n  \"partial_image_b64\": \"...\"\n}\n",
  }
)
model ResponseImageGenCallPartialImageEvent {
  /** The type of the event. Always 'response.image_generation_call.partial_image'. */
  @extension("x-stainless-const", true)
  type: "response.image_generation_call.partial_image";

  /** The index of the output item in the response's output array. */
  output_index: integer;

  /** The unique identifier of the image generation item being processed. */
  item_id: string;

  /** The sequence number of the image generation item being processed. */
  sequence_number: integer;

  /** 0-based index for the partial image (backend is 1-based, but this is 0-based for the user). */
  partial_image_index: integer;

  /** Base64-encoded partial image data, suitable for rendering as an image. */
  partial_image_b64: string;
}

/** Emitted when the response is in progress. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.in_progress",
    group: "responses",
    example: "{\n  \"type\": \"response.in_progress\",\n  \"response\": {\n    \"id\": \"resp_67ccfcdd16748190a91872c75d38539e09e4d4aac714747c\",\n    \"object\": \"response\",\n    \"created_at\": 1741487325,\n    \"status\": \"in_progress\",\n    \"error\": null,\n    \"incomplete_details\": null,\n    \"instructions\": null,\n    \"max_output_tokens\": null,\n    \"model\": \"gpt-4o-2024-08-06\",\n    \"output\": [],\n    \"parallel_tool_calls\": true,\n    \"previous_response_id\": null,\n    \"reasoning\": {\n      \"effort\": null,\n      \"summary\": null\n    },\n    \"store\": true,\n    \"temperature\": 1,\n    \"text\": {\n      \"format\": {\n        \"type\": \"text\"\n      }\n    },\n    \"tool_choice\": \"auto\",\n    \"tools\": [],\n    \"top_p\": 1,\n    \"truncation\": \"disabled\",\n    \"usage\": null,\n    \"user\": null,\n    \"metadata\": {}\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseInProgressEvent {
  /** The type of the event. Always `response.in_progress`. */
  @extension("x-stainless-const", true)
  type: "response.in_progress";

  /** The response that is in progress. */
  response: Response;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** An event that is emitted when a response finishes as incomplete. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.incomplete",
    group: "responses",
    example: "{\n  \"type\": \"response.incomplete\",\n  \"response\": {\n    \"id\": \"resp_123\",\n    \"object\": \"response\",\n    \"created_at\": 1740855869,\n    \"status\": \"incomplete\",\n    \"error\": null, \n    \"incomplete_details\": {\n      \"reason\": \"max_tokens\"\n    },\n    \"instructions\": null,\n    \"max_output_tokens\": null,\n    \"model\": \"gpt-4o-mini-2024-07-18\",\n    \"output\": [],\n    \"previous_response_id\": null,\n    \"reasoning_effort\": null,\n    \"store\": false,\n    \"temperature\": 1,\n    \"text\": {\n      \"format\": {\n        \"type\": \"text\"\n      }\n    },\n    \"tool_choice\": \"auto\",\n    \"tools\": [],\n    \"top_p\": 1,\n    \"truncation\": \"disabled\",\n    \"usage\": null,\n    \"user\": null,\n    \"metadata\": {}\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseIncompleteEvent {
  /** The type of the event. Always `response.incomplete`. */
  @extension("x-stainless-const", true)
  type: "response.incomplete";

  /** The response that was incomplete. */
  response: Response;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** A list of Response items. */
@extension(
  "x-oaiMeta",
  #{
    name: "The input item list",
    group: "responses",
    example: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"msg_abc123\",\n      \"type\": \"message\",\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"input_text\",\n          \"text\": \"Tell me a three sentence bedtime story about a unicorn.\"\n        }\n      ]\n    }\n  ],\n  \"first_id\": \"msg_abc123\",\n  \"last_id\": \"msg_abc123\",\n  \"has_more\": false\n}\n",
  }
)
model ResponseItemList {
  /** The type of object returned, must be `list`. */
  @extension("x-stainless-const", true)
  object: "list";

  /** A list of items used to generate this response. */
  data: ItemResource[];

  /** Whether there are more items available. */
  has_more: boolean;

  /** The ID of the first item in the list. */
  first_id: string;

  /** The ID of the last item in the list. */
  last_id: string;
}

/** Content item used to generate a response. */
@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union ItemResource {
  message: InputMessageResource,
  output_message: OutputMessage,
  file_search_call: FileSearchToolCall,
  computer_call: ComputerToolCall,
  computer_call_output: ComputerToolCallOutputResource,
  web_search_call: WebSearchToolCall,
  function_call: FunctionToolCallResource,
  function_call_output: FunctionToolCallOutputResource,
  image_generation_call: ImageGenToolCall,
  code_interpreter_call: CodeInterpreterToolCall,
  local_shell_call: LocalShellToolCall,
  local_shell_call_output: LocalShellToolCallOutput,
  mcp_list_tools: MCPListTools,
  mcp_approval_request: MCPApprovalRequest,
  mcp_approval_response: MCPApprovalResponseResource,
  mcp_call: MCPToolCall,
}

/**A logprob is the logarithmic probability that the model assigns to producing
a particular token at a given position in the sequence. Less-negative (higher)
logprob values indicate greater model confidence in that token choice.*/
model ResponseLogProb {
  /** A possible text token. */
  token: string;

  /** The log probability of this token. */
  logprob: numeric;

  /** The log probability of the top 20 most likely tokens. */
  top_logprobs?: {
    token?: string;
    logprob?: numeric;
  }[];
}

/** Emitted when there is a delta (partial update) to the arguments of an MCP tool call. */
@summary("ResponseMCPCallArgumentsDeltaEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.mcp_call_arguments.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.mcp_call_arguments.delta\",\n  \"output_index\": 0,\n  \"item_id\": \"item-abc\",\n  \"delta\": \"{\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseMCPCallArgumentsDeltaEvent {
  /** The type of the event. Always 'response.mcp_call_arguments.delta'. */
  @extension("x-stainless-const", true)
  type: "response.mcp_call_arguments.delta";

  /** The index of the output item in the response's output array. */
  output_index: integer;

  /** The unique identifier of the MCP tool call item being processed. */
  item_id: string;

  /** A JSON string containing the partial update to the arguments for the MCP tool call. */
  delta: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when the arguments for an MCP tool call are finalized. */
@summary("ResponseMCPCallArgumentsDoneEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.mcp_call_arguments.done",
    group: "responses",
    example: "{\n  \"type\": \"response.mcp_call_arguments.done\",\n  \"output_index\": 0,\n  \"item_id\": \"item-abc\",\n  \"arguments\": \"{\\\"arg1\\\": \\\"value1\\\", \\\"arg2\\\": \\\"value2\\\"}\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseMCPCallArgumentsDoneEvent {
  /** The type of the event. Always 'response.mcp_call_arguments.done'. */
  @extension("x-stainless-const", true)
  type: "response.mcp_call_arguments.done";

  /** The index of the output item in the response's output array. */
  output_index: integer;

  /** The unique identifier of the MCP tool call item being processed. */
  item_id: string;

  /** A JSON string containing the finalized arguments for the MCP tool call. */
  arguments: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when an MCP  tool call has completed successfully. */
@summary("ResponseMCPCallCompletedEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.mcp_call.completed",
    group: "responses",
    example: "{\n  \"type\": \"response.mcp_call.completed\",\n  \"sequence_number\": 1,\n  \"item_id\": \"mcp_682d437d90a88191bf88cd03aae0c3e503937d5f622d7a90\",\n  \"output_index\": 0\n}\n",
  }
)
model ResponseMCPCallCompletedEvent {
  /** The type of the event. Always 'response.mcp_call.completed'. */
  @extension("x-stainless-const", true)
  type: "response.mcp_call.completed";

  /** The ID of the MCP tool call item that completed. */
  item_id: string;

  /** The index of the output item that completed. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when an MCP  tool call has failed. */
@summary("ResponseMCPCallFailedEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.mcp_call.failed",
    group: "responses",
    example: "{\n  \"type\": \"response.mcp_call.failed\",\n  \"sequence_number\": 1,\n  \"item_id\": \"mcp_682d437d90a88191bf88cd03aae0c3e503937d5f622d7a90\",\n  \"output_index\": 0\n}\n",
  }
)
model ResponseMCPCallFailedEvent {
  /** The type of the event. Always 'response.mcp_call.failed'. */
  @extension("x-stainless-const", true)
  type: "response.mcp_call.failed";

  /** The ID of the MCP tool call item that failed. */
  item_id: string;

  /** The index of the output item that failed. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when an MCP  tool call is in progress. */
@summary("ResponseMCPCallInProgressEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.mcp_call.in_progress",
    group: "responses",
    example: "{\n  \"type\": \"response.mcp_call.in_progress\",\n  \"sequence_number\": 1,\n  \"output_index\": 0,\n  \"item_id\": \"mcp_682d437d90a88191bf88cd03aae0c3e503937d5f622d7a90\"\n}\n",
  }
)
model ResponseMCPCallInProgressEvent {
  /** The type of the event. Always 'response.mcp_call.in_progress'. */
  @extension("x-stainless-const", true)
  type: "response.mcp_call.in_progress";

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The index of the output item in the response's output array. */
  output_index: integer;

  /** The unique identifier of the MCP tool call item being processed. */
  item_id: string;
}

/** Emitted when the list of available MCP tools has been successfully retrieved. */
@summary("ResponseMCPListToolsCompletedEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.mcp_list_tools.completed",
    group: "responses",
    example: "{\n  \"type\": \"response.mcp_list_tools.completed\",\n  \"sequence_number\": 1,\n  \"output_index\": 0,\n  \"item_id\": \"mcpl_682d4379df088191886b70f4ec39f90403937d5f622d7a90\"\n}\n",
  }
)
model ResponseMCPListToolsCompletedEvent {
  /** The type of the event. Always 'response.mcp_list_tools.completed'. */
  @extension("x-stainless-const", true)
  type: "response.mcp_list_tools.completed";

  /** The ID of the MCP tool call item that produced this output. */
  item_id: string;

  /** The index of the output item that was processed. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when the attempt to list available MCP tools has failed. */
@summary("ResponseMCPListToolsFailedEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.mcp_list_tools.failed",
    group: "responses",
    example: "{\n  \"type\": \"response.mcp_list_tools.failed\",\n  \"sequence_number\": 1,\n  \"output_index\": 0,\n  \"item_id\": \"mcpl_682d4379df088191886b70f4ec39f90403937d5f622d7a90\"\n}\n",
  }
)
model ResponseMCPListToolsFailedEvent {
  /** The type of the event. Always 'response.mcp_list_tools.failed'. */
  @extension("x-stainless-const", true)
  type: "response.mcp_list_tools.failed";

  /** The ID of the MCP tool call item that failed. */
  item_id: string;

  /** The index of the output item that failed. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when the system is in the process of retrieving the list of available MCP tools. */
@summary("ResponseMCPListToolsInProgressEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.mcp_list_tools.in_progress",
    group: "responses",
    example: "{\n  \"type\": \"response.mcp_list_tools.in_progress\",\n  \"sequence_number\": 1,\n  \"output_index\": 0,\n  \"item_id\": \"mcpl_682d4379df088191886b70f4ec39f90403937d5f622d7a90\"\n}\n",
  }
)
model ResponseMCPListToolsInProgressEvent {
  /** The type of the event. Always 'response.mcp_list_tools.in_progress'. */
  @extension("x-stainless-const", true)
  type: "response.mcp_list_tools.in_progress";

  /** The ID of the MCP tool call item that is being processed. */
  item_id: string;

  /** The index of the output item that is being processed. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when a new output item is added. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.output_item.added",
    group: "responses",
    example: "{\n  \"type\": \"response.output_item.added\",\n  \"output_index\": 0,\n  \"item\": {\n    \"id\": \"msg_123\",\n    \"status\": \"in_progress\",\n    \"type\": \"message\",\n    \"role\": \"assistant\",\n    \"content\": []\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseOutputItemAddedEvent {
  /** The type of the event. Always `response.output_item.added`. */
  @extension("x-stainless-const", true)
  type: "response.output_item.added";

  /** The index of the output item that was added. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The output item that was added. */
  item: OutputItem;
}

/** Emitted when an output item is marked done. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.output_item.done",
    group: "responses",
    example: "{\n  \"type\": \"response.output_item.done\",\n  \"output_index\": 0,\n  \"item\": {\n    \"id\": \"msg_123\",\n    \"status\": \"completed\",\n    \"type\": \"message\",\n    \"role\": \"assistant\",\n    \"content\": [\n      {\n        \"type\": \"output_text\",\n        \"text\": \"In a shimmering forest under a sky full of stars, a lonely unicorn named Lila discovered a hidden pond that glowed with moonlight. Every night, she would leave sparkling, magical flowers by the water's edge, hoping to share her beauty with others. One enchanting evening, she woke to find a group of friendly animals gathered around, eager to be friends and share in her magic.\",\n        \"annotations\": []\n      }\n    ]\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseOutputItemDoneEvent {
  /** The type of the event. Always `response.output_item.done`. */
  @extension("x-stainless-const", true)
  type: "response.output_item.done";

  /** The index of the output item that was marked done. */
  output_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The output item that was marked done. */
  item: OutputItem;
}

/** Emitted when an annotation is added to output text content. */
@summary("ResponseOutputTextAnnotationAddedEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.output_text.annotation.added",
    group: "responses",
    example: "{\n  \"type\": \"response.output_text.annotation.added\",\n  \"item_id\": \"item-abc\",\n  \"output_index\": 0,\n  \"content_index\": 0,\n  \"annotation_index\": 0,\n  \"annotation\": {\n    \"type\": \"text_annotation\",\n    \"text\": \"This is a test annotation\",\n    \"start\": 0,\n    \"end\": 10\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseOutputTextAnnotationAddedEvent {
  /** The type of the event. Always 'response.output_text.annotation.added'. */
  @extension("x-stainless-const", true)
  type: "response.output_text.annotation.added";

  /** The unique identifier of the item to which the annotation is being added. */
  item_id: string;

  /** The index of the output item in the response's output array. */
  output_index: integer;

  /** The index of the content part within the output item. */
  content_index: integer;

  /** The index of the annotation within the content part. */
  annotation_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The annotation object being added. (See annotation schema for details.) */
  annotation: {};
}

/** Emitted when a response is queued and waiting to be processed. */
@summary("ResponseQueuedEvent")
@extension(
  "x-oaiMeta",
  #{
    name: "response.queued",
    group: "responses",
    example: "{\n  \"type\": \"response.queued\",\n  \"response\": {\n    \"id\": \"res_123\",\n    \"status\": \"queued\",\n    \"created_at\": \"2021-01-01T00:00:00Z\",\n    \"updated_at\": \"2021-01-01T00:00:00Z\"\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseQueuedEvent {
  /** The type of the event. Always 'response.queued'. */
  @extension("x-stainless-const", true)
  type: "response.queued";

  /** The full response object that is queued. */
  response: Response;

  /** The sequence number for this event. */
  sequence_number: integer;
}

/** Emitted when a new reasoning summary part is added. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.reasoning_summary_part.added",
    group: "responses",
    example: "{\n  \"type\": \"response.reasoning_summary_part.added\",\n  \"item_id\": \"rs_6806bfca0b2481918a5748308061a2600d3ce51bdffd5476\",\n  \"output_index\": 0,\n  \"summary_index\": 0,\n  \"part\": {\n    \"type\": \"summary_text\",\n    \"text\": \"\"\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseReasoningSummaryPartAddedEvent {
  /** The type of the event. Always `response.reasoning_summary_part.added`. */
  @extension("x-stainless-const", true)
  type: "response.reasoning_summary_part.added";

  /** The ID of the item this summary part is associated with. */
  item_id: string;

  /** The index of the output item this summary part is associated with. */
  output_index: integer;

  /** The index of the summary part within the reasoning summary. */
  summary_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The summary part that was added. */
  part: {
    @extension("x-stainless-const", true)
    type: "summary_text";

    text: string;
  };
}

/** Emitted when a reasoning summary part is completed. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.reasoning_summary_part.done",
    group: "responses",
    example: "{\n  \"type\": \"response.reasoning_summary_part.done\",\n  \"item_id\": \"rs_6806bfca0b2481918a5748308061a2600d3ce51bdffd5476\",\n  \"output_index\": 0,\n  \"summary_index\": 0,\n  \"part\": {\n    \"type\": \"summary_text\",\n    \"text\": \"**Responding to a greeting**\\n\\nThe user just said, \\\"Hello!\\\" So, it seems I need to engage. I'll greet them back and offer help since they're looking to chat. I could say something like, \\\"Hello! How can I assist you today?\\\" That feels friendly and open. They didn't ask a specific question, so this approach will work well for starting a conversation. Let's see where it goes from there!\"\n  },\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseReasoningSummaryPartDoneEvent {
  /** The type of the event. Always `response.reasoning_summary_part.done`. */
  @extension("x-stainless-const", true)
  type: "response.reasoning_summary_part.done";

  /** The ID of the item this summary part is associated with. */
  item_id: string;

  /** The index of the output item this summary part is associated with. */
  output_index: integer;

  /** The index of the summary part within the reasoning summary. */
  summary_index: integer;

  /** The sequence number of this event. */
  sequence_number: integer;

  /** The completed summary part. */
  part: {
    @extension("x-stainless-const", true)
    type: "summary_text";

    text: string;
  };
}

/** Emitted when a delta is added to a reasoning summary text. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.reasoning_summary_text.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.reasoning_summary_text.delta\",\n  \"item_id\": \"rs_6806bfca0b2481918a5748308061a2600d3ce51bdffd5476\",\n  \"output_index\": 0,\n  \"summary_index\": 0,\n  \"delta\": \"**Responding to a greeting**\\n\\nThe user just said, \\\"Hello!\\\" So, it seems I need to engage. I'll greet them back and offer help since they're looking to chat. I could say something like, \\\"Hello! How can I assist you today?\\\" That feels friendly and open. They didn't ask a specific question, so this approach will work well for starting a conversation. Let's see where it goes from there!\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseReasoningSummaryTextDeltaEvent {
  /** The type of the event. Always `response.reasoning_summary_text.delta`. */
  @extension("x-stainless-const", true)
  type: "response.reasoning_summary_text.delta";

  /** The ID of the item this summary text delta is associated with. */
  item_id: string;

  /** The index of the output item this summary text delta is associated with. */
  output_index: integer;

  /** The index of the summary part within the reasoning summary. */
  summary_index: integer;

  /** The text delta that was added to the summary. */
  delta: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when a reasoning summary text is completed. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.reasoning_summary_text.done",
    group: "responses",
    example: "{\n  \"type\": \"response.reasoning_summary_text.done\",\n  \"item_id\": \"rs_6806bfca0b2481918a5748308061a2600d3ce51bdffd5476\",\n  \"output_index\": 0,\n  \"summary_index\": 0,\n  \"text\": \"**Responding to a greeting**\\n\\nThe user just said, \\\"Hello!\\\" So, it seems I need to engage. I'll greet them back and offer help since they're looking to chat. I could say something like, \\\"Hello! How can I assist you today?\\\" That feels friendly and open. They didn't ask a specific question, so this approach will work well for starting a conversation. Let's see where it goes from there!\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseReasoningSummaryTextDoneEvent {
  /** The type of the event. Always `response.reasoning_summary_text.done`. */
  @extension("x-stainless-const", true)
  type: "response.reasoning_summary_text.done";

  /** The ID of the item this summary text is associated with. */
  item_id: string;

  /** The index of the output item this summary text is associated with. */
  output_index: integer;

  /** The index of the summary part within the reasoning summary. */
  summary_index: integer;

  /** The full text of the completed reasoning summary. */
  text: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when a delta is added to a reasoning text. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.reasoning_text.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.reasoning_text.delta\",\n  \"item_id\": \"rs_123\",\n  \"output_index\": 0,\n  \"content_index\": 0,\n  \"delta\": \"The\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseReasoningTextDeltaEvent {
  /** The type of the event. Always `response.reasoning_text.delta`. */
  @extension("x-stainless-const", true)
  type: "response.reasoning_text.delta";

  /** The ID of the item this reasoning text delta is associated with. */
  item_id: string;

  /** The index of the output item this reasoning text delta is associated with. */
  output_index: integer;

  /** The index of the reasoning content part this delta is associated with. */
  content_index: integer;

  /** The text delta that was added to the reasoning content. */
  delta: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when a reasoning text is completed. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.reasoning_text.done",
    group: "responses",
    example: "{\n  \"type\": \"response.reasoning_text.done\",\n  \"item_id\": \"rs_123\",\n  \"output_index\": 0,\n  \"content_index\": 0,\n  \"text\": \"The user is asking...\",\n  \"sequence_number\": 4\n}\n",
  }
)
model ResponseReasoningTextDoneEvent {
  /** The type of the event. Always `response.reasoning_text.done`. */
  @extension("x-stainless-const", true)
  type: "response.reasoning_text.done";

  /** The ID of the item this reasoning text is associated with. */
  item_id: string;

  /** The index of the output item this reasoning text is associated with. */
  output_index: integer;

  /** The index of the reasoning content part. */
  content_index: integer;

  /** The full text of the completed reasoning content. */
  text: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when there is a partial refusal text. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.refusal.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.refusal.delta\",\n  \"item_id\": \"msg_123\",\n  \"output_index\": 0,\n  \"content_index\": 0,\n  \"delta\": \"refusal text so far\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseRefusalDeltaEvent {
  /** The type of the event. Always `response.refusal.delta`. */
  @extension("x-stainless-const", true)
  type: "response.refusal.delta";

  /** The ID of the output item that the refusal text is added to. */
  item_id: string;

  /** The index of the output item that the refusal text is added to. */
  output_index: integer;

  /** The index of the content part that the refusal text is added to. */
  content_index: integer;

  /** The refusal text that is added. */
  delta: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when refusal text is finalized. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.refusal.done",
    group: "responses",
    example: "{\n  \"type\": \"response.refusal.done\",\n  \"item_id\": \"item-abc\",\n  \"output_index\": 1,\n  \"content_index\": 2,\n  \"refusal\": \"final refusal text\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseRefusalDoneEvent {
  /** The type of the event. Always `response.refusal.done`. */
  @extension("x-stainless-const", true)
  type: "response.refusal.done";

  /** The ID of the output item that the refusal text is finalized. */
  item_id: string;

  /** The index of the output item that the refusal text is finalized. */
  output_index: integer;

  /** The index of the content part that the refusal text is finalized. */
  content_index: integer;

  /** The refusal text that is finalized. */
  refusal: string;

  /** The sequence number of this event. */
  sequence_number: integer;
}

/** Emitted when there is an additional text delta. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.output_text.delta",
    group: "responses",
    example: "{\n  \"type\": \"response.output_text.delta\",\n  \"item_id\": \"msg_123\",\n  \"output_index\": 0,\n  \"content_index\": 0,\n  \"delta\": \"In\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseTextDeltaEvent {
  /** The type of the event. Always `response.output_text.delta`. */
  @extension("x-stainless-const", true)
  type: "response.output_text.delta";

  /** The ID of the output item that the text delta was added to. */
  item_id: string;

  /** The index of the output item that the text delta was added to. */
  output_index: integer;

  /** The index of the content part that the text delta was added to. */
  content_index: integer;

  /** The text delta that was added. */
  delta: string;

  /** The sequence number for this event. */
  sequence_number: integer;

  /** The log probabilities of the tokens in the delta. */
  logprobs: ResponseLogProb[];
}

/** Emitted when text content is finalized. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.output_text.done",
    group: "responses",
    example: "{\n  \"type\": \"response.output_text.done\",\n  \"item_id\": \"msg_123\",\n  \"output_index\": 0,\n  \"content_index\": 0,\n  \"text\": \"In a shimmering forest under a sky full of stars, a lonely unicorn named Lila discovered a hidden pond that glowed with moonlight. Every night, she would leave sparkling, magical flowers by the water's edge, hoping to share her beauty with others. One enchanting evening, she woke to find a group of friendly animals gathered around, eager to be friends and share in her magic.\",\n  \"sequence_number\": 1\n}\n",
  }
)
model ResponseTextDoneEvent {
  /** The type of the event. Always `response.output_text.done`. */
  @extension("x-stainless-const", true)
  type: "response.output_text.done";

  /** The ID of the output item that the text content is finalized. */
  item_id: string;

  /** The index of the output item that the text content is finalized. */
  output_index: integer;

  /** The index of the content part that the text content is finalized. */
  content_index: integer;

  /** The text content that is finalized. */
  text: string;

  /** The sequence number for this event. */
  sequence_number: integer;

  /** The log probabilities of the tokens in the delta. */
  logprobs: ResponseLogProb[];
}

/** Emitted when a web search call is completed. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.web_search_call.completed",
    group: "responses",
    example: "{\n  \"type\": \"response.web_search_call.completed\",\n  \"output_index\": 0,\n  \"item_id\": \"ws_123\",\n  \"sequence_number\": 0\n}\n",
  }
)
model ResponseWebSearchCallCompletedEvent {
  /** The type of the event. Always `response.web_search_call.completed`. */
  @extension("x-stainless-const", true)
  type: "response.web_search_call.completed";

  /** The index of the output item that the web search call is associated with. */
  output_index: integer;

  /** Unique ID for the output item associated with the web search call. */
  item_id: string;

  /** The sequence number of the web search call being processed. */
  sequence_number: integer;
}

/** Emitted when a web search call is initiated. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.web_search_call.in_progress",
    group: "responses",
    example: "{\n  \"type\": \"response.web_search_call.in_progress\",\n  \"output_index\": 0,\n  \"item_id\": \"ws_123\",\n  \"sequence_number\": 0\n}\n",
  }
)
model ResponseWebSearchCallInProgressEvent {
  /** The type of the event. Always `response.web_search_call.in_progress`. */
  @extension("x-stainless-const", true)
  type: "response.web_search_call.in_progress";

  /** The index of the output item that the web search call is associated with. */
  output_index: integer;

  /** Unique ID for the output item associated with the web search call. */
  item_id: string;

  /** The sequence number of the web search call being processed. */
  sequence_number: integer;
}

/** Emitted when a web search call is executing. */
@extension(
  "x-oaiMeta",
  #{
    name: "response.web_search_call.searching",
    group: "responses",
    example: "{\n  \"type\": \"response.web_search_call.searching\",\n  \"output_index\": 0,\n  \"item_id\": \"ws_123\",\n  \"sequence_number\": 0\n}\n",
  }
)
model ResponseWebSearchCallSearchingEvent {
  /** The type of the event. Always `response.web_search_call.searching`. */
  @extension("x-stainless-const", true)
  type: "response.web_search_call.searching";

  /** The index of the output item that the web search call is associated with. */
  output_index: integer;

  /** Unique ID for the output item associated with the web search call. */
  item_id: string;

  /** The sequence number of the web search call being processed. */
  sequence_number: integer;
}

@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
union ResponseStreamEvent {
  `response.audio.delta`: ResponseAudioDeltaEvent,
  `response.audio.done`: ResponseAudioDoneEvent,
  `response.audio.transcript.delta`: ResponseAudioTranscriptDeltaEvent,
  `response.audio.transcript.done`: ResponseAudioTranscriptDoneEvent,
  `response.code_interpreter_call_code.delta`: ResponseCodeInterpreterCallCodeDeltaEvent,
  `response.code_interpreter_call_code.done`: ResponseCodeInterpreterCallCodeDoneEvent,
  `response.code_interpreter_call.completed`: ResponseCodeInterpreterCallCompletedEvent,
  `response.code_interpreter_call.in_progress`: ResponseCodeInterpreterCallInProgressEvent,
  `response.code_interpreter_call.interpreting`: ResponseCodeInterpreterCallInterpretingEvent,
  `response.completed`: ResponseCompletedEvent,
  `response.content_part.added`: ResponseContentPartAddedEvent,
  `response.content_part.done`: ResponseContentPartDoneEvent,
  `response.created`: ResponseCreatedEvent,
  error: ResponseErrorEvent,
  `response.file_search_call.completed`: ResponseFileSearchCallCompletedEvent,
  `response.file_search_call.in_progress`: ResponseFileSearchCallInProgressEvent,
  `response.file_search_call.searching`: ResponseFileSearchCallSearchingEvent,
  `response.function_call_arguments.delta`: ResponseFunctionCallArgumentsDeltaEvent,
  `response.function_call_arguments.done`: ResponseFunctionCallArgumentsDoneEvent,
  `response.in_progress`: ResponseInProgressEvent,
  `response.failed`: ResponseFailedEvent,
  `response.incomplete`: ResponseIncompleteEvent,
  `response.output_item.added`: ResponseOutputItemAddedEvent,
  `response.output_item.done`: ResponseOutputItemDoneEvent,
  `response.reasoning_summary_part.added`: ResponseReasoningSummaryPartAddedEvent,
  `response.reasoning_summary_part.done`: ResponseReasoningSummaryPartDoneEvent,
  `response.reasoning_summary_text.delta`: ResponseReasoningSummaryTextDeltaEvent,
  `response.reasoning_summary_text.done`: ResponseReasoningSummaryTextDoneEvent,
  `response.reasoning_text.delta`: ResponseReasoningTextDeltaEvent,
  `response.reasoning_text.done`: ResponseReasoningTextDoneEvent,
  `response.refusal.delta`: ResponseRefusalDeltaEvent,
  `response.refusal.done`: ResponseRefusalDoneEvent,
  `response.output_text.delta`: ResponseTextDeltaEvent,
  `response.output_text.done`: ResponseTextDoneEvent,
  `response.web_search_call.completed`: ResponseWebSearchCallCompletedEvent,
  `response.web_search_call.in_progress`: ResponseWebSearchCallInProgressEvent,
  `response.web_search_call.searching`: ResponseWebSearchCallSearchingEvent,
  `response.image_generation_call.completed`: ResponseImageGenCallCompletedEvent,
  `response.image_generation_call.generating`: ResponseImageGenCallGeneratingEvent,
  `response.image_generation_call.in_progress`: ResponseImageGenCallInProgressEvent,
  `response.image_generation_call.partial_image`: ResponseImageGenCallPartialImageEvent,
  `response.mcp_call_arguments.delta`: ResponseMCPCallArgumentsDeltaEvent,
  `response.mcp_call_arguments.done`: ResponseMCPCallArgumentsDoneEvent,
  `response.mcp_call.completed`: ResponseMCPCallCompletedEvent,
  `response.mcp_call.failed`: ResponseMCPCallFailedEvent,
  `response.mcp_call.in_progress`: ResponseMCPCallInProgressEvent,
  `response.mcp_list_tools.completed`: ResponseMCPListToolsCompletedEvent,
  `response.mcp_list_tools.failed`: ResponseMCPListToolsFailedEvent,
  `response.mcp_list_tools.in_progress`: ResponseMCPListToolsInProgressEvent,
  `response.output_text.annotation.added`: ResponseOutputTextAnnotationAddedEvent,
  `response.queued`: ResponseQueuedEvent,
  `response.custom_tool_call_input.delta`: ResponseCustomToolCallInputDeltaEvent,
  `response.custom_tool_call_input.done`: ResponseCustomToolCallInputDoneEvent,
}

/** Multi-modal input and output contents. */
union Content {
  InputContent,
  OutputContent,
}

union ModelIds {
  ModelIdsShared,
  ModelIdsResponses,
}

/**Indicates that the model should use a built-in tool to generate a response.
[Learn more about built-in tools](https://platform.openai.com/docs/guides/tools).*/
@discriminated(#{ envelope: "none", discriminatorPropertyName: "type" })
@summary("Hosted tool")
union ToolChoiceTypes {
  file_search: ToolChoiceFileSearch,
  web_search_preview: ToolChoiceWebSearchPreview,
  computer_use_preview: ToolChoiceComputerUsePreview,
  web_search_preview_2025_03_11: ToolChoiceWebSearchPreview20250311,
  image_generation: ToolChoiceImageGeneration,
  code_interpreter: ToolChoiceCodeInterpreter,
}
